Attaching to kafka1
[36mkafka1             |[0m ===> User
[36mkafka1             |[0m uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
[36mkafka1             |[0m ===> Configuring ...
[36mkafka1             |[0m ===> Running preflight checks ...
[36mkafka1             |[0m ===> Check if /var/lib/kafka/data is writable ...
[36mkafka1             |[0m ===> Check if Zookeeper is healthy ...
[36mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:zookeeper.version=3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:53 GMT
[36mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:host.name=kafka1
[36mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.version=11.0.9.1
[36mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.vendor=Azul Systems, Inc.
[36mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.home=/usr/lib/jvm/zulu11-ca
[36mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.class.path=/usr/share/java/cp-base-new/snappy-java-1.1.7.3.jar:/usr/share/java/cp-base-new/utility-belt-6.0.1.jar:/usr/share/java/cp-base-new/slf4j-simple-1.7.30.jar:/usr/share/java/cp-base-new/commons-cli-1.4.jar:/usr/share/java/cp-base-new/jackson-dataformat-yaml-2.10.5.jar:/usr/share/java/cp-base-new/jackson-module-scala_2.13-2.10.5.jar:/usr/share/java/cp-base-new/paranamer-2.8.jar:/usr/share/java/cp-base-new/netty-transport-native-unix-common-4.1.48.Final.jar:/usr/share/java/cp-base-new/zstd-jni-1.4.4-7.jar:/usr/share/java/cp-base-new/lz4-java-1.7.1.jar:/usr/share/java/cp-base-new/common-utils-6.0.1.jar:/usr/share/java/cp-base-new/zookeeper-3.5.8.jar:/usr/share/java/cp-base-new/netty-buffer-4.1.48.Final.jar:/usr/share/java/cp-base-new/scala-reflect-2.13.2.jar:/usr/share/java/cp-base-new/jopt-simple-5.0.4.jar:/usr/share/java/cp-base-new/netty-codec-4.1.48.Final.jar:/usr/share/java/cp-base-new/kafka_2.13-6.0.1-ccs.jar:/usr/share/java/cp-base-new/netty-transport-4.1.48.Final.jar:/usr/share/java/cp-base-new/metrics-core-2.2.0.jar:/usr/share/java/cp-base-new/slf4j-api-1.7.30.jar:/usr/share/java/cp-base-new/gson-2.8.6.jar:/usr/share/java/cp-base-new/netty-common-4.1.48.Final.jar:/usr/share/java/cp-base-new/scala-java8-compat_2.13-0.9.1.jar:/usr/share/java/cp-base-new/netty-transport-native-epoll-4.1.48.Final.jar:/usr/share/java/cp-base-new/jackson-datatype-jdk8-2.10.5.jar:/usr/share/java/cp-base-new/jackson-core-2.10.5.jar:/usr/share/java/cp-base-new/zookeeper-jute-3.5.8.jar:/usr/share/java/cp-base-new/scala-library-2.13.2.jar:/usr/share/java/cp-base-new/jackson-annotations-2.10.5.jar:/usr/share/java/cp-base-new/snakeyaml-1.26.jar:/usr/share/java/cp-base-new/scala-logging_2.13-3.9.2.jar:/usr/share/java/cp-base-new/jackson-dataformat-csv-2.10.5.jar:/usr/share/java/cp-base-new/netty-resolver-4.1.48.Final.jar:/usr/share/java/cp-base-new/scala-collection-compat_2.13-2.1.6.jar:/usr/share/java/cp-base-new/kafka-clients-6.0.1-ccs.jar:/usr/share/java/cp-base-new/netty-handler-4.1.48.Final.jar:/usr/share/java/cp-base-new/argparse4j-0.7.0.jar:/usr/share/java/cp-base-new/jackson-module-paranamer-2.10.5.jar:/usr/share/java/cp-base-new/jackson-databind-2.10.5.jar:/usr/share/java/cp-base-new/audience-annotations-0.5.0.jar
[36mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib
[36mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.io.tmpdir=/tmp
[36mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.compiler=<NA>
[36mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.name=Linux
[36mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.arch=amd64
[36mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.version=4.19.121-linuxkit
[36mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.name=appuser
[36mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.home=/home/appuser
[36mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.dir=/home/appuser
[36mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.free=128MB
[36mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.max=2116MB
[36mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.total=134MB
[36mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=zookeeper:2181 sessionTimeout=40000 watcher=io.confluent.admin.utils.ZookeeperConnectionWatcher@72b6cbcc
[36mkafka1             |[0m [main] INFO org.apache.zookeeper.common.X509Util - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
[36mkafka1             |[0m [main] INFO org.apache.zookeeper.ClientCnxnSocket - jute.maxbuffer value is 4194304 Bytes
[36mkafka1             |[0m [main] INFO org.apache.zookeeper.ClientCnxn - zookeeper.request.timeout value is 0. feature enabled=
[36mkafka1             |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/192.168.176.2:2181. Will not attempt to authenticate using SASL (unknown error)
[36mkafka1             |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Socket error occurred: zookeeper/192.168.176.2:2181: Connection refused
[36mkafka1             |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/192.168.176.2:2181. Will not attempt to authenticate using SASL (unknown error)
[36mkafka1             |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Socket connection established, initiating session, client: /192.168.176.3:36706, server: zookeeper/192.168.176.2:2181
[36mkafka1             |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Unable to read additional data from server sessionid 0x0, likely server has closed socket, closing socket connection and attempting reconnect
[36mkafka1             |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/192.168.176.2:2181. Will not attempt to authenticate using SASL (unknown error)
[36mkafka1             |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Socket connection established, initiating session, client: /192.168.176.3:37738, server: zookeeper/192.168.176.2:2181
[36mkafka1             |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Session establishment complete on server zookeeper/192.168.176.2:2181, sessionid = 0x10011273bc80002, negotiated timeout = 40000
[36mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Session: 0x10011273bc80002 closed
[36mkafka1             |[0m [main-EventThread] INFO org.apache.zookeeper.ClientCnxn - EventThread shut down for session: 0x10011273bc80002
[36mkafka1             |[0m ===> Launching ...
[36mkafka1             |[0m ===> Launching kafka ...
[36mkafka1             |[0m [2021-01-09 19:50:11,684] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[36mkafka1             |[0m [2021-01-09 19:50:14,159] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[36mkafka1             |[0m [2021-01-09 19:50:14,473] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[36mkafka1             |[0m [2021-01-09 19:50:14,501] INFO starting (kafka.server.KafkaServer)
[36mkafka1             |[0m [2021-01-09 19:50:14,506] INFO FIPS mode enabled: false (kafka.server.KafkaServer)
[36mkafka1             |[0m [2021-01-09 19:50:14,517] INFO Connecting to zookeeper on zookeeper:2181 (kafka.server.KafkaServer)
[36mkafka1             |[0m [2021-01-09 19:50:14,629] INFO [ZooKeeperClient Kafka server] Initializing a new session to zookeeper:2181. (kafka.zookeeper.ZooKeeperClient)
[36mkafka1             |[0m [2021-01-09 19:50:14,667] INFO Client environment:zookeeper.version=3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:53 GMT (org.apache.zookeeper.ZooKeeper)
[36mkafka1             |[0m [2021-01-09 19:50:14,673] INFO Client environment:host.name=kafka1 (org.apache.zookeeper.ZooKeeper)
[36mkafka1             |[0m [2021-01-09 19:50:14,673] INFO Client environment:java.version=11.0.9.1 (org.apache.zookeeper.ZooKeeper)
[36mkafka1             |[0m [2021-01-09 19:50:14,673] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)
[36mkafka1             |[0m [2021-01-09 19:50:14,673] INFO Client environment:java.home=/usr/lib/jvm/zulu11-ca (org.apache.zookeeper.ZooKeeper)
[36mkafka1             |[0m [2021-01-09 19:50:14,673] INFO Client environment:java.class.path=/usr/bin/../ce-broker-plugins/build/libs/*:/usr/bin/../ce-broker-plugins/build/dependant-libs/*:/usr/bin/../ce-auth-providers/build/libs/*:/usr/bin/../ce-auth-providers/build/dependant-libs/*:/usr/bin/../ce-rest-server/build/libs/*:/usr/bin/../ce-rest-server/build/dependant-libs/*:/usr/bin/../ce-audit/build/libs/*:/usr/bin/../ce-audit/build/dependant-libs/*:/usr/bin/../share/java/kafka/jersey-container-servlet-2.30.jar:/usr/bin/../share/java/kafka/google-cloud-core-1.82.0.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.7.3.jar:/usr/bin/../share/java/kafka/kafka_2.13-6.0.1-ce-javadoc.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.10.5.jar:/usr/bin/../share/java/kafka/bctls-fips-1.0.10.jar:/usr/bin/../share/java/kafka/slf4j-log4j12-1.7.30.jar:/usr/bin/../share/java/kafka/httpcore-4.4.13.jar:/usr/bin/../share/java/kafka/bc-fips-1.0.2.jar:/usr/bin/../share/java/kafka/telemetry-client-1.192.1.jar:/usr/bin/../share/java/kafka/google-cloud-storage-1.82.0.jar:/usr/bin/../share/java/kafka/ce-sbk_2.13-6.0.1-ce.jar:/usr/bin/../share/java/kafka/netty-all-4.1.50.Final.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/google-http-client-jackson2-1.30.1.jar:/usr/bin/../share/java/kafka/commons-cli-1.4.jar:/usr/bin/../share/java/kafka/cloudevents-api-1.3.0.jar:/usr/bin/../share/java/kafka/proto-google-common-protos-1.16.0.jar:/usr/bin/../share/java/kafka/httpclient-4.5.11.jar:/usr/bin/../share/java/kafka/jsr305-3.0.2.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/threetenbp-1.3.3.jar:/usr/bin/../share/java/kafka/connect-mirror-client-6.0.1-ce.jar:/usr/bin/../share/java/kafka/error_prone_annotations-2.3.4.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.13-2.10.5.jar:/usr/bin/../share/java/kafka/netty-transport-4.1.50.Final.jar:/usr/bin/../share/java/kafka/hk2-api-2.6.1.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/bcpkix-fips-1.0.3.jar:/usr/bin/../share/java/kafka/connect-runtime-6.0.1-ce.jar:/usr/bin/../share/java/kafka/jakarta.el-3.0.3.jar:/usr/bin/../share/java/kafka/google-http-client-1.30.1.jar:/usr/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/kafka/classmate-1.3.4.jar:/usr/bin/../share/java/kafka/netty-handler-4.1.50.Final.jar:/usr/bin/../share/java/kafka/commons-codec-1.11.jar:/usr/bin/../share/java/kafka/zstd-jni-1.4.4-7.jar:/usr/bin/../share/java/kafka/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/usr/bin/../share/java/kafka/jose4j-0.7.2.jar:/usr/bin/../share/java/kafka/connect-mirror-6.0.1-ce.jar:/usr/bin/../share/java/kafka/google-api-services-storage-v1-rev20190624-1.30.1.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/ion-java-1.0.2.jar:/usr/bin/../share/java/kafka/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/kafka/confluent-log4j-1.2.17-cp2.jar:/usr/bin/../share/java/kafka/cloudevents-kafka-1.3.0.jar:/usr/bin/../share/java/kafka/api-common-1.8.1.jar:/usr/bin/../share/java/kafka/commons-logging-1.2.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.10.5.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/gax-1.47.1.jar:/usr/bin/../share/java/kafka/connect-json-6.0.1-ce.jar:/usr/bin/../share/java/kafka/jersey-client-2.30.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/auth-providers-6.0.1-ce.jar:/usr/bin/../share/java/kafka/flatbuffers-java-1.9.0.jar:/usr/bin/../share/java/kafka/kafka-tools-6.0.1-ce.jar:/usr/bin/../share/java/kafka/google-auth-library-credentials-0.16.2.jar:/usr/bin/../share/java/kafka/animal-sniffer-annotations-1.18.jar:/usr/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-6.0.1-ce.jar:/usr/bin/../share/java/kafka/netty-buffer-4.1.50.Final.jar:/usr/bin/../share/java/kafka/netty-codec-4.1.50.Final.jar:/usr/bin/../share/java/kafka/lz4-java-1.7.1.jar:/usr/bin/../share/java/kafka/reflections-0.9.12.jar:/usr/bin/../share/java/kafka/zookeeper-3.5.8.jar:/usr/bin/../share/java/kafka/opencensus-contrib-http-util-0.21.0.jar:/usr/bin/../share/java/kafka/netty-resolver-4.1.50.Final.jar:/usr/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/kafka/failureaccess-1.0.1.jar:/usr/bin/../share/java/kafka/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/kafka/aws-java-sdk-s3-1.11.784.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/kafka/rocksdbjni-5.18.4.jar:/usr/bin/../share/java/kafka/kafka_2.13-6.0.1-ce-scaladoc.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.30.jar:/usr/bin/../share/java/kafka/rbac-6.0.1-ce.jar:/usr/bin/../share/java/kafka/connect-api-6.0.1-ce.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.10.5.jar:/usr/bin/../share/java/kafka/kafka-client-plugins-6.0.1-ce.jar:/usr/bin/../share/java/kafka/commons-lang3-3.11.jar:/usr/bin/../share/java/kafka/scala-reflect-2.13.2.jar:/usr/bin/../share/java/kafka/telemetry-api-1.192.1.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/broker-plugins-6.0.1-ce.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-6.0.1-ce.jar:/usr/bin/../share/java/kafka/proto-google-iam-v1-0.12.0.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-6.0.1-ce.jar:/usr/bin/../share/java/kafka/google-cloud-core-http-1.82.0.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-6.0.1-ce.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.13-6.0.1-ce.jar:/usr/bin/../share/java/kafka/guava-28.1-jre.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/connect-file-6.0.1-ce.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.30.jar:/usr/bin/../share/java/kafka/gson-2.8.6.jar:/usr/bin/../share/java/kafka/validation-api-2.0.1.Final.jar:/usr/bin/../share/java/kafka/grpc-context-1.19.0.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.30.jar:/usr/bin/../share/java/kafka/jbcrypt-0.4.jar:/usr/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/usr/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.13-0.9.1.jar:/usr/bin/../share/java/kafka/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/kafka/netty-common-4.1.50.Final.jar:/usr/bin/../share/java/kafka/kafka-clients-6.0.1-ce.jar:/usr/bin/../share/java/kafka/kafka-streams-6.0.1-ce.jar:/usr/bin/../share/java/kafka/gax-httpjson-0.64.1.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/protobuf-java-util-3.11.4.jar:/usr/bin/../share/java/kafka/maven-artifact-3.6.3.jar:/usr/bin/../share/java/kafka/jersey-server-2.30.jar:/usr/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.50.Final.jar:/usr/bin/../share/java/kafka/confluent-audit-6.0.1-ce.jar:/usr/bin/../share/java/kafka/jackson-datatype-jdk8-2.10.5.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/jackson-core-2.10.5.jar:/usr/bin/../share/java/kafka/checker-qual-2.8.1.jar:/usr/bin/../share/java/kafka/zookeeper-jute-3.5.8.jar:/usr/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/kafka/scala-library-2.13.2.jar:/usr/bin/../share/java/kafka/authorizer-6.0.1-ce.jar:/usr/bin/../share/java/kafka/javassist-3.26.0-GA.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.10.5.jar:/usr/bin/../share/java/kafka/j2objc-annotations-1.3.jar:/usr/bin/../share/java/kafka/google-auth-library-oauth2-http-0.16.2.jar:/usr/bin/../share/java/kafka/kafka_2.13-6.0.1-ce-test.jar:/usr/bin/../share/java/kafka/jersey-common-2.30.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/aws-java-sdk-core-1.11.784.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/rest-authorizer-6.0.1-ce.jar:/usr/bin/../share/java/kafka/scala-logging_2.13-3.9.2.jar:/usr/bin/../share/java/kafka/aws-java-sdk-kms-1.11.784.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/google-oauth-client-1.30.1.jar:/usr/bin/../share/java/kafka/jackson-dataformat-csv-2.10.5.jar:/usr/bin/../share/java/kafka/joda-time-2.9.9.jar:/usr/bin/../share/java/kafka/hibernate-validator-6.0.17.Final.jar:/usr/bin/../share/java/kafka/google-http-client-appengine-1.30.1.jar:/usr/bin/../share/java/kafka/internal-rest-server-6.0.1-ce.jar:/usr/bin/../share/java/kafka/kafka_2.13-6.0.1-ce.jar:/usr/bin/../share/java/kafka/connect-transforms-6.0.1-ce.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.13-2.1.6.jar:/usr/bin/../share/java/kafka/opencensus-proto-0.2.0.jar:/usr/bin/../share/java/kafka/jackson-dataformat-cbor-2.10.5.jar:/usr/bin/../share/java/kafka/jboss-logging-3.3.2.Final.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/aws-java-sdk-sts-1.11.784.jar:/usr/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/kafka/google-api-client-1.30.2.jar:/usr/bin/../share/java/kafka/plexus-utils-3.2.1.jar:/usr/bin/../share/java/kafka/commons-math3-3.6.1.jar:/usr/bin/../share/java/kafka/jmespath-java-1.11.784.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.50.Final.jar:/usr/bin/../share/java/kafka/telemetry-events-6.0.1-ce.jar:/usr/bin/../share/java/kafka/opencensus-api-0.21.0.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/jackson-module-paranamer-2.10.5.jar:/usr/bin/../share/java/kafka/jackson-databind-2.10.5.jar:/usr/bin/../share/java/kafka/confluent-resource-names-6.0.1-ce.jar:/usr/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka/annotations-3.0.1.jar:/usr/bin/../share/java/kafka/netty-tcnative-boringssl-static-2.0.30.Final.jar:/usr/bin/../share/java/kafka/jersey-media-jaxb-2.30.jar:/usr/bin/../share/java/kafka/protobuf-java-3.11.4.jar:/usr/bin/../share/java/confluent-metadata-service/concurrent-trees-2.6.1.jar:/usr/bin/../share/java/confluent-metadata-service/snappy-java-1.1.7.3.jar:/usr/bin/../share/java/confluent-metadata-service/bctls-fips-1.0.10.jar:/usr/bin/../share/java/confluent-metadata-service/confluent-security-plugins-common-6.0.1.jar:/usr/bin/../share/java/confluent-metadata-service/bc-fips-1.0.2.jar:/usr/bin/../share/java/confluent-metadata-service/telemetry-client-1.192.1.jar:/usr/bin/../share/java/confluent-metadata-service/cloudevents-api-1.3.0.jar:/usr/bin/../share/java/confluent-metadata-service/jsr305-3.0.2.jar:/usr/bin/../share/java/confluent-metadata-service/jetty-client-9.4.24.v20191120.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.el-api-3.0.3.jar:/usr/bin/../share/java/confluent-metadata-service/bcpkix-fips-1.0.3.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/confluent-metadata-service/classmate-1.3.4.jar:/usr/bin/../share/java/confluent-metadata-service/zstd-jni-1.4.4-7.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/usr/bin/../share/java/confluent-metadata-service/jose4j-0.7.2.jar:/usr/bin/../share/java/confluent-metadata-service/cloudevents-kafka-1.3.0.jar:/usr/bin/../share/java/confluent-metadata-service/jersey-client-2.30.jar:/usr/bin/../share/java/confluent-metadata-service/auth-providers-6.0.1-ce.jar:/usr/bin/../share/java/confluent-metadata-service/animal-sniffer-annotations-1.18.jar:/usr/bin/../share/java/confluent-metadata-service/lz4-java-1.7.1.jar:/usr/bin/../share/java/confluent-metadata-service/common-utils-6.0.1.jar:/usr/bin/../share/java/confluent-metadata-service/failureaccess-1.0.1.jar:/usr/bin/../share/java/confluent-metadata-service/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/confluent-metadata-service/error_prone_annotations-2.3.2.jar:/usr/bin/../share/java/confluent-metadata-service/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/confluent-metadata-service/jetty-proxy-9.4.24.v20191120.jar:/usr/bin/../share/java/confluent-metadata-service/rbac-6.0.1-ce.jar:/usr/bin/../share/java/confluent-metadata-service/kafka-client-plugins-6.0.1-ce.jar:/usr/bin/../share/java/confluent-metadata-service/telemetry-api-1.192.1.jar:/usr/bin/../share/java/confluent-metadata-service/guava-28.1-jre.jar:/usr/bin/../share/java/confluent-metadata-service/metrics-core-2.2.0.jar:/usr/bin/../share/java/confluent-metadata-service/gson-2.8.6.jar:/usr/bin/../share/java/confluent-metadata-service/jbcrypt-0.4.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.el-3.0.2.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-datatype-jsr310-2.10.5.jar:/usr/bin/../share/java/confluent-metadata-service/kafka-clients-6.0.1-ce.jar:/usr/bin/../share/java/confluent-metadata-service/jetty-util-9.4.24.v20191120.jar:/usr/bin/../share/java/confluent-metadata-service/protobuf-java-util-3.11.4.jar:/usr/bin/../share/java/confluent-metadata-service/jersey-server-2.30.jar:/usr/bin/../share/java/confluent-metadata-service/confluent-audit-6.0.1-ce.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-datatype-jdk8-2.10.5.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-core-2.10.5.jar:/usr/bin/../share/java/confluent-metadata-service/checker-qual-2.8.1.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/confluent-metadata-service/authorizer-6.0.1-ce.jar:/usr/bin/../share/java/confluent-metadata-service/j2objc-annotations-1.3.jar:/usr/bin/../share/java/confluent-metadata-service/jersey-common-2.30.jar:/usr/bin/../share/java/confluent-metadata-service/auto-value-annotations-1.7.2.jar:/usr/bin/../share/java/confluent-metadata-service/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/confluent-metadata-service/rest-authorizer-6.0.1-ce.jar:/usr/bin/../share/java/confluent-metadata-service/hibernate-validator-6.0.17.Final.jar:/usr/bin/../share/java/confluent-metadata-service/opencensus-proto-0.2.0.jar:/usr/bin/../share/java/confluent-metadata-service/jboss-logging-3.3.2.Final.jar:/usr/bin/../share/java/confluent-metadata-service/rbac-api-server-6.0.1.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/confluent-metadata-service/ce-kafka-http-server-6.0.1.jar:/usr/bin/../share/java/confluent-metadata-service/commons-lang3-3.8.1.jar:/usr/bin/../share/java/confluent-metadata-service/telemetry-events-6.0.1-ce.jar:/usr/bin/../share/java/confluent-metadata-service/jersey-bean-validation-2.30.jar:/usr/bin/../share/java/confluent-metadata-service/confluent-resource-names-6.0.1-ce.jar:/usr/bin/../share/java/confluent-metadata-service/annotations-3.0.1.jar:/usr/bin/../share/java/confluent-metadata-service/jersey-media-jaxb-2.30.jar:/usr/bin/../share/java/confluent-metadata-service/protobuf-java-3.11.4.jar:/usr/bin/../share/java/rest-utils/jetty-jmx-9.4.24.v20191120.jar:/usr/bin/../share/java/rest-utils/javassist-3.25.0-GA.jar:/usr/bin/../share/java/rest-utils/jersey-container-servlet-2.30.jar:/usr/bin/../share/java/rest-utils/snappy-java-1.1.7.3.jar:/usr/bin/../share/java/rest-utils/jackson-module-jaxb-annotations-2.10.5.jar:/usr/bin/../share/java/rest-utils/websocket-servlet-9.4.24.v20191120.jar:/usr/bin/../share/java/rest-utils/jetty-io-9.4.24.v20191120.jar:/usr/bin/../share/java/rest-utils/jetty-client-9.4.24.v20191120.jar:/usr/bin/../share/java/rest-utils/jakarta.el-api-3.0.3.jar:/usr/bin/../share/java/rest-utils/jetty-servlet-9.4.24.v20191120.jar:/usr/bin/../share/java/rest-utils/activation-1.1.1.jar:/usr/bin/../share/java/rest-utils/hk2-api-2.6.1.jar:/usr/bin/../share/java/rest-utils/javax.websocket-client-api-1.0.jar:/usr/bin/../share/java/rest-utils/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/rest-utils/classmate-1.3.4.jar:/usr/bin/../share/java/rest-utils/zstd-jni-1.4.4-7.jar:/usr/bin/../share/java/rest-utils/jackson-jaxrs-json-provider-2.10.5.jar:/usr/bin/../share/java/rest-utils/jetty-security-9.4.24.v20191120.jar:/usr/bin/../share/java/rest-utils/jersey-client-2.30.jar:/usr/bin/../share/java/rest-utils/jetty-continuation-9.4.24.v20191120.jar:/usr/bin/../share/java/rest-utils/jetty-webapp-9.4.24.v20191120.jar:/usr/bin/../share/java/rest-utils/hk2-locator-2.6.1.jar:/usr/bin/../share/java/rest-utils/asm-tree-7.2.jar:/usr/bin/../share/java/rest-utils/lz4-java-1.7.1.jar:/usr/bin/../share/java/rest-utils/javax-websocket-client-impl-9.4.24.v20191120.jar:/usr/bin/../share/java/rest-utils/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/rest-utils/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/rest-utils/jersey-hk2-2.30.jar:/usr/bin/../share/java/rest-utils/jackson-jaxrs-base-2.10.5.jar:/usr/bin/../share/java/rest-utils/rest-utils-6.0.1.jar:/usr/bin/../share/java/rest-utils/websocket-client-9.4.24.v20191120.jar:/usr/bin/../share/java/rest-utils/javax.websocket-api-1.0.jar:/usr/bin/../share/java/rest-utils/asm-analysis-7.2.jar:/usr/bin/../share/java/rest-utils/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/rest-utils/jetty-plus-9.4.24.v20191120.jar:/usr/bin/../share/java/rest-utils/jersey-container-servlet-core-2.30.jar:/usr/bin/../share/java/rest-utils/jakarta.el-3.0.2.jar:/usr/bin/../share/java/rest-utils/hk2-utils-2.6.1.jar:/usr/bin/../share/java/rest-utils/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/rest-utils/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/rest-utils/jetty-util-9.4.24.v20191120.jar:/usr/bin/../share/java/rest-utils/jersey-server-2.30.jar:/usr/bin/../share/java/rest-utils/websocket-api-9.4.24.v20191120.jar:/usr/bin/../share/java/rest-utils/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/rest-utils/jackson-core-2.10.5.jar:/usr/bin/../share/java/rest-utils/websocket-common-9.4.24.v20191120.jar:/usr/bin/../share/java/rest-utils/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/rest-utils/asm-commons-7.2.jar:/usr/bin/../share/java/rest-utils/jackson-annotations-2.10.5.jar:/usr/bin/../share/java/rest-utils/jetty-jndi-9.4.24.v20191120.jar:/usr/bin/../share/java/rest-utils/javax-websocket-server-impl-9.4.24.v20191120.jar:/usr/bin/../share/java/rest-utils/jetty-annotations-9.4.24.v20191120.jar:/usr/bin/../share/java/rest-utils/jersey-common-2.30.jar:/usr/bin/../share/java/rest-utils/jetty-xml-9.4.24.v20191120.jar:/usr/bin/../share/java/rest-utils/jetty-servlets-9.4.24.v20191120.jar:/usr/bin/../share/java/rest-utils/jetty-http-9.4.24.v20191120.jar:/usr/bin/../share/java/rest-utils/hibernate-validator-6.0.17.Final.jar:/usr/bin/../share/java/rest-utils/kafka-clients-6.0.1-ccs.jar:/usr/bin/../share/java/rest-utils/jetty-jaas-9.4.24.v20191120.jar:/usr/bin/../share/java/rest-utils/jboss-logging-3.3.2.Final.jar:/usr/bin/../share/java/rest-utils/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/rest-utils/jaxb-api-2.3.0.jar:/usr/bin/../share/java/rest-utils/jersey-bean-validation-2.30.jar:/usr/bin/../share/java/rest-utils/jetty-server-9.4.24.v20191120.jar:/usr/bin/../share/java/rest-utils/javax.annotation-api-1.3.jar:/usr/bin/../share/java/rest-utils/jackson-databind-2.10.5.jar:/usr/bin/../share/java/rest-utils/websocket-server-9.4.24.v20191120.jar:/usr/bin/../share/java/rest-utils/asm-7.2.jar:/usr/bin/../share/java/rest-utils/jersey-media-jaxb-2.30.jar:/usr/bin/../share/java/confluent-common/build-tools-6.0.1.jar:/usr/bin/../share/java/confluent-common/common-config-6.0.1.jar:/usr/bin/../share/java/confluent-common/common-utils-6.0.1.jar:/usr/bin/../share/java/confluent-common/common-metrics-6.0.1.jar:/usr/bin/../share/java/confluent-common/slf4j-api-1.7.30.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-jmx-9.4.24.v20191120.jar:/usr/bin/../share/java/ce-kafka-http-server/javassist-3.25.0-GA.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-container-servlet-2.30.jar:/usr/bin/../share/java/ce-kafka-http-server/snappy-java-1.1.7.3.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-module-jaxb-annotations-2.10.5.jar:/usr/bin/../share/java/ce-kafka-http-server/websocket-servlet-9.4.24.v20191120.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-io-9.4.24.v20191120.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-client-9.4.24.v20191120.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.el-api-3.0.3.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-servlet-9.4.24.v20191120.jar:/usr/bin/../share/java/ce-kafka-http-server/activation-1.1.1.jar:/usr/bin/../share/java/ce-kafka-http-server/hk2-api-2.6.1.jar:/usr/bin/../share/java/ce-kafka-http-server/javax.websocket-client-api-1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/ce-kafka-http-server/classmate-1.3.4.jar:/usr/bin/../share/java/ce-kafka-http-server/zstd-jni-1.4.4-7.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-jaxrs-json-provider-2.10.5.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-security-9.4.24.v20191120.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-client-2.30.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-continuation-9.4.24.v20191120.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-webapp-9.4.24.v20191120.jar:/usr/bin/../share/java/ce-kafka-http-server/hk2-locator-2.6.1.jar:/usr/bin/../share/java/ce-kafka-http-server/asm-tree-7.2.jar:/usr/bin/../share/java/ce-kafka-http-server/lz4-java-1.7.1.jar:/usr/bin/../share/java/ce-kafka-http-server/common-utils-6.0.1.jar:/usr/bin/../share/java/ce-kafka-http-server/javax-websocket-client-impl-9.4.24.v20191120.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/ce-kafka-http-server/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-hk2-2.30.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-jaxrs-base-2.10.5.jar:/usr/bin/../share/java/ce-kafka-http-server/rest-utils-6.0.1.jar:/usr/bin/../share/java/ce-kafka-http-server/websocket-client-9.4.24.v20191120.jar:/usr/bin/../share/java/ce-kafka-http-server/javax.websocket-api-1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/asm-analysis-7.2.jar:/usr/bin/../share/java/ce-kafka-http-server/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-plus-9.4.24.v20191120.jar:/usr/bin/../share/java/ce-kafka-http-server/slf4j-api-1.7.30.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-container-servlet-core-2.30.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.el-3.0.2.jar:/usr/bin/../share/java/ce-kafka-http-server/hk2-utils-2.6.1.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/ce-kafka-http-server/kafka-clients-6.0.1-ce.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-util-9.4.24.v20191120.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-server-2.30.jar:/usr/bin/../share/java/ce-kafka-http-server/websocket-api-9.4.24.v20191120.jar:/usr/bin/../share/java/ce-kafka-http-server/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-core-2.10.5.jar:/usr/bin/../share/java/ce-kafka-http-server/websocket-common-9.4.24.v20191120.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/ce-kafka-http-server/asm-commons-7.2.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-annotations-2.10.5.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-jndi-9.4.24.v20191120.jar:/usr/bin/../share/java/ce-kafka-http-server/javax-websocket-server-impl-9.4.24.v20191120.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-annotations-9.4.24.v20191120.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-common-2.30.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-xml-9.4.24.v20191120.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-servlets-9.4.24.v20191120.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-http-9.4.24.v20191120.jar:/usr/bin/../share/java/ce-kafka-http-server/hibernate-validator-6.0.17.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-jaas-9.4.24.v20191120.jar:/usr/bin/../share/java/ce-kafka-http-server/jboss-logging-3.3.2.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/ce-kafka-http-server/ce-kafka-http-server-6.0.1.jar:/usr/bin/../share/java/ce-kafka-http-server/jaxb-api-2.3.0.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-bean-validation-2.30.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-server-9.4.24.v20191120.jar:/usr/bin/../share/java/ce-kafka-http-server/javax.annotation-api-1.3.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-databind-2.10.5.jar:/usr/bin/../share/java/ce-kafka-http-server/websocket-server-9.4.24.v20191120.jar:/usr/bin/../share/java/ce-kafka-http-server/asm-7.2.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-media-jaxb-2.30.jar:/usr/bin/../share/java/ce-kafka-rest-servlet/ce-kafka-rest-servlet-6.0.1.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/snappy-java-1.1.7.3.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/zstd-jni-1.4.4-7.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/lz4-java-1.7.1.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/common-utils-6.0.1.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/slf4j-api-1.7.30.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/kafka-clients-6.0.1-ce.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/ce-kafka-rest-extensions-6.0.1.jar:/usr/bin/../share/java/kafka-rest-lib/snappy-java-1.1.7.3.jar:/usr/bin/../share/java/kafka-rest-lib/mbknor-jackson-jsonschema_2.13-1.0.39.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-rest-6.0.1.jar:/usr/bin/../share/java/kafka-rest-lib/commons-collections-3.2.2.jar:/usr/bin/../share/java/kafka-rest-lib/json-20190722.jar:/usr/bin/../share/java/kafka-rest-lib/jsr305-3.0.2.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-schema-serializer-6.0.1.jar:/usr/bin/../share/java/kafka-rest-lib/avro-1.9.2.jar:/usr/bin/../share/java/kafka-rest-lib/wire-runtime-3.2.2.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-script-runtime-1.3.50.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-scripting-compiler-embeddable-1.3.50.jar:/usr/bin/../share/java/kafka-rest-lib/spotbugs-annotations-4.0.2.jar:/usr/bin/../share/java/kafka-rest-lib/re2j-1.3.jar:/usr/bin/../share/java/kafka-rest-lib/zstd-jni-1.4.4-7.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-avro-serializer-6.0.1.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-json-serializer-6.0.1.jar:/usr/bin/../share/java/kafka-rest-lib/confluent-log4j-1.2.17-cp2.jar:/usr/bin/../share/java/kafka-rest-lib/commons-logging-1.2.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-protobuf-serializer-6.0.1.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-protobuf-provider-6.0.1.jar:/usr/bin/../share/java/kafka-rest-lib/animal-sniffer-annotations-1.18.jar:/usr/bin/../share/java/kafka-rest-lib/lz4-java-1.7.1.jar:/usr/bin/../share/java/kafka-rest-lib/common-utils-6.0.1.jar:/usr/bin/../share/java/kafka-rest-lib/failureaccess-1.0.1.jar:/usr/bin/../share/java/kafka-rest-lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/kafka-rest-lib/error_prone_annotations-2.3.2.jar:/usr/bin/../share/java/kafka-rest-lib/swagger-annotations-1.6.2.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-module-parameter-names-2.10.5.jar:/usr/bin/../share/java/kafka-rest-lib/handy-uri-templates-2.1.8.jar:/usr/bin/../share/java/kafka-rest-lib/org.everit.json.schema-1.12.1.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-datatype-guava-2.10.5.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-reflect-1.3.50.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-stdlib-1.4.0.jar:/usr/bin/../share/java/kafka-rest-lib/guava-28.1-jre.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-stdlib-jdk8-1.3.71.jar:/usr/bin/../share/java/kafka-rest-lib/slf4j-api-1.7.30.jar:/usr/bin/../share/java/kafka-rest-lib/gson-2.8.6.jar:/usr/bin/../share/java/kafka-rest-lib/classgraph-4.8.21.jar:/usr/bin/../share/java/kafka-rest-lib/validation-api-2.0.1.Final.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-datatype-jsr310-2.10.5.jar:/usr/bin/../share/java/kafka-rest-lib/annotations-13.0.jar:/usr/bin/../share/java/kafka-rest-lib/okio-2.5.0.jar:/usr/bin/../share/java/kafka-rest-lib/wire-schema-3.2.2.jar:/usr/bin/../share/java/kafka-rest-lib/protobuf-java-util-3.11.4.jar:/usr/bin/../share/java/kafka-rest-lib/commons-validator-1.6.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-json-schema-serializer-6.0.1.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-datatype-jdk8-2.10.5.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-core-2.10.5.jar:/usr/bin/../share/java/kafka-rest-lib/checker-qual-2.8.1.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-scripting-jvm-1.3.50.jar:/usr/bin/../share/java/kafka-rest-lib/scala-library-2.13.2.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-scripting-compiler-impl-embeddable-1.3.50.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-stdlib-common-1.4.0.jar:/usr/bin/../share/java/kafka-rest-lib/j2objc-annotations-1.3.jar:/usr/bin/../share/java/kafka-rest-lib/commons-compress-1.19.jar:/usr/bin/../share/java/kafka-rest-lib/auto-value-annotations-1.7.2.jar:/usr/bin/../share/java/kafka-rest-lib/kotlinx-coroutines-core-1.1.1.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-stdlib-jdk7-1.3.71.jar:/usr/bin/../share/java/kafka-rest-lib/joda-time-2.9.9.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-schema-registry-client-6.0.1.jar:/usr/bin/../share/java/kafka-rest-lib/commons-digester-1.8.1.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-clients-6.0.1-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-datatype-joda-2.10.5.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-scripting-common-1.3.50.jar:/usr/bin/../share/java/kafka-rest-lib/protobuf-java-3.11.4.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-json-schema-provider-6.0.1.jar:/usr/bin/../share/java/kafka-rest-lib/kotlinx-coroutines-core-common-1.1.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/javassist-3.25.0-GA.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-container-servlet-2.30.jar:/usr/bin/../share/java/confluent-security/kafka-rest/snappy-java-1.1.7.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/broker-plugins-6.0.1-ce-test.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-module-jaxb-annotations-2.10.5.jar:/usr/bin/../share/java/confluent-security/kafka-rest/confluent-security-plugins-common-6.0.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-io-9.4.24.v20191120.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jsr305-3.0.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-client-9.4.24.v20191120.jar:/usr/bin/../share/java/confluent-security/kafka-rest/avro-1.9.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-servlet-9.4.24.v20191120.jar:/usr/bin/../share/java/confluent-security/kafka-rest/activation-1.1.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/hk2-api-2.6.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/connect-runtime-6.0.1-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/zstd-jni-1.4.4-7.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jose4j-0.7.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/confluent-licensing-new-6.0.1-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-jaxrs-json-provider-2.10.5.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-security-9.4.24.v20191120.jar:/usr/bin/../share/java/confluent-security/kafka-rest/connect-json-6.0.1-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-client-2.30.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-continuation-9.4.24.v20191120.jar:/usr/bin/../share/java/confluent-security/kafka-rest/kafka-tools-6.0.1-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/animal-sniffer-annotations-1.18.jar:/usr/bin/../share/java/confluent-security/kafka-rest/hk2-locator-2.6.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/lz4-java-1.7.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/reflections-0.9.12.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/failureaccess-1.0.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/confluent-security/kafka-rest/error_prone_annotations-2.3.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-hk2-2.30.jar:/usr/bin/../share/java/confluent-security/kafka-rest/connect-api-6.0.1-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-jaxrs-base-2.10.5.jar:/usr/bin/../share/java/confluent-security/kafka-rest/kafka-client-plugins-6.0.1-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/swagger-annotations-1.6.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/kafka-log4j-appender-6.0.1-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/guava-28.1-jre.jar:/usr/bin/../share/java/confluent-security/kafka-rest/metrics-core-2.2.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/gson-2.8.6.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-container-servlet-core-2.30.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jbcrypt-0.4.jar:/usr/bin/../share/java/confluent-security/kafka-rest/confluent-serializers-new-6.0.1-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/hk2-utils-2.6.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/kafka-clients-6.0.1-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-util-9.4.24.v20191120.jar:/usr/bin/../share/java/confluent-security/kafka-rest/protobuf-java-util-3.11.4.jar:/usr/bin/../share/java/confluent-security/kafka-rest/maven-artifact-3.6.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-server-2.30.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-datatype-jdk8-2.10.5.jar:/usr/bin/../share/java/confluent-security/kafka-rest/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-core-2.10.5.jar:/usr/bin/../share/java/confluent-security/kafka-rest/checker-qual-2.8.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/confluent-security/kafka-rest/authorizer-6.0.1-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-annotations-2.10.5.jar:/usr/bin/../share/java/confluent-security/kafka-rest/j2objc-annotations-1.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/commons-compress-1.19.jar:/usr/bin/../share/java/confluent-security/kafka-rest/confluent-kafka-rest-security-plugin-6.0.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-common-2.30.jar:/usr/bin/../share/java/confluent-security/kafka-rest/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-servlets-9.4.24.v20191120.jar:/usr/bin/../share/java/confluent-security/kafka-rest/rest-authorizer-6.0.1-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-http-9.4.24.v20191120.jar:/usr/bin/../share/java/confluent-security/kafka-rest/kafka-schema-registry-client-6.0.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/connect-transforms-6.0.1-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/bcpkix-jdk15on-1.66.jar:/usr/bin/../share/java/confluent-security/kafka-rest/argparse4j-0.7.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/plexus-utils-3.2.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jaxb-api-2.3.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/commons-lang3-3.8.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-server-9.4.24.v20191120.jar:/usr/bin/../share/java/confluent-security/kafka-rest/bcprov-jdk15on-1.66.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-databind-2.10.5.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-media-jaxb-2.30.jar:/usr/bin/../share/java/confluent-security/kafka-rest/protobuf-java-3.11.4.jar:/usr/bin/../share/java/confluent-security/schema-validator/snappy-java-1.1.7.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/mbknor-jackson-jsonschema_2.13-1.0.39.jar:/usr/bin/../share/java/confluent-security/schema-validator/commons-collections-3.2.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/json-20190722.jar:/usr/bin/../share/java/confluent-security/schema-validator/jsr305-3.0.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-schema-serializer-6.0.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/avro-1.9.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/wire-runtime-3.2.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-script-runtime-1.3.50.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-scripting-compiler-embeddable-1.3.50.jar:/usr/bin/../share/java/confluent-security/schema-validator/re2j-1.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/zstd-jni-1.4.4-7.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-avro-serializer-6.0.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/commons-logging-1.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-protobuf-provider-6.0.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/animal-sniffer-annotations-1.18.jar:/usr/bin/../share/java/confluent-security/schema-validator/lz4-java-1.7.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/common-utils-6.0.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/failureaccess-1.0.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/confluent-security/schema-validator/error_prone_annotations-2.3.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/swagger-annotations-1.6.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-module-parameter-names-2.10.5.jar:/usr/bin/../share/java/confluent-security/schema-validator/handy-uri-templates-2.1.8.jar:/usr/bin/../share/java/confluent-security/schema-validator/org.everit.json.schema-1.12.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-datatype-guava-2.10.5.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-reflect-1.3.50.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-stdlib-1.4.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/guava-28.1-jre.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-stdlib-jdk8-1.3.71.jar:/usr/bin/../share/java/confluent-security/schema-validator/gson-2.8.6.jar:/usr/bin/../share/java/confluent-security/schema-validator/classgraph-4.8.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/validation-api-2.0.1.Final.jar:/usr/bin/../share/java/confluent-security/schema-validator/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-datatype-jsr310-2.10.5.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-clients-6.0.1-ce.jar:/usr/bin/../share/java/confluent-security/schema-validator/annotations-13.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/okio-2.5.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/wire-schema-3.2.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/protobuf-java-util-3.11.4.jar:/usr/bin/../share/java/confluent-security/schema-validator/commons-validator-1.6.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-datatype-jdk8-2.10.5.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-core-2.10.5.jar:/usr/bin/../share/java/confluent-security/schema-validator/checker-qual-2.8.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-scripting-jvm-1.3.50.jar:/usr/bin/../share/java/confluent-security/schema-validator/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/confluent-security/schema-validator/scala-library-2.13.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-scripting-compiler-impl-embeddable-1.3.50.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-annotations-2.10.5.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-stdlib-common-1.4.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/j2objc-annotations-1.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/commons-compress-1.19.jar:/usr/bin/../share/java/confluent-security/schema-validator/jersey-common-2.30.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlinx-coroutines-core-1.1.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-stdlib-jdk7-1.3.71.jar:/usr/bin/../share/java/confluent-security/schema-validator/confluent-schema-registry-validator-plugin-6.0.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/joda-time-2.9.9.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-schema-registry-client-6.0.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/commons-digester-1.8.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-datatype-joda-2.10.5.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-scripting-common-1.3.50.jar:/usr/bin/../share/java/confluent-security/schema-validator/caffeine-2.8.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-databind-2.10.5.jar:/usr/bin/../share/java/confluent-security/schema-validator/protobuf-java-3.11.4.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-json-schema-provider-6.0.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlinx-coroutines-core-common-1.1.1.jar:/usr/bin/../support-metrics-client/build/dependant-libs-2.13.2/*:/usr/bin/../support-metrics-client/build/libs/*:/usr/bin/../share/java/confluent-telemetry/confluent-metrics-6.0.1-ce.jar:/usr/share/java/support-metrics-client/* (org.apache.zookeeper.ZooKeeper)
[36mkafka1             |[0m [2021-01-09 19:50:14,698] INFO Client environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
[36mkafka1             |[0m [2021-01-09 19:50:14,700] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
[36mkafka1             |[0m [2021-01-09 19:50:14,700] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[36mkafka1             |[0m [2021-01-09 19:50:14,712] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
[36mkafka1             |[0m [2021-01-09 19:50:14,713] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[36mkafka1             |[0m [2021-01-09 19:50:14,713] INFO Client environment:os.version=4.19.121-linuxkit (org.apache.zookeeper.ZooKeeper)
[36mkafka1             |[0m [2021-01-09 19:50:14,714] INFO Client environment:user.name=appuser (org.apache.zookeeper.ZooKeeper)
[36mkafka1             |[0m [2021-01-09 19:50:14,715] INFO Client environment:user.home=/home/appuser (org.apache.zookeeper.ZooKeeper)
[36mkafka1             |[0m [2021-01-09 19:50:14,715] INFO Client environment:user.dir=/home/appuser (org.apache.zookeeper.ZooKeeper)
[36mkafka1             |[0m [2021-01-09 19:50:14,715] INFO Client environment:os.memory.free=972MB (org.apache.zookeeper.ZooKeeper)
[36mkafka1             |[0m [2021-01-09 19:50:14,716] INFO Client environment:os.memory.max=1024MB (org.apache.zookeeper.ZooKeeper)
[36mkafka1             |[0m [2021-01-09 19:50:14,716] INFO Client environment:os.memory.total=1024MB (org.apache.zookeeper.ZooKeeper)
[36mkafka1             |[0m [2021-01-09 19:50:14,742] INFO Initiating client connection, connectString=zookeeper:2181 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@5be82d43 (org.apache.zookeeper.ZooKeeper)
[36mkafka1             |[0m [2021-01-09 19:50:14,783] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)
[36mkafka1             |[0m [2021-01-09 19:50:14,823] INFO zookeeper.request.timeout value is 0. feature enabled= (org.apache.zookeeper.ClientCnxn)
[36mkafka1             |[0m [2021-01-09 19:50:14,851] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[36mkafka1             |[0m [2021-01-09 19:50:14,917] INFO Opening socket connection to server zookeeper/192.168.176.2:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[36mkafka1             |[0m [2021-01-09 19:50:14,968] INFO Socket connection established, initiating session, client: /192.168.176.3:43464, server: zookeeper/192.168.176.2:2181 (org.apache.zookeeper.ClientCnxn)
[36mkafka1             |[0m [2021-01-09 19:50:15,034] INFO Session establishment complete on server zookeeper/192.168.176.2:2181, sessionid = 0x10011273bc80005, negotiated timeout = 18000 (org.apache.zookeeper.ClientCnxn)
[36mkafka1             |[0m [2021-01-09 19:50:15,045] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)
[36mkafka1             |[0m [2021-01-09 19:50:16,035] INFO Cluster ID = VVoXKnO1RKKKeAWg2zUUqg (kafka.server.KafkaServer)
[36mkafka1             |[0m [2021-01-09 19:50:16,046] WARN No meta.properties file under dir /var/lib/kafka/data/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[36mkafka1             |[0m [2021-01-09 19:50:16,479] INFO KafkaConfig values:
[36mkafka1             |[0m 	advertised.host.name = null
[36mkafka1             |[0m 	advertised.listeners = PLAINTEXT://kafka1:19092,PLAINTEXT_HOST://kafka1:9092
[36mkafka1             |[0m 	advertised.port = null
[36mkafka1             |[0m 	alter.config.policy.class.name = null
[36mkafka1             |[0m 	alter.log.dirs.replication.quota.window.num = 11
[36mkafka1             |[0m 	alter.log.dirs.replication.quota.window.size.seconds = 1
[36mkafka1             |[0m 	authorizer.class.name =
[36mkafka1             |[0m 	auto.create.topics.enable = true
[36mkafka1             |[0m 	auto.leader.rebalance.enable = true
[36mkafka1             |[0m 	background.threads = 10
[36mkafka1             |[0m 	broker.id = 0
[36mkafka1             |[0m 	broker.id.generation.enable = true
[36mkafka1             |[0m 	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
[36mkafka1             |[0m 	broker.rack = null
[36mkafka1             |[0m 	broker.session.uuid = 6GhCwTukSJe6HKL1CSnVWA
[36mkafka1             |[0m 	client.quota.callback.class = null
[36mkafka1             |[0m 	compression.type = producer
[36mkafka1             |[0m 	confluent.append.record.interceptor.classes = []
[36mkafka1             |[0m 	confluent.apply.create.topic.policy.to.create.partitions = false
[36mkafka1             |[0m 	confluent.authorizer.authority.name =
[36mkafka1             |[0m 	confluent.backpressure.disk.enable = false
[36mkafka1             |[0m 	confluent.backpressure.disk.free.threshold.bytes = 21474836480
[36mkafka1             |[0m 	confluent.backpressure.disk.produce.bytes.per.second = 131072
[36mkafka1             |[0m 	confluent.backpressure.disk.threshold.recovery.factor = 1.5
[36mkafka1             |[0m 	confluent.backpressure.request.min.broker.limit = 200
[36mkafka1             |[0m 	confluent.backpressure.request.queue.size.percentile = p95
[36mkafka1             |[0m 	confluent.backpressure.types = null
[36mkafka1             |[0m 	confluent.balancer.api.state.topic = _confluent_balancer_api_state
[36mkafka1             |[0m 	confluent.balancer.class = io.confluent.databalancer.KafkaDataBalanceManager
[36mkafka1             |[0m 	confluent.balancer.disk.max.load = 0.85
[36mkafka1             |[0m 	confluent.balancer.enable = true
[36mkafka1             |[0m 	confluent.balancer.exclude.topic.names = []
[36mkafka1             |[0m 	confluent.balancer.exclude.topic.prefixes = []
[36mkafka1             |[0m 	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
[36mkafka1             |[0m 	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
[36mkafka1             |[0m 	confluent.balancer.max.replicas = 2147483647
[36mkafka1             |[0m 	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
[36mkafka1             |[0m 	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
[36mkafka1             |[0m 	confluent.balancer.task.history.retention.days = 30
[36mkafka1             |[0m 	confluent.balancer.throttle.bytes.per.second = 10485760
[36mkafka1             |[0m 	confluent.balancer.topic.replication.factor = 2
[36mkafka1             |[0m 	confluent.basic.auth.credentials.source = null
[36mkafka1             |[0m 	confluent.basic.auth.user.info = null
[36mkafka1             |[0m 	confluent.bearer.auth.credentials.source = null
[36mkafka1             |[0m 	confluent.bearer.auth.token = null
[36mkafka1             |[0m 	confluent.broker.registration.delay.ms = 0
[36mkafka1             |[0m 	confluent.cluster.link.enable = false
[36mkafka1             |[0m 	confluent.cluster.link.replication.quota.window.num = 11
[36mkafka1             |[0m 	confluent.cluster.link.replication.quota.window.size.seconds = 1
[36mkafka1             |[0m 	confluent.enable.stray.partition.deletion = false
[36mkafka1             |[0m 	confluent.http.server.start.timeout.ms = 30000
[36mkafka1             |[0m 	confluent.http.server.stop.timeout.ms = 30000
[36mkafka1             |[0m 	confluent.internal.rest.server.bind.port = null
[36mkafka1             |[0m 	confluent.log.placement.constraints =
[36mkafka1             |[0m 	confluent.metadata.server.cluster.registry.clusters = []
[36mkafka1             |[0m 	confluent.missing.id.cache.ttl.sec = 60
[36mkafka1             |[0m 	confluent.missing.id.query.range = 200
[36mkafka1             |[0m 	confluent.multitenant.listener.names = null
[36mkafka1             |[0m 	confluent.offsets.topic.placement.constraints =
[36mkafka1             |[0m 	confluent.operator.managed = false
[36mkafka1             |[0m 	confluent.prefer.tier.fetch.ms = -1
[36mkafka1             |[0m 	confluent.quota.tenant.broker.max.consumer.rate = 13107200
[36mkafka1             |[0m 	confluent.quota.tenant.broker.max.producer.rate = 13107200
[36mkafka1             |[0m 	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
[36mkafka1             |[0m 	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
[36mkafka1             |[0m 	confluent.reporters.telemetry.auto.enable = true
[36mkafka1             |[0m 	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
[36mkafka1             |[0m 	confluent.schema.registry.max.cache.size = 10000
[36mkafka1             |[0m 	confluent.schema.registry.max.retries = 1
[36mkafka1             |[0m 	confluent.schema.registry.retries.wait.ms = 0
[36mkafka1             |[0m 	confluent.schema.registry.url = http://schema-registry:8081
[36mkafka1             |[0m 	confluent.security.event.logger.authentication.enable = false
[36mkafka1             |[0m 	confluent.security.event.logger.enable = true
[36mkafka1             |[0m 	confluent.security.event.router.config =
[36mkafka1             |[0m 	confluent.segment.speculative.prefetch.enable = false
[36mkafka1             |[0m 	confluent.ssl.key.password = null
[36mkafka1             |[0m 	confluent.ssl.keystore.location = null
[36mkafka1             |[0m 	confluent.ssl.keystore.password = null
[36mkafka1             |[0m 	confluent.ssl.keystore.type = null
[36mkafka1             |[0m 	confluent.ssl.protocol = null
[36mkafka1             |[0m 	confluent.ssl.truststore.location = null
[36mkafka1             |[0m 	confluent.ssl.truststore.password = null
[36mkafka1             |[0m 	confluent.ssl.truststore.type = null
[36mkafka1             |[0m 	confluent.tier.archiver.num.threads = 2
[36mkafka1             |[0m 	confluent.tier.backend =
[36mkafka1             |[0m 	confluent.tier.enable = false
[36mkafka1             |[0m 	confluent.tier.feature = false
[36mkafka1             |[0m 	confluent.tier.fenced.segment.delete.delay.ms = 600000
[36mkafka1             |[0m 	confluent.tier.fetcher.memorypool.bytes = 0
[36mkafka1             |[0m 	confluent.tier.fetcher.num.threads = 4
[36mkafka1             |[0m 	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
[36mkafka1             |[0m 	confluent.tier.fetcher.offset.cache.period.ms = 60000
[36mkafka1             |[0m 	confluent.tier.fetcher.offset.cache.size = 200000
[36mkafka1             |[0m 	confluent.tier.gcs.bucket = null
[36mkafka1             |[0m 	confluent.tier.gcs.cred.file.path = null
[36mkafka1             |[0m 	confluent.tier.gcs.prefix =
[36mkafka1             |[0m 	confluent.tier.gcs.region = null
[36mkafka1             |[0m 	confluent.tier.gcs.write.chunk.size = 0
[36mkafka1             |[0m 	confluent.tier.local.hotset.bytes = -1
[36mkafka1             |[0m 	confluent.tier.local.hotset.ms = 86400000
[36mkafka1             |[0m 	confluent.tier.max.partition.fetch.bytes.override = 0
[36mkafka1             |[0m 	confluent.tier.metadata.bootstrap.servers = null
[36mkafka1             |[0m 	confluent.tier.metadata.max.poll.ms = 100
[36mkafka1             |[0m 	confluent.tier.metadata.namespace = null
[36mkafka1             |[0m 	confluent.tier.metadata.num.partitions = 50
[36mkafka1             |[0m 	confluent.tier.metadata.replication.factor = 3
[36mkafka1             |[0m 	confluent.tier.metadata.request.timeout.ms = 30000
[36mkafka1             |[0m 	confluent.tier.object.fetcher.num.threads = 1
[36mkafka1             |[0m 	confluent.tier.partition.state.commit.interval.ms = 15000
[36mkafka1             |[0m 	confluent.tier.s3.assumerole.arn = null
[36mkafka1             |[0m 	confluent.tier.s3.auto.abort.threshold.bytes = 500000
[36mkafka1             |[0m 	confluent.tier.s3.aws.endpoint.override = null
[36mkafka1             |[0m 	confluent.tier.s3.aws.signer.override = null
[36mkafka1             |[0m 	confluent.tier.s3.bucket = null
[36mkafka1             |[0m 	confluent.tier.s3.cred.file.path = null
[36mkafka1             |[0m 	confluent.tier.s3.prefix =
[36mkafka1             |[0m 	confluent.tier.s3.region = null
[36mkafka1             |[0m 	confluent.tier.s3.sse.algorithm = AES256
[36mkafka1             |[0m 	confluent.tier.s3.sse.customer.encryption.key = null
[36mkafka1             |[0m 	confluent.tier.segment.hotset.roll.min.bytes = 104857600
[36mkafka1             |[0m 	confluent.tier.topic.delete.check.interval.ms = 10800000
[36mkafka1             |[0m 	confluent.transaction.state.log.placement.constraints =
[36mkafka1             |[0m 	confluent.verify.group.subscription.prefix = false
[36mkafka1             |[0m 	connection.failed.authentication.delay.ms = 100
[36mkafka1             |[0m 	connections.max.idle.ms = 600000
[36mkafka1             |[0m 	connections.max.reauth.ms = 0
[36mkafka1             |[0m 	control.plane.listener.name = null
[36mkafka1             |[0m 	controlled.shutdown.enable = true
[36mkafka1             |[0m 	controlled.shutdown.max.retries = 3
[36mkafka1             |[0m 	controlled.shutdown.retry.backoff.ms = 5000
[36mkafka1             |[0m 	controller.socket.timeout.ms = 30000
[36mkafka1             |[0m 	create.topic.policy.class.name = null
[36mkafka1             |[0m 	default.replication.factor = 1
[36mkafka1             |[0m 	delegation.token.expiry.check.interval.ms = 3600000
[36mkafka1             |[0m 	delegation.token.expiry.time.ms = 86400000
[36mkafka1             |[0m 	delegation.token.master.key = null
[36mkafka1             |[0m 	delegation.token.max.lifetime.ms = 604800000
[36mkafka1             |[0m 	delete.records.purgatory.purge.interval.requests = 1
[36mkafka1             |[0m 	delete.topic.enable = true
[36mkafka1             |[0m 	enable.fips = false
[36mkafka1             |[0m 	fetch.max.bytes = 57671680
[36mkafka1             |[0m 	fetch.purgatory.purge.interval.requests = 1000
[36mkafka1             |[0m 	follower.replication.throttled.rate = 9223372036854775807
[36mkafka1             |[0m 	follower.replication.throttled.replicas = none
[36mkafka1             |[0m 	group.initial.rebalance.delay.ms = 0
[36mkafka1             |[0m 	group.max.session.timeout.ms = 1800000
[36mkafka1             |[0m 	group.max.size = 2147483647
[36mkafka1             |[0m 	group.min.session.timeout.ms = 6000
[36mkafka1             |[0m 	host.name =
[36mkafka1             |[0m 	inter.broker.listener.name = null
[36mkafka1             |[0m 	inter.broker.protocol.version = 2.6-IV0
[36mkafka1             |[0m 	kafka.metrics.polling.interval.secs = 10
[36mkafka1             |[0m 	kafka.metrics.reporters = []
[36mkafka1             |[0m 	leader.imbalance.check.interval.seconds = 300
[36mkafka1             |[0m 	leader.imbalance.per.broker.percentage = 10
[36mkafka1             |[0m 	leader.replication.throttled.rate = 9223372036854775807
[36mkafka1             |[0m 	leader.replication.throttled.replicas = none
[36mkafka1             |[0m 	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
[36mkafka1             |[0m 	listeners = PLAINTEXT://0.0.0.0:19092,PLAINTEXT_HOST://0.0.0.0:9092
[36mkafka1             |[0m 	log.cleaner.backoff.ms = 15000
[36mkafka1             |[0m 	log.cleaner.dedupe.buffer.size = 134217728
[36mkafka1             |[0m 	log.cleaner.delete.retention.ms = 86400000
[36mkafka1             |[0m 	log.cleaner.enable = true
[36mkafka1             |[0m 	log.cleaner.io.buffer.load.factor = 0.9
[36mkafka1             |[0m 	log.cleaner.io.buffer.size = 524288
[36mkafka1             |[0m 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
[36mkafka1             |[0m 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
[36mkafka1             |[0m 	log.cleaner.min.cleanable.ratio = 0.5
[36mkafka1             |[0m 	log.cleaner.min.compaction.lag.ms = 0
[36mkafka1             |[0m 	log.cleaner.threads = 1
[36mkafka1             |[0m 	log.cleanup.policy = [delete]
[36mkafka1             |[0m 	log.deletion.max.segments.per.run = 2147483647
[36mkafka1             |[0m 	log.dir = /tmp/kafka-logs
[36mkafka1             |[0m 	log.dirs = /var/lib/kafka/data
[36mkafka1             |[0m 	log.flush.interval.messages = 9223372036854775807
[36mkafka1             |[0m 	log.flush.interval.ms = null
[36mkafka1             |[0m 	log.flush.offset.checkpoint.interval.ms = 60000
[36mkafka1             |[0m 	log.flush.scheduler.interval.ms = 9223372036854775807
[36mkafka1             |[0m 	log.flush.start.offset.checkpoint.interval.ms = 60000
[36mkafka1             |[0m 	log.index.interval.bytes = 4096
[36mkafka1             |[0m 	log.index.size.max.bytes = 10485760
[36mkafka1             |[0m 	log.message.downconversion.enable = true
[36mkafka1             |[0m 	log.message.format.version = 2.6-IV0
[36mkafka1             |[0m 	log.message.timestamp.difference.max.ms = 9223372036854775807
[36mkafka1             |[0m 	log.message.timestamp.type = CreateTime
[36mkafka1             |[0m 	log.preallocate = false
[36mkafka1             |[0m 	log.retention.bytes = -1
[36mkafka1             |[0m 	log.retention.check.interval.ms = 300000
[36mkafka1             |[0m 	log.retention.hours = 168
[36mkafka1             |[0m 	log.retention.minutes = null
[36mkafka1             |[0m 	log.retention.ms = null
[36mkafka1             |[0m 	log.roll.hours = 168
[36mkafka1             |[0m 	log.roll.jitter.hours = 0
[36mkafka1             |[0m 	log.roll.jitter.ms = null
[36mkafka1             |[0m 	log.roll.ms = null
[36mkafka1             |[0m 	log.segment.bytes = 1073741824
[36mkafka1             |[0m 	log.segment.delete.delay.ms = 60000
[36mkafka1             |[0m 	max.connections = 2147483647
[36mkafka1             |[0m 	max.connections.per.ip = 2147483647
[36mkafka1             |[0m 	max.connections.per.ip.overrides =
[36mkafka1             |[0m 	max.incremental.fetch.session.cache.slots = 1000
[36mkafka1             |[0m 	message.max.bytes = 1048588
[36mkafka1             |[0m 	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
[36mkafka1             |[0m 	metrics.num.samples = 2
[36mkafka1             |[0m 	metrics.recording.level = INFO
[36mkafka1             |[0m 	metrics.sample.window.ms = 30000
[36mkafka1             |[0m 	min.insync.replicas = 1
[36mkafka1             |[0m 	multitenant.metadata.class = null
[36mkafka1             |[0m 	multitenant.metadata.dir = null
[36mkafka1             |[0m 	multitenant.metadata.reload.delay.ms = 120000
[36mkafka1             |[0m 	multitenant.metadata.ssl.certs.path = null
[36mkafka1             |[0m 	multitenant.tenant.delete.batch.size = 10
[36mkafka1             |[0m 	multitenant.tenant.delete.delay = 604800000
[36mkafka1             |[0m 	num.io.threads = 8
[36mkafka1             |[0m 	num.network.threads = 3
[36mkafka1             |[0m 	num.partitions = 1
[36mkafka1             |[0m 	num.recovery.threads.per.data.dir = 1
[36mkafka1             |[0m 	num.replica.alter.log.dirs.threads = null
[36mkafka1             |[0m 	num.replica.fetchers = 1
[36mkafka1             |[0m 	offset.metadata.max.bytes = 4096
[36mkafka1             |[0m 	offsets.commit.required.acks = -1
[36mkafka1             |[0m 	offsets.commit.timeout.ms = 5000
[36mkafka1             |[0m 	offsets.load.buffer.size = 5242880
[36mkafka1             |[0m 	offsets.retention.check.interval.ms = 600000
[36mkafka1             |[0m 	offsets.retention.minutes = 10080
[36mkafka1             |[0m 	offsets.topic.compression.codec = 0
[36mkafka1             |[0m 	offsets.topic.num.partitions = 50
[36mkafka1             |[0m 	offsets.topic.replication.factor = 2
[36mkafka1             |[0m 	offsets.topic.segment.bytes = 104857600
[36mkafka1             |[0m 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
[36mkafka1             |[0m 	password.encoder.iterations = 4096
[36mkafka1             |[0m 	password.encoder.key.length = 128
[36mkafka1             |[0m 	password.encoder.keyfactory.algorithm = null
[36mkafka1             |[0m 	password.encoder.old.secret = null
[36mkafka1             |[0m 	password.encoder.secret = null
[36mkafka1             |[0m 	port = 9092
[36mkafka1             |[0m 	principal.builder.class = null
[36mkafka1             |[0m 	producer.purgatory.purge.interval.requests = 1000
[36mkafka1             |[0m 	queued.max.request.bytes = -1
[36mkafka1             |[0m 	queued.max.requests = 500
[36mkafka1             |[0m 	quota.consumer.default = 9223372036854775807
[36mkafka1             |[0m 	quota.producer.default = 9223372036854775807
[36mkafka1             |[0m 	quota.window.num = 11
[36mkafka1             |[0m 	quota.window.size.seconds = 1
[36mkafka1             |[0m 	replica.fetch.backoff.ms = 1000
[36mkafka1             |[0m 	replica.fetch.max.bytes = 1048576
[36mkafka1             |[0m 	replica.fetch.min.bytes = 1
[36mkafka1             |[0m 	replica.fetch.response.max.bytes = 10485760
[36mkafka1             |[0m 	replica.fetch.wait.max.ms = 500
[36mkafka1             |[0m 	replica.high.watermark.checkpoint.interval.ms = 5000
[36mkafka1             |[0m 	replica.lag.time.max.ms = 30000
[36mkafka1             |[0m 	replica.selector.class = null
[36mkafka1             |[0m 	replica.socket.receive.buffer.bytes = 65536
[36mkafka1             |[0m 	replica.socket.timeout.ms = 30000
[36mkafka1             |[0m 	replication.quota.window.num = 11
[36mkafka1             |[0m 	replication.quota.window.size.seconds = 1
[36mkafka1             |[0m 	request.timeout.ms = 30000
[36mkafka1             |[0m 	reserved.broker.max.id = 1000
[36mkafka1             |[0m 	sasl.client.callback.handler.class = null
[36mkafka1             |[0m 	sasl.enabled.mechanisms = [GSSAPI]
[36mkafka1             |[0m 	sasl.jaas.config = null
[36mkafka1             |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mkafka1             |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mkafka1             |[0m 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
[36mkafka1             |[0m 	sasl.kerberos.service.name = null
[36mkafka1             |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mkafka1             |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mkafka1             |[0m 	sasl.login.callback.handler.class = null
[36mkafka1             |[0m 	sasl.login.class = null
[36mkafka1             |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mkafka1             |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mkafka1             |[0m 	sasl.login.refresh.window.factor = 0.8
[36mkafka1             |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mkafka1             |[0m 	sasl.mechanism.inter.broker.protocol = GSSAPI
[36mkafka1             |[0m 	sasl.server.callback.handler.class = null
[36mkafka1             |[0m 	security.inter.broker.protocol = PLAINTEXT
[36mkafka1             |[0m 	security.providers = null
[36mkafka1             |[0m 	socket.receive.buffer.bytes = 102400
[36mkafka1             |[0m 	socket.request.max.bytes = 104857600
[36mkafka1             |[0m 	socket.send.buffer.bytes = 102400
[36mkafka1             |[0m 	ssl.cipher.suites = []
[36mkafka1             |[0m 	ssl.client.auth = none
[36mkafka1             |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mkafka1             |[0m 	ssl.endpoint.identification.algorithm = https
[36mkafka1             |[0m 	ssl.engine.factory.class = null
[36mkafka1             |[0m 	ssl.key.password = null
[36mkafka1             |[0m 	ssl.keymanager.algorithm = SunX509
[36mkafka1             |[0m 	ssl.keystore.location = null
[36mkafka1             |[0m 	ssl.keystore.password = null
[36mkafka1             |[0m 	ssl.keystore.type = JKS
[36mkafka1             |[0m 	ssl.principal.mapping.rules = DEFAULT
[36mkafka1             |[0m 	ssl.protocol = TLSv1.3
[36mkafka1             |[0m 	ssl.provider = null
[36mkafka1             |[0m 	ssl.secure.random.implementation = null
[36mkafka1             |[0m 	ssl.trustmanager.algorithm = PKIX
[36mkafka1             |[0m 	ssl.truststore.location = null
[36mkafka1             |[0m 	ssl.truststore.password = null
[36mkafka1             |[0m 	ssl.truststore.type = JKS
[36mkafka1             |[0m 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
[36mkafka1             |[0m 	transaction.max.timeout.ms = 900000
[36mkafka1             |[0m 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
[36mkafka1             |[0m 	transaction.state.log.load.buffer.size = 5242880
[36mkafka1             |[0m 	transaction.state.log.min.isr = 1
[36mkafka1             |[0m 	transaction.state.log.num.partitions = 50
[36mkafka1             |[0m 	transaction.state.log.replication.factor = 2
[36mkafka1             |[0m 	transaction.state.log.segment.bytes = 104857600
[36mkafka1             |[0m 	transactional.id.expiration.ms = 604800000
[36mkafka1             |[0m 	unclean.leader.election.enable = false
[36mkafka1             |[0m 	zookeeper.clientCnxnSocket = null
[36mkafka1             |[0m 	zookeeper.connect = zookeeper:2181
[36mkafka1             |[0m 	zookeeper.connection.timeout.ms = null
[36mkafka1             |[0m 	zookeeper.max.in.flight.requests = 10
[36mkafka1             |[0m 	zookeeper.session.timeout.ms = 18000
[36mkafka1             |[0m 	zookeeper.set.acl = false
[36mkafka1             |[0m 	zookeeper.ssl.cipher.suites = null
[36mkafka1             |[0m 	zookeeper.ssl.client.enable = false
[36mkafka1             |[0m 	zookeeper.ssl.crl.enable = false
[36mkafka1             |[0m 	zookeeper.ssl.enabled.protocols = null
[36mkafka1             |[0m 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
[36mkafka1             |[0m 	zookeeper.ssl.keystore.location = null
[36mkafka1             |[0m 	zookeeper.ssl.keystore.password = null
[36mkafka1             |[0m 	zookeeper.ssl.keystore.type = null
[36mkafka1             |[0m 	zookeeper.ssl.ocsp.enable = false
[36mkafka1             |[0m 	zookeeper.ssl.protocol = TLSv1.2
[36mkafka1             |[0m 	zookeeper.ssl.truststore.location = null
[36mkafka1             |[0m 	zookeeper.ssl.truststore.password = null
[36mkafka1             |[0m 	zookeeper.ssl.truststore.type = null
[36mkafka1             |[0m 	zookeeper.sync.time.ms = 2000
[36mkafka1             |[0m  (kafka.server.KafkaConfig)
[36mkafka1             |[0m [2021-01-09 19:50:16,516] INFO KafkaConfig values:
[36mkafka1             |[0m 	advertised.host.name = null
[36mkafka1             |[0m 	advertised.listeners = PLAINTEXT://kafka1:19092,PLAINTEXT_HOST://kafka1:9092
[36mkafka1             |[0m 	advertised.port = null
[36mkafka1             |[0m 	alter.config.policy.class.name = null
[36mkafka1             |[0m 	alter.log.dirs.replication.quota.window.num = 11
[36mkafka1             |[0m 	alter.log.dirs.replication.quota.window.size.seconds = 1
[36mkafka1             |[0m 	authorizer.class.name =
[36mkafka1             |[0m 	auto.create.topics.enable = true
[36mkafka1             |[0m 	auto.leader.rebalance.enable = true
[36mkafka1             |[0m 	background.threads = 10
[36mkafka1             |[0m 	broker.id = 0
[36mkafka1             |[0m 	broker.id.generation.enable = true
[36mkafka1             |[0m 	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
[36mkafka1             |[0m 	broker.rack = null
[36mkafka1             |[0m 	broker.session.uuid = 6GhCwTukSJe6HKL1CSnVWA
[36mkafka1             |[0m 	client.quota.callback.class = null
[36mkafka1             |[0m 	compression.type = producer
[36mkafka1             |[0m 	confluent.append.record.interceptor.classes = []
[36mkafka1             |[0m 	confluent.apply.create.topic.policy.to.create.partitions = false
[36mkafka1             |[0m 	confluent.authorizer.authority.name =
[36mkafka1             |[0m 	confluent.backpressure.disk.enable = false
[36mkafka1             |[0m 	confluent.backpressure.disk.free.threshold.bytes = 21474836480
[36mkafka1             |[0m 	confluent.backpressure.disk.produce.bytes.per.second = 131072
[36mkafka1             |[0m 	confluent.backpressure.disk.threshold.recovery.factor = 1.5
[36mkafka1             |[0m 	confluent.backpressure.request.min.broker.limit = 200
[36mkafka1             |[0m 	confluent.backpressure.request.queue.size.percentile = p95
[36mkafka1             |[0m 	confluent.backpressure.types = null
[36mkafka1             |[0m 	confluent.balancer.api.state.topic = _confluent_balancer_api_state
[36mkafka1             |[0m 	confluent.balancer.class = io.confluent.databalancer.KafkaDataBalanceManager
[36mkafka1             |[0m 	confluent.balancer.disk.max.load = 0.85
[36mkafka1             |[0m 	confluent.balancer.enable = true
[36mkafka1             |[0m 	confluent.balancer.exclude.topic.names = []
[36mkafka1             |[0m 	confluent.balancer.exclude.topic.prefixes = []
[36mkafka1             |[0m 	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
[36mkafka1             |[0m 	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
[36mkafka1             |[0m 	confluent.balancer.max.replicas = 2147483647
[36mkafka1             |[0m 	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
[36mkafka1             |[0m 	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
[36mkafka1             |[0m 	confluent.balancer.task.history.retention.days = 30
[36mkafka1             |[0m 	confluent.balancer.throttle.bytes.per.second = 10485760
[36mkafka1             |[0m 	confluent.balancer.topic.replication.factor = 2
[36mkafka1             |[0m 	confluent.basic.auth.credentials.source = null
[36mkafka1             |[0m 	confluent.basic.auth.user.info = null
[36mkafka1             |[0m 	confluent.bearer.auth.credentials.source = null
[36mkafka1             |[0m 	confluent.bearer.auth.token = null
[36mkafka1             |[0m 	confluent.broker.registration.delay.ms = 0
[36mkafka1             |[0m 	confluent.cluster.link.enable = false
[36mkafka1             |[0m 	confluent.cluster.link.replication.quota.window.num = 11
[36mkafka1             |[0m 	confluent.cluster.link.replication.quota.window.size.seconds = 1
[36mkafka1             |[0m 	confluent.enable.stray.partition.deletion = false
[36mkafka1             |[0m 	confluent.http.server.start.timeout.ms = 30000
[36mkafka1             |[0m 	confluent.http.server.stop.timeout.ms = 30000
[36mkafka1             |[0m 	confluent.internal.rest.server.bind.port = null
[36mkafka1             |[0m 	confluent.log.placement.constraints =
[36mkafka1             |[0m 	confluent.metadata.server.cluster.registry.clusters = []
[36mkafka1             |[0m 	confluent.missing.id.cache.ttl.sec = 60
[36mkafka1             |[0m 	confluent.missing.id.query.range = 200
[36mkafka1             |[0m 	confluent.multitenant.listener.names = null
[36mkafka1             |[0m 	confluent.offsets.topic.placement.constraints =
[36mkafka1             |[0m 	confluent.operator.managed = false
[36mkafka1             |[0m 	confluent.prefer.tier.fetch.ms = -1
[36mkafka1             |[0m 	confluent.quota.tenant.broker.max.consumer.rate = 13107200
[36mkafka1             |[0m 	confluent.quota.tenant.broker.max.producer.rate = 13107200
[36mkafka1             |[0m 	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
[36mkafka1             |[0m 	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
[36mkafka1             |[0m 	confluent.reporters.telemetry.auto.enable = true
[36mkafka1             |[0m 	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
[36mkafka1             |[0m 	confluent.schema.registry.max.cache.size = 10000
[36mkafka1             |[0m 	confluent.schema.registry.max.retries = 1
[36mkafka1             |[0m 	confluent.schema.registry.retries.wait.ms = 0
[36mkafka1             |[0m 	confluent.schema.registry.url = http://schema-registry:8081
[36mkafka1             |[0m 	confluent.security.event.logger.authentication.enable = false
[36mkafka1             |[0m 	confluent.security.event.logger.enable = true
[36mkafka1             |[0m 	confluent.security.event.router.config =
[36mkafka1             |[0m 	confluent.segment.speculative.prefetch.enable = false
[36mkafka1             |[0m 	confluent.ssl.key.password = null
[36mkafka1             |[0m 	confluent.ssl.keystore.location = null
[36mkafka1             |[0m 	confluent.ssl.keystore.password = null
[36mkafka1             |[0m 	confluent.ssl.keystore.type = null
[36mkafka1             |[0m 	confluent.ssl.protocol = null
[36mkafka1             |[0m 	confluent.ssl.truststore.location = null
[36mkafka1             |[0m 	confluent.ssl.truststore.password = null
[36mkafka1             |[0m 	confluent.ssl.truststore.type = null
[36mkafka1             |[0m 	confluent.tier.archiver.num.threads = 2
[36mkafka1             |[0m 	confluent.tier.backend =
[36mkafka1             |[0m 	confluent.tier.enable = false
[36mkafka1             |[0m 	confluent.tier.feature = false
[36mkafka1             |[0m 	confluent.tier.fenced.segment.delete.delay.ms = 600000
[36mkafka1             |[0m 	confluent.tier.fetcher.memorypool.bytes = 0
[36mkafka1             |[0m 	confluent.tier.fetcher.num.threads = 4
[36mkafka1             |[0m 	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
[36mkafka1             |[0m 	confluent.tier.fetcher.offset.cache.period.ms = 60000
[36mkafka1             |[0m 	confluent.tier.fetcher.offset.cache.size = 200000
[36mkafka1             |[0m 	confluent.tier.gcs.bucket = null
[36mkafka1             |[0m 	confluent.tier.gcs.cred.file.path = null
[36mkafka1             |[0m 	confluent.tier.gcs.prefix =
[36mkafka1             |[0m 	confluent.tier.gcs.region = null
[36mkafka1             |[0m 	confluent.tier.gcs.write.chunk.size = 0
[36mkafka1             |[0m 	confluent.tier.local.hotset.bytes = -1
[36mkafka1             |[0m 	confluent.tier.local.hotset.ms = 86400000
[36mkafka1             |[0m 	confluent.tier.max.partition.fetch.bytes.override = 0
[36mkafka1             |[0m 	confluent.tier.metadata.bootstrap.servers = null
[36mkafka1             |[0m 	confluent.tier.metadata.max.poll.ms = 100
[36mkafka1             |[0m 	confluent.tier.metadata.namespace = null
[36mkafka1             |[0m 	confluent.tier.metadata.num.partitions = 50
[36mkafka1             |[0m 	confluent.tier.metadata.replication.factor = 3
[36mkafka1             |[0m 	confluent.tier.metadata.request.timeout.ms = 30000
[36mkafka1             |[0m 	confluent.tier.object.fetcher.num.threads = 1
[36mkafka1             |[0m 	confluent.tier.partition.state.commit.interval.ms = 15000
[36mkafka1             |[0m 	confluent.tier.s3.assumerole.arn = null
[36mkafka1             |[0m 	confluent.tier.s3.auto.abort.threshold.bytes = 500000
[36mkafka1             |[0m 	confluent.tier.s3.aws.endpoint.override = null
[36mkafka1             |[0m 	confluent.tier.s3.aws.signer.override = null
[36mkafka1             |[0m 	confluent.tier.s3.bucket = null
[36mkafka1             |[0m 	confluent.tier.s3.cred.file.path = null
[36mkafka1             |[0m 	confluent.tier.s3.prefix =
[36mkafka1             |[0m 	confluent.tier.s3.region = null
[36mkafka1             |[0m 	confluent.tier.s3.sse.algorithm = AES256
[36mkafka1             |[0m 	confluent.tier.s3.sse.customer.encryption.key = null
[36mkafka1             |[0m 	confluent.tier.segment.hotset.roll.min.bytes = 104857600
[36mkafka1             |[0m 	confluent.tier.topic.delete.check.interval.ms = 10800000
[36mkafka1             |[0m 	confluent.transaction.state.log.placement.constraints =
[36mkafka1             |[0m 	confluent.verify.group.subscription.prefix = false
[36mkafka1             |[0m 	connection.failed.authentication.delay.ms = 100
[36mkafka1             |[0m 	connections.max.idle.ms = 600000
[36mkafka1             |[0m 	connections.max.reauth.ms = 0
[36mkafka1             |[0m 	control.plane.listener.name = null
[36mkafka1             |[0m 	controlled.shutdown.enable = true
[36mkafka1             |[0m 	controlled.shutdown.max.retries = 3
[36mkafka1             |[0m 	controlled.shutdown.retry.backoff.ms = 5000
[36mkafka1             |[0m 	controller.socket.timeout.ms = 30000
[36mkafka1             |[0m 	create.topic.policy.class.name = null
[36mkafka1             |[0m 	default.replication.factor = 1
[36mkafka1             |[0m 	delegation.token.expiry.check.interval.ms = 3600000
[36mkafka1             |[0m 	delegation.token.expiry.time.ms = 86400000
[36mkafka1             |[0m 	delegation.token.master.key = null
[36mkafka1             |[0m 	delegation.token.max.lifetime.ms = 604800000
[36mkafka1             |[0m 	delete.records.purgatory.purge.interval.requests = 1
[36mkafka1             |[0m 	delete.topic.enable = true
[36mkafka1             |[0m 	enable.fips = false
[36mkafka1             |[0m 	fetch.max.bytes = 57671680
[36mkafka1             |[0m 	fetch.purgatory.purge.interval.requests = 1000
[36mkafka1             |[0m 	follower.replication.throttled.rate = 9223372036854775807
[36mkafka1             |[0m 	follower.replication.throttled.replicas = none
[36mkafka1             |[0m 	group.initial.rebalance.delay.ms = 0
[36mkafka1             |[0m 	group.max.session.timeout.ms = 1800000
[36mkafka1             |[0m 	group.max.size = 2147483647
[36mkafka1             |[0m 	group.min.session.timeout.ms = 6000
[36mkafka1             |[0m 	host.name =
[36mkafka1             |[0m 	inter.broker.listener.name = null
[36mkafka1             |[0m 	inter.broker.protocol.version = 2.6-IV0
[36mkafka1             |[0m 	kafka.metrics.polling.interval.secs = 10
[36mkafka1             |[0m 	kafka.metrics.reporters = []
[36mkafka1             |[0m 	leader.imbalance.check.interval.seconds = 300
[36mkafka1             |[0m 	leader.imbalance.per.broker.percentage = 10
[36mkafka1             |[0m 	leader.replication.throttled.rate = 9223372036854775807
[36mkafka1             |[0m 	leader.replication.throttled.replicas = none
[36mkafka1             |[0m 	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
[36mkafka1             |[0m 	listeners = PLAINTEXT://0.0.0.0:19092,PLAINTEXT_HOST://0.0.0.0:9092
[36mkafka1             |[0m 	log.cleaner.backoff.ms = 15000
[36mkafka1             |[0m 	log.cleaner.dedupe.buffer.size = 134217728
[36mkafka1             |[0m 	log.cleaner.delete.retention.ms = 86400000
[36mkafka1             |[0m 	log.cleaner.enable = true
[36mkafka1             |[0m 	log.cleaner.io.buffer.load.factor = 0.9
[36mkafka1             |[0m 	log.cleaner.io.buffer.size = 524288
[36mkafka1             |[0m 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
[36mkafka1             |[0m 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
[36mkafka1             |[0m 	log.cleaner.min.cleanable.ratio = 0.5
[36mkafka1             |[0m 	log.cleaner.min.compaction.lag.ms = 0
[36mkafka1             |[0m 	log.cleaner.threads = 1
[36mkafka1             |[0m 	log.cleanup.policy = [delete]
[36mkafka1             |[0m 	log.deletion.max.segments.per.run = 2147483647
[36mkafka1             |[0m 	log.dir = /tmp/kafka-logs
[36mkafka1             |[0m 	log.dirs = /var/lib/kafka/data
[36mkafka1             |[0m 	log.flush.interval.messages = 9223372036854775807
[36mkafka1             |[0m 	log.flush.interval.ms = null
[36mkafka1             |[0m 	log.flush.offset.checkpoint.interval.ms = 60000
[36mkafka1             |[0m 	log.flush.scheduler.interval.ms = 9223372036854775807
[36mkafka1             |[0m 	log.flush.start.offset.checkpoint.interval.ms = 60000
[36mkafka1             |[0m 	log.index.interval.bytes = 4096
[36mkafka1             |[0m 	log.index.size.max.bytes = 10485760
[36mkafka1             |[0m 	log.message.downconversion.enable = true
[36mkafka1             |[0m 	log.message.format.version = 2.6-IV0
[36mkafka1             |[0m 	log.message.timestamp.difference.max.ms = 9223372036854775807
[36mkafka1             |[0m 	log.message.timestamp.type = CreateTime
[36mkafka1             |[0m 	log.preallocate = false
[36mkafka1             |[0m 	log.retention.bytes = -1
[36mkafka1             |[0m 	log.retention.check.interval.ms = 300000
[36mkafka1             |[0m 	log.retention.hours = 168
[36mkafka1             |[0m 	log.retention.minutes = null
[36mkafka1             |[0m 	log.retention.ms = null
[36mkafka1             |[0m 	log.roll.hours = 168
[36mkafka1             |[0m 	log.roll.jitter.hours = 0
[36mkafka1             |[0m 	log.roll.jitter.ms = null
[36mkafka1             |[0m 	log.roll.ms = null
[36mkafka1             |[0m 	log.segment.bytes = 1073741824
[36mkafka1             |[0m 	log.segment.delete.delay.ms = 60000
[36mkafka1             |[0m 	max.connections = 2147483647
[36mkafka1             |[0m 	max.connections.per.ip = 2147483647
[36mkafka1             |[0m 	max.connections.per.ip.overrides =
[36mkafka1             |[0m 	max.incremental.fetch.session.cache.slots = 1000
[36mkafka1             |[0m 	message.max.bytes = 1048588
[36mkafka1             |[0m 	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
[36mkafka1             |[0m 	metrics.num.samples = 2
[36mkafka1             |[0m 	metrics.recording.level = INFO
[36mkafka1             |[0m 	metrics.sample.window.ms = 30000
[36mkafka1             |[0m 	min.insync.replicas = 1
[36mkafka1             |[0m 	multitenant.metadata.class = null
[36mkafka1             |[0m 	multitenant.metadata.dir = null
[36mkafka1             |[0m 	multitenant.metadata.reload.delay.ms = 120000
[36mkafka1             |[0m 	multitenant.metadata.ssl.certs.path = null
[36mkafka1             |[0m 	multitenant.tenant.delete.batch.size = 10
[36mkafka1             |[0m 	multitenant.tenant.delete.delay = 604800000
[36mkafka1             |[0m 	num.io.threads = 8
[36mkafka1             |[0m 	num.network.threads = 3
[36mkafka1             |[0m 	num.partitions = 1
[36mkafka1             |[0m 	num.recovery.threads.per.data.dir = 1
[36mkafka1             |[0m 	num.replica.alter.log.dirs.threads = null
[36mkafka1             |[0m 	num.replica.fetchers = 1
[36mkafka1             |[0m 	offset.metadata.max.bytes = 4096
[36mkafka1             |[0m 	offsets.commit.required.acks = -1
[36mkafka1             |[0m 	offsets.commit.timeout.ms = 5000
[36mkafka1             |[0m 	offsets.load.buffer.size = 5242880
[36mkafka1             |[0m 	offsets.retention.check.interval.ms = 600000
[36mkafka1             |[0m 	offsets.retention.minutes = 10080
[36mkafka1             |[0m 	offsets.topic.compression.codec = 0
[36mkafka1             |[0m 	offsets.topic.num.partitions = 50
[36mkafka1             |[0m 	offsets.topic.replication.factor = 2
[36mkafka1             |[0m 	offsets.topic.segment.bytes = 104857600
[36mkafka1             |[0m 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
[36mkafka1             |[0m 	password.encoder.iterations = 4096
[36mkafka1             |[0m 	password.encoder.key.length = 128
[36mkafka1             |[0m 	password.encoder.keyfactory.algorithm = null
[36mkafka1             |[0m 	password.encoder.old.secret = null
[36mkafka1             |[0m 	password.encoder.secret = null
[36mkafka1             |[0m 	port = 9092
[36mkafka1             |[0m 	principal.builder.class = null
[36mkafka1             |[0m 	producer.purgatory.purge.interval.requests = 1000
[36mkafka1             |[0m 	queued.max.request.bytes = -1
[36mkafka1             |[0m 	queued.max.requests = 500
[36mkafka1             |[0m 	quota.consumer.default = 9223372036854775807
[36mkafka1             |[0m 	quota.producer.default = 9223372036854775807
[36mkafka1             |[0m 	quota.window.num = 11
[36mkafka1             |[0m 	quota.window.size.seconds = 1
[36mkafka1             |[0m 	replica.fetch.backoff.ms = 1000
[36mkafka1             |[0m 	replica.fetch.max.bytes = 1048576
[36mkafka1             |[0m 	replica.fetch.min.bytes = 1
[36mkafka1             |[0m 	replica.fetch.response.max.bytes = 10485760
[36mkafka1             |[0m 	replica.fetch.wait.max.ms = 500
[36mkafka1             |[0m 	replica.high.watermark.checkpoint.interval.ms = 5000
[36mkafka1             |[0m 	replica.lag.time.max.ms = 30000
[36mkafka1             |[0m 	replica.selector.class = null
[36mkafka1             |[0m 	replica.socket.receive.buffer.bytes = 65536
[36mkafka1             |[0m 	replica.socket.timeout.ms = 30000
[36mkafka1             |[0m 	replication.quota.window.num = 11
[36mkafka1             |[0m 	replication.quota.window.size.seconds = 1
[36mkafka1             |[0m 	request.timeout.ms = 30000
[36mkafka1             |[0m 	reserved.broker.max.id = 1000
[36mkafka1             |[0m 	sasl.client.callback.handler.class = null
[36mkafka1             |[0m 	sasl.enabled.mechanisms = [GSSAPI]
[36mkafka1             |[0m 	sasl.jaas.config = null
[36mkafka1             |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mkafka1             |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mkafka1             |[0m 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
[36mkafka1             |[0m 	sasl.kerberos.service.name = null
[36mkafka1             |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mkafka1             |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mkafka1             |[0m 	sasl.login.callback.handler.class = null
[36mkafka1             |[0m 	sasl.login.class = null
[36mkafka1             |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mkafka1             |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mkafka1             |[0m 	sasl.login.refresh.window.factor = 0.8
[36mkafka1             |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mkafka1             |[0m 	sasl.mechanism.inter.broker.protocol = GSSAPI
[36mkafka1             |[0m 	sasl.server.callback.handler.class = null
[36mkafka1             |[0m 	security.inter.broker.protocol = PLAINTEXT
[36mkafka1             |[0m 	security.providers = null
[36mkafka1             |[0m 	socket.receive.buffer.bytes = 102400
[36mkafka1             |[0m 	socket.request.max.bytes = 104857600
[36mkafka1             |[0m 	socket.send.buffer.bytes = 102400
[36mkafka1             |[0m 	ssl.cipher.suites = []
[36mkafka1             |[0m 	ssl.client.auth = none
[36mkafka1             |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mkafka1             |[0m 	ssl.endpoint.identification.algorithm = https
[36mkafka1             |[0m 	ssl.engine.factory.class = null
[36mkafka1             |[0m 	ssl.key.password = null
[36mkafka1             |[0m 	ssl.keymanager.algorithm = SunX509
[36mkafka1             |[0m 	ssl.keystore.location = null
[36mkafka1             |[0m 	ssl.keystore.password = null
[36mkafka1             |[0m 	ssl.keystore.type = JKS
[36mkafka1             |[0m 	ssl.principal.mapping.rules = DEFAULT
[36mkafka1             |[0m 	ssl.protocol = TLSv1.3
[36mkafka1             |[0m 	ssl.provider = null
[36mkafka1             |[0m 	ssl.secure.random.implementation = null
[36mkafka1             |[0m 	ssl.trustmanager.algorithm = PKIX
[36mkafka1             |[0m 	ssl.truststore.location = null
[36mkafka1             |[0m 	ssl.truststore.password = null
[36mkafka1             |[0m 	ssl.truststore.type = JKS
[36mkafka1             |[0m 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
[36mkafka1             |[0m 	transaction.max.timeout.ms = 900000
[36mkafka1             |[0m 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
[36mkafka1             |[0m 	transaction.state.log.load.buffer.size = 5242880
[36mkafka1             |[0m 	transaction.state.log.min.isr = 1
[36mkafka1             |[0m 	transaction.state.log.num.partitions = 50
[36mkafka1             |[0m 	transaction.state.log.replication.factor = 2
[36mkafka1             |[0m 	transaction.state.log.segment.bytes = 104857600
[36mkafka1             |[0m 	transactional.id.expiration.ms = 604800000
[36mkafka1             |[0m 	unclean.leader.election.enable = false
[36mkafka1             |[0m 	zookeeper.clientCnxnSocket = null
[36mkafka1             |[0m 	zookeeper.connect = zookeeper:2181
[36mkafka1             |[0m 	zookeeper.connection.timeout.ms = null
[36mkafka1             |[0m 	zookeeper.max.in.flight.requests = 10
[36mkafka1             |[0m 	zookeeper.session.timeout.ms = 18000
[36mkafka1             |[0m 	zookeeper.set.acl = false
[36mkafka1             |[0m 	zookeeper.ssl.cipher.suites = null
[36mkafka1             |[0m 	zookeeper.ssl.client.enable = false
[36mkafka1             |[0m 	zookeeper.ssl.crl.enable = false
[36mkafka1             |[0m 	zookeeper.ssl.enabled.protocols = null
[36mkafka1             |[0m 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
[36mkafka1             |[0m 	zookeeper.ssl.keystore.location = null
[36mkafka1             |[0m 	zookeeper.ssl.keystore.password = null
[36mkafka1             |[0m 	zookeeper.ssl.keystore.type = null
[36mkafka1             |[0m 	zookeeper.ssl.ocsp.enable = false
[36mkafka1             |[0m 	zookeeper.ssl.protocol = TLSv1.2
[36mkafka1             |[0m 	zookeeper.ssl.truststore.location = null
[36mkafka1             |[0m 	zookeeper.ssl.truststore.password = null
[36mkafka1             |[0m 	zookeeper.ssl.truststore.type = null
[36mkafka1             |[0m 	zookeeper.sync.time.ms = 2000
[36mkafka1             |[0m  (kafka.server.KafkaConfig)
[36mkafka1             |[0m [2021-01-09 19:50:17,057] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[36mkafka1             |[0m [2021-01-09 19:50:17,062] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[36mkafka1             |[0m [2021-01-09 19:50:17,102] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[36mkafka1             |[0m [2021-01-09 19:50:17,109] INFO Empty logDirs received! Disk based throttling won't be activated! (kafka.server.DiskUsageBasedThrottlingConfig$)
[36mkafka1             |[0m [2021-01-09 19:50:17,120] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[36mkafka1             |[0m [2021-01-09 19:50:17,455] INFO Loading logs from log dirs ArraySeq(/var/lib/kafka/data) (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:17,466] INFO Attempting recovery for all logs in /var/lib/kafka/data since no clean shutdown file was found (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:17,480] INFO Loaded 0 logs in 24ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:17,588] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:17,658] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:17,792] INFO Starting the log cleaner (kafka.log.LogCleaner)
[36mkafka1             |[0m [2021-01-09 19:50:17,972] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner)
[36mkafka1             |[0m [2021-01-09 19:50:18,096] INFO AuditLogConfig values:
[36mkafka1             |[0m 	confluent.security.event.logger.authentication.enable = false
[36mkafka1             |[0m 	confluent.security.event.logger.cloudevent.codec = structured
[36mkafka1             |[0m 	confluent.security.event.logger.enable = true
[36mkafka1             |[0m 	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
[36mkafka1             |[0m 	confluent.security.event.logger.exporter.kafka.topic.create = true
[36mkafka1             |[0m 	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
[36mkafka1             |[0m 	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
[36mkafka1             |[0m 	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
[36mkafka1             |[0m 	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
[36mkafka1             |[0m 	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
[36mkafka1             |[0m 	confluent.security.event.router.cache.entries = 10000
[36mkafka1             |[0m 	confluent.security.event.router.config =
[36mkafka1             |[0m  (io.confluent.security.audit.AuditLogConfig)
[36mkafka1             |[0m [2021-01-09 19:50:18,098] INFO AuditLogConfig values:
[36mkafka1             |[0m 	confluent.security.event.logger.authentication.enable = false
[36mkafka1             |[0m 	confluent.security.event.logger.cloudevent.codec = structured
[36mkafka1             |[0m 	confluent.security.event.logger.enable = true
[36mkafka1             |[0m 	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
[36mkafka1             |[0m 	confluent.security.event.logger.exporter.kafka.topic.create = true
[36mkafka1             |[0m 	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
[36mkafka1             |[0m 	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
[36mkafka1             |[0m 	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
[36mkafka1             |[0m 	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
[36mkafka1             |[0m 	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
[36mkafka1             |[0m 	confluent.security.event.router.cache.entries = 10000
[36mkafka1             |[0m 	confluent.security.event.router.config =
[36mkafka1             |[0m  (io.confluent.security.audit.AuditLogConfig)
[36mkafka1             |[0m [2021-01-09 19:50:18,835] INFO CrnAuthorityConfig values:
[36mkafka1             |[0m 	confluent.authorizer.authority.cache.entries = 10000
[36mkafka1             |[0m 	confluent.authorizer.authority.name =
[36mkafka1             |[0m  (io.confluent.crn.CrnAuthorityConfig)
[36mkafka1             |[0m [2021-01-09 19:50:18,841] INFO CrnAuthorityConfig values:
[36mkafka1             |[0m 	confluent.authorizer.authority.cache.entries = 10000
[36mkafka1             |[0m 	confluent.authorizer.authority.name =
[36mkafka1             |[0m  (io.confluent.crn.CrnAuthorityConfig)
[36mkafka1             |[0m [2021-01-09 19:50:20,550] INFO Awaiting socket connections on 0.0.0.0:19092. (kafka.network.Acceptor)
[36mkafka1             |[0m [2021-01-09 19:50:20,851] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[36mkafka1             |[0m [2021-01-09 19:50:20,854] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[36mkafka1             |[0m [2021-01-09 19:50:21,080] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[36mkafka1             |[0m [2021-01-09 19:50:21,266] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:50:21,362] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:50:21,380] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:50:21,390] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:50:21,392] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:50:21,397] INFO [ExpirationReaper-0-ListOffsets]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:50:21,476] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[36mkafka1             |[0m [2021-01-09 19:50:22,340] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[36mkafka1             |[0m [2021-01-09 19:50:22,460] INFO Stat of the created znode at /brokers/ids/0 is: 66,66,1610221822433,1610221822433,1,0,0,72076454242287621,246,0,66
[36mkafka1             |[0m  (kafka.zk.KafkaZkClient)
[36mkafka1             |[0m [2021-01-09 19:50:22,471] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://kafka1:19092,PLAINTEXT_HOST://kafka1:9092, czxid (broker epoch): 66 (kafka.zk.KafkaZkClient)
[36mkafka1             |[0m [2021-01-09 19:50:22,890] INFO DataBalancer: attempting startup with io.confluent.databalancer.KafkaDataBalanceManager (kafka.controller.DataBalanceManager)
[36mkafka1             |[0m [2021-01-09 19:50:23,028] INFO Registering metric ActiveBalancerCount (io.confluent.databalancer.KafkaDataBalanceManager)
[36mkafka1             |[0m [2021-01-09 19:50:23,136] INFO [ControllerEventThread controllerId=0] Starting (kafka.controller.ControllerEventManager$ControllerEventThread)
[36mkafka1             |[0m [2021-01-09 19:50:23,244] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:50:23,247] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:50:23,295] DEBUG [Controller id=0] Broker 2 has been elected as the controller, so stopping the election process. (kafka.controller.KafkaController)
[36mkafka1             |[0m [2021-01-09 19:50:23,426] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[36mkafka1             |[0m [2021-01-09 19:50:23,439] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[36mkafka1             |[0m [2021-01-09 19:50:23,536] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 83 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:50:23,605] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:2000,blockEndProducerId:2999) by writing to Zk with path version 3 (kafka.coordinator.transaction.ProducerIdManager)
[36mkafka1             |[0m [2021-01-09 19:50:23,689] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[36mkafka1             |[0m [2021-01-09 19:50:23,693] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[36mkafka1             |[0m [2021-01-09 19:50:23,698] INFO MetadataServerConfig values:
[36mkafka1             |[0m 	confluent.http.server.listeners = [http://0.0.0.0:8090]
[36mkafka1             |[0m 	confluent.metadata.server.advertised.listeners = null
[36mkafka1             |[0m 	confluent.metadata.server.listeners = null
[36mkafka1             |[0m  (org.apache.kafka.server.http.MetadataServerConfig)
[36mkafka1             |[0m [2021-01-09 19:50:23,717] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[36mkafka1             |[0m [2021-01-09 19:50:23,736] WARN Could not find suitable MetadataServer implementation. (org.apache.kafka.server.http.MetadataServerFactory)
[36mkafka1             |[0m [2021-01-09 19:50:24,038] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:50:24,629] INFO ConfluentMetricsReporterConfig values:
[36mkafka1             |[0m 	confluent.metrics.reporter.bootstrap.servers = kafka1:9092
[36mkafka1             |[0m 	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec).*|.*(BytesFetchedRate).*
[36mkafka1             |[0m 	confluent.metrics.reporter.publish.ms = 15000
[36mkafka1             |[0m 	confluent.metrics.reporter.topic = _confluent-metrics
[36mkafka1             |[0m 	confluent.metrics.reporter.topic.create = true
[36mkafka1             |[0m 	confluent.metrics.reporter.topic.max.message.bytes = 10485760
[36mkafka1             |[0m 	confluent.metrics.reporter.topic.partitions = 12
[36mkafka1             |[0m 	confluent.metrics.reporter.topic.replicas = 2
[36mkafka1             |[0m 	confluent.metrics.reporter.topic.retention.bytes = -1
[36mkafka1             |[0m 	confluent.metrics.reporter.topic.retention.ms = 259200000
[36mkafka1             |[0m 	confluent.metrics.reporter.topic.roll.ms = 14400000
[36mkafka1             |[0m 	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
[36mkafka1             |[0m 	confluent.metrics.reporter.whitelist = null
[36mkafka1             |[0m  (io.confluent.metrics.reporter.ConfluentMetricsReporterConfig)
[36mkafka1             |[0m [2021-01-09 19:50:25,028] INFO ProducerConfig values:
[36mkafka1             |[0m 	acks = -1
[36mkafka1             |[0m 	batch.size = 16384
[36mkafka1             |[0m 	bootstrap.servers = [kafka1:9092]
[36mkafka1             |[0m 	buffer.memory = 33554432
[36mkafka1             |[0m 	client.dns.lookup = use_all_dns_ips
[36mkafka1             |[0m 	client.id = confluent-metrics-reporter
[36mkafka1             |[0m 	compression.type = lz4
[36mkafka1             |[0m 	connections.max.idle.ms = 540000
[36mkafka1             |[0m 	delivery.timeout.ms = 120000
[36mkafka1             |[0m 	enable.idempotence = false
[36mkafka1             |[0m 	interceptor.classes = []
[36mkafka1             |[0m 	internal.auto.downgrade.txn.commit = false
[36mkafka1             |[0m 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[36mkafka1             |[0m 	linger.ms = 500
[36mkafka1             |[0m 	max.block.ms = 60000
[36mkafka1             |[0m 	max.in.flight.requests.per.connection = 1
[36mkafka1             |[0m 	max.request.size = 10485760
[36mkafka1             |[0m 	metadata.max.age.ms = 300000
[36mkafka1             |[0m 	metadata.max.idle.ms = 300000
[36mkafka1             |[0m 	metric.reporters = []
[36mkafka1             |[0m 	metrics.num.samples = 2
[36mkafka1             |[0m 	metrics.recording.level = INFO
[36mkafka1             |[0m 	metrics.sample.window.ms = 30000
[36mkafka1             |[0m 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
[36mkafka1             |[0m 	receive.buffer.bytes = 32768
[36mkafka1             |[0m 	reconnect.backoff.max.ms = 1000
[36mkafka1             |[0m 	reconnect.backoff.ms = 50
[36mkafka1             |[0m 	request.timeout.ms = 30000
[36mkafka1             |[0m 	retries = 10
[36mkafka1             |[0m 	retry.backoff.ms = 500
[36mkafka1             |[0m 	sasl.client.callback.handler.class = null
[36mkafka1             |[0m 	sasl.jaas.config = null
[36mkafka1             |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mkafka1             |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mkafka1             |[0m 	sasl.kerberos.service.name = null
[36mkafka1             |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mkafka1             |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mkafka1             |[0m 	sasl.login.callback.handler.class = null
[36mkafka1             |[0m 	sasl.login.class = null
[36mkafka1             |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mkafka1             |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mkafka1             |[0m 	sasl.login.refresh.window.factor = 0.8
[36mkafka1             |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mkafka1             |[0m 	sasl.mechanism = GSSAPI
[36mkafka1             |[0m 	security.protocol = PLAINTEXT
[36mkafka1             |[0m 	security.providers = null
[36mkafka1             |[0m 	send.buffer.bytes = 131072
[36mkafka1             |[0m 	ssl.cipher.suites = null
[36mkafka1             |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mkafka1             |[0m 	ssl.endpoint.identification.algorithm = https
[36mkafka1             |[0m 	ssl.engine.factory.class = null
[36mkafka1             |[0m 	ssl.key.password = null
[36mkafka1             |[0m 	ssl.keymanager.algorithm = SunX509
[36mkafka1             |[0m 	ssl.keystore.location = null
[36mkafka1             |[0m 	ssl.keystore.password = null
[36mkafka1             |[0m 	ssl.keystore.type = JKS
[36mkafka1             |[0m 	ssl.protocol = TLSv1.3
[36mkafka1             |[0m 	ssl.provider = null
[36mkafka1             |[0m 	ssl.secure.random.implementation = null
[36mkafka1             |[0m 	ssl.trustmanager.algorithm = PKIX
[36mkafka1             |[0m 	ssl.truststore.location = null
[36mkafka1             |[0m 	ssl.truststore.password = null
[36mkafka1             |[0m 	ssl.truststore.type = JKS
[36mkafka1             |[0m 	transaction.timeout.ms = 60000
[36mkafka1             |[0m 	transactional.id = null
[36mkafka1             |[0m 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[36mkafka1             |[0m  (org.apache.kafka.clients.producer.ProducerConfig)
[36mkafka1             |[0m [2021-01-09 19:50:25,268] WARN The configuration 'topic.replicas' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mkafka1             |[0m [2021-01-09 19:50:25,275] INFO Kafka version: 6.0.1-ce (org.apache.kafka.common.utils.AppInfoParser)
[36mkafka1             |[0m [2021-01-09 19:50:25,276] INFO Kafka commitId: f75f566c7a4b38d8 (org.apache.kafka.common.utils.AppInfoParser)
[36mkafka1             |[0m [2021-01-09 19:50:25,308] INFO Kafka startTimeMs: 1610221825268 (org.apache.kafka.common.utils.AppInfoParser)
[36mkafka1             |[0m [2021-01-09 19:50:25,631] INFO Starting Confluent metrics reporter for cluster id VVoXKnO1RKKKeAWg2zUUqg with an interval of 15000 ms (io.confluent.metrics.reporter.ConfluentMetricsReporter)
[36mkafka1             |[0m [2021-01-09 19:50:26,222] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
[36mkafka1             |[0m [2021-01-09 19:50:26,848] WARN Ignoring redefinition of existing telemetry label kafka.version (io.confluent.shaded.io.confluent.telemetry.ResourceBuilderFacade)
[36mkafka1             |[0m [2021-01-09 19:50:26,914] INFO ConfluentTelemetryConfig values:
[36mkafka1             |[0m 	confluent.telemetry.api.key = null
[36mkafka1             |[0m 	confluent.telemetry.api.secret = null
[36mkafka1             |[0m 	confluent.telemetry.debug.enabled = false
[36mkafka1             |[0m 	confluent.telemetry.enabled = false
[36mkafka1             |[0m 	confluent.telemetry.events.collector.include = .*auto.create.topics.enable.*|.*broker.id.*|.*compression.type.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.tier.s3.region.*|.*delete.topic.enable.*|.*leader.imbalance.check.interval.seconds.*|.*leader.imbalance.per.broker.percentage.*|.*log.dir.*|.*log.dirs.*|.*log.flush.interval.messages.*|.*log.flush.interval.ms.*|.*log.flush.offset.checkpoint.interval.ms.*|.*log.flush.scheduler.interval.ms.*|.*log.flush.start.offset.checkpoint.interval.ms.*|.*log.retention.bytes.*|.*log.retention.hours.*|.*log.retention.minutes.*|.*log.retention.ms.*|.*log.roll.hours.*|.*log.roll.ms.*|.*log.segment.bytes.*|.*log.segment.delete.delay.ms.*|.*message.max.bytes.*|.*min.insync.replicas.*|.*num.io.threads.*|.*num.network.threads.*|.*num.recovery.threads.per.data.dir.*|.*num.replica.alter.log.dirs.threads.*|.*num.replica.fetchers.*|.*offset.metadata.max.bytes.*|.*offsets.commit.required.acks.*|.*offsets.commit.timeout.ms.*|.*offsets.load.buffer.size.*|.*offsets.retention.check.interval.ms.*|.*offsets.retention.minutes.*|.*offsets.topic.compression.codec.*|.*offsets.topic.num.partitions.*|.*offsets.topic.replication.factor.*|.*queued.max.requests.*|.*replica.fetch.min.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*request.timeout.ms.*|.*socket.receive.buffer.bytes.*|.*socket.request.max.bytes.*|.*socket.send.buffer.bytes.*|.*transacation.*.*|.*unclean.leader.election.enable.*|.*zookeeper.connection.timeout.ms.*|.*zookeeper.max.in.flight.requests.*|.*zookeeper.session.timeout.ms.*|.*zookeeper.set.acl.*|.*broker.id.generation.enable.*|.*broker.rack.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*connections.max.idle.ms.*|.*connections.max.reauth.ms.*|.*controlled.*.*|.*controller.socket.timeout.ms.*|.*default.replication.factor.*|.*delegation.token.expiry.time.ms.*|.*delegation.token.max.lifetime.ms.*|.*delete.records.purgatory.purge.interval.requests.*|.*fetch.*.*|.*group.*.*|.*log.cleaner.*.*|.*log.index.*.*|.*log.message.*.*|.*log.preallocate.*|.*log.retention.check.interval.ms.*|.*max.*.*|.*num.partitions.*|.*principal.builder.class.*|.*producer.purgatory.purge.interval.requests.*|.*queued.max.request.bytes.*|.*replica.*.*|.*reserved.broker.max.id.*|.*sasl.client.callback.handler.class.*|.*sasl.enabled.mechanisms.*|.*sasl.login.class.*|.*sasl.mechanism.inter.broker.protocol.*|.*sasl.server.callback.handler.class.*|.*security.inter.broker.protocol.*|.*ssl.client.auth.*|.*ssl.enabled.protocols.*|.*ssl.engine.builder.class.*|.*ssl.protocol.*|.*ssl.provider.*|.*zookeeper.clientCnxnSocket.*|.*zookeeper.ssl.client.enable.*|.*alter.*.*|.*authorizer.class.name.*|.*client.quota.callback.class.*|.*confluent.log.placement.constraints.*|.*confluent.tier.topic.delete.check.interval.ms.*|.*connection.failed.authentication.delay.ms.*|.*create.topic.policy.class.name.*|.*kafka.metrics.polling.interval.secs.*|.*kafka.metrics.reporters.*|.*listener.security.protocol.map.*|.*log.message.downconversion.enable.*|.*metric.reporters.*|.*metrics.*.*|.*password.encoder.cipher.algorithm.*|.*password.encoder.iterations.*|.*password.encoder.key.length.*|.*password.encoder.keyfactory.algorithm.*|.*quota.*.*|.*replication.*.*|.*security.providers.*|.*ssl.endpoint.identification.algorithm.*|.*enable.fips.*
[36mkafka1             |[0m 	confluent.telemetry.events.enable = false
[36mkafka1             |[0m 	confluent.telemetry.metrics.collector.include = .*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*|.*io.confluent.kafka.server/.*(confluent_audit/audit_log_fallback_rate_per_minute|confluent_audit/audit_log_rate_per_minute|confluent_authorizer/authorization_request_rate_per_minute|confluent_authorizer/authorization_allowed_rate_per_minute|confluent_authorizer/authorization_denied_rate_per_minute|confluent_auth_store/rbac_role_bindings_count|confluent_auth_store/rbac_access_rules_count|confluent_auth_store/acl_access_rules_count).*|.*io.confluent.kafka.server/.*(acl_authorizer/zookeeper_disconnects/total/delta|acl_authorizer/zookeeper_expires/total/delta|broker_failure/zookeeper_disconnects/total/delta|broker_failure/zookeeper_expires/total/delta|broker_topic/bytes_in/total/delta|broker_topic/bytes_out/total/delta|controller/active_controller_count|controller/leader_election_rate_and_time_ms|controller/offline_partitions_count|controller/unclean_leader_elections/total|controller_channel/connection_close_rate|controller_channel/connection_close_total|controller_channel/connection_count|controller_channel/connection_creation_rate|controller_channel/connection_creation_total|controller_channel/request_size_avg|controller_channel/request_size_max|controller_channel_manager/queue_size|controller_channel_manager/total_queue_size|controller_event_manager/event_queue_size|delayed_operation_purgatory/purgatory_size|executor/zookeeper_disconnects/total/delta|executor/zookeeper_expires/total/delta|fetch/queue_size|group_coordinator/partition_load_time_max|replica_alter_log_dirs_manager/max_lag|replica_fetcher/request_size_avg|replica_fetcher/request_size_max|replica_fetcher_manager/max_lag|replica_manager/isr_shrinks|replica_manager/leader_count|replica_manager/partition_count|replica_manager/under_min_isr_partition_count|replica_manager/under_replicated_partitions|request/errors/total/delta|request/local_time_ms/time/delta|request/local_time_ms/total/delta|request/queue_size|request/remote_time_ms/time/delta|request/remote_time_ms/total/delta|request/request_queue_time_ms/time/delta|request/request_queue_time_ms/total/delta|request/requests|request/response_queue_time_ms/time/delta|request/response_queue_time_ms/total/delta|request/response_send_time_ms/time/delta|request/response_send_time_ms/total/delta|request/total_time_ms/time/delta|request/total_time_ms/total/delta|request_channel/request_queue_size|request_channel/response_queue_size|request_handler_pool/request_handler_avg_idle_percent|session_expire_listener/zookeeper_disconnects/total/delta|session_expire_listener/zookeeper_expires/total/delta|socket_server/connections|socket_server/network_processor_avg_idle_percent|socket_server/request_size_avg|socket_server/request_size_max).*
[36mkafka1             |[0m 	confluent.telemetry.metrics.collector.interval.ms = 60000
[36mkafka1             |[0m 	confluent.telemetry.proxy.password = null
[36mkafka1             |[0m 	confluent.telemetry.proxy.url = null
[36mkafka1             |[0m 	confluent.telemetry.proxy.username = null
[36mkafka1             |[0m  (io.confluent.telemetry.ConfluentTelemetryConfig)
[36mkafka1             |[0m [2021-01-09 19:50:26,920] INFO VolumeMetricsCollectorConfig values:
[36mkafka1             |[0m 	confluent.telemetry.metrics.collector.volume.update.ms = 15000
[36mkafka1             |[0m  (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
[36mkafka1             |[0m [2021-01-09 19:50:26,934] INFO HttpExporterConfig values:
[36mkafka1             |[0m 	api.key = null
[36mkafka1             |[0m 	api.secret = null
[36mkafka1             |[0m 	buffer.batch.duration.max.ms = null
[36mkafka1             |[0m 	buffer.batch.items.max = null
[36mkafka1             |[0m 	buffer.inflight.submissions.max = null
[36mkafka1             |[0m 	buffer.pending.batches.max = null
[36mkafka1             |[0m 	client.attempts.max = null
[36mkafka1             |[0m 	client.base.url = https://collector.telemetry.confluent.cloud
[36mkafka1             |[0m 	client.compression = null
[36mkafka1             |[0m 	client.connect.timeout.ms = null
[36mkafka1             |[0m 	client.request.timeout.ms = null
[36mkafka1             |[0m 	client.retry.delay.seconds = null
[36mkafka1             |[0m 	enabled = false
[36mkafka1             |[0m 	metrics.include = .*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*|.*io.confluent.kafka.server/.*(confluent_audit/audit_log_fallback_rate_per_minute|confluent_audit/audit_log_rate_per_minute|confluent_authorizer/authorization_request_rate_per_minute|confluent_authorizer/authorization_allowed_rate_per_minute|confluent_authorizer/authorization_denied_rate_per_minute|confluent_auth_store/rbac_role_bindings_count|confluent_auth_store/rbac_access_rules_count|confluent_auth_store/acl_access_rules_count).*|.*io.confluent.kafka.server/.*(acl_authorizer/zookeeper_disconnects/total/delta|acl_authorizer/zookeeper_expires/total/delta|broker_failure/zookeeper_disconnects/total/delta|broker_failure/zookeeper_expires/total/delta|broker_topic/bytes_in/total/delta|broker_topic/bytes_out/total/delta|controller/active_controller_count|controller/leader_election_rate_and_time_ms|controller/offline_partitions_count|controller/unclean_leader_elections/total|controller_channel/connection_close_rate|controller_channel/connection_close_total|controller_channel/connection_count|controller_channel/connection_creation_rate|controller_channel/connection_creation_total|controller_channel/request_size_avg|controller_channel/request_size_max|controller_channel_manager/queue_size|controller_channel_manager/total_queue_size|controller_event_manager/event_queue_size|delayed_operation_purgatory/purgatory_size|executor/zookeeper_disconnects/total/delta|executor/zookeeper_expires/total/delta|fetch/queue_size|group_coordinator/partition_load_time_max|replica_alter_log_dirs_manager/max_lag|replica_fetcher/request_size_avg|replica_fetcher/request_size_max|replica_fetcher_manager/max_lag|replica_manager/isr_shrinks|replica_manager/leader_count|replica_manager/partition_count|replica_manager/under_min_isr_partition_count|replica_manager/under_replicated_partitions|request/errors/total/delta|request/local_time_ms/time/delta|request/local_time_ms/total/delta|request/queue_size|request/remote_time_ms/time/delta|request/remote_time_ms/total/delta|request/request_queue_time_ms/time/delta|request/request_queue_time_ms/total/delta|request/requests|request/response_queue_time_ms/time/delta|request/response_queue_time_ms/total/delta|request/response_send_time_ms/time/delta|request/response_send_time_ms/total/delta|request/total_time_ms/time/delta|request/total_time_ms/total/delta|request_channel/request_queue_size|request_channel/response_queue_size|request_handler_pool/request_handler_avg_idle_percent|session_expire_listener/zookeeper_disconnects/total/delta|session_expire_listener/zookeeper_expires/total/delta|socket_server/connections|socket_server/network_processor_avg_idle_percent|socket_server/request_size_avg|socket_server/request_size_max).*
[36mkafka1             |[0m 	proxy.password = null
[36mkafka1             |[0m 	proxy.url = null
[36mkafka1             |[0m 	proxy.username = null
[36mkafka1             |[0m 	type = http
[36mkafka1             |[0m  (io.confluent.telemetry.exporter.http.HttpExporterConfig)
[36mkafka1             |[0m [2021-01-09 19:50:26,955] INFO KafkaExporterConfig values:
[36mkafka1             |[0m 	enabled = true
[36mkafka1             |[0m 	metrics.include = .*bytes_in.*|.*bytes_out.*|.*process_cpu_load.*|.*local_time_ms.*|.*log_flush_rate_and_time_ms.*|.*messages_in.*|.*request_handler_avg_idle_percent.*|.*requests.*|.*request_queue_size.*|.*request_queue_time_ms.*|.*response_queue_size.*|.*/log/size|.*total_fetch_requests.*|.*total_produce_requests.*|.*total_time_ms.*|.*replication_bytes_in.*|.*replication_bytes_out.*|.*disk_total_bytes.*
[36mkafka1             |[0m 	producer.bootstrap.servers = kafka1:19092
[36mkafka1             |[0m 	topic.create = true
[36mkafka1             |[0m 	topic.max.message.bytes = 10485760
[36mkafka1             |[0m 	topic.name = _confluent-telemetry-metrics
[36mkafka1             |[0m 	topic.partitions = 12
[36mkafka1             |[0m 	topic.replicas = 2
[36mkafka1             |[0m 	topic.retention.bytes = -1
[36mkafka1             |[0m 	topic.retention.ms = 259200000
[36mkafka1             |[0m 	topic.roll.ms = 14400000
[36mkafka1             |[0m 	type = kafka
[36mkafka1             |[0m  (io.confluent.telemetry.exporter.kafka.KafkaExporterConfig)
[36mkafka1             |[0m [2021-01-09 19:50:27,194] INFO Creating kafka exporter named '_local' (io.confluent.telemetry.reporter.TelemetryReporter)
[36mkafka1             |[0m [2021-01-09 19:50:27,479] INFO ProducerConfig values:
[36mkafka1             |[0m 	acks = -1
[36mkafka1             |[0m 	batch.size = 16384
[36mkafka1             |[0m 	bootstrap.servers = [kafka1:19092]
[36mkafka1             |[0m 	buffer.memory = 33554432
[36mkafka1             |[0m 	client.dns.lookup = use_all_dns_ips
[36mkafka1             |[0m 	client.id = confluent-telemetry-reporter-local-producer
[36mkafka1             |[0m 	compression.type = lz4
[36mkafka1             |[0m 	connections.max.idle.ms = 540000
[36mkafka1             |[0m 	delivery.timeout.ms = 120000
[36mkafka1             |[0m 	enable.idempotence = false
[36mkafka1             |[0m 	interceptor.classes = []
[36mkafka1             |[0m 	internal.auto.downgrade.txn.commit = false
[36mkafka1             |[0m 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[36mkafka1             |[0m 	linger.ms = 500
[36mkafka1             |[0m 	max.block.ms = 60000
[36mkafka1             |[0m 	max.in.flight.requests.per.connection = 1
[36mkafka1             |[0m 	max.request.size = 10485760
[36mkafka1             |[0m 	metadata.max.age.ms = 300000
[36mkafka1             |[0m 	metadata.max.idle.ms = 300000
[36mkafka1             |[0m 	metric.reporters = []
[36mkafka1             |[0m 	metrics.num.samples = 2
[36mkafka1             |[0m 	metrics.recording.level = INFO
[36mkafka1             |[0m 	metrics.sample.window.ms = 30000
[36mkafka1             |[0m 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
[36mkafka1             |[0m 	receive.buffer.bytes = 32768
[36mkafka1             |[0m 	reconnect.backoff.max.ms = 1000
[36mkafka1             |[0m 	reconnect.backoff.ms = 50
[36mkafka1             |[0m 	request.timeout.ms = 30000
[36mkafka1             |[0m 	retries = 10
[36mkafka1             |[0m 	retry.backoff.ms = 500
[36mkafka1             |[0m 	sasl.client.callback.handler.class = null
[36mkafka1             |[0m 	sasl.jaas.config = null
[36mkafka1             |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mkafka1             |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mkafka1             |[0m 	sasl.kerberos.service.name = null
[36mkafka1             |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mkafka1             |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mkafka1             |[0m 	sasl.login.callback.handler.class = null
[36mkafka1             |[0m 	sasl.login.class = null
[36mkafka1             |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mkafka1             |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mkafka1             |[0m 	sasl.login.refresh.window.factor = 0.8
[36mkafka1             |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mkafka1             |[0m 	sasl.mechanism = GSSAPI
[36mkafka1             |[0m 	security.protocol = PLAINTEXT
[36mkafka1             |[0m 	security.providers = null
[36mkafka1             |[0m 	send.buffer.bytes = 131072
[36mkafka1             |[0m 	ssl.cipher.suites = null
[36mkafka1             |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mkafka1             |[0m 	ssl.endpoint.identification.algorithm = https
[36mkafka1             |[0m 	ssl.engine.factory.class = null
[36mkafka1             |[0m 	ssl.key.password = null
[36mkafka1             |[0m 	ssl.keymanager.algorithm = SunX509
[36mkafka1             |[0m 	ssl.keystore.location = null
[36mkafka1             |[0m 	ssl.keystore.password = null
[36mkafka1             |[0m 	ssl.keystore.type = JKS
[36mkafka1             |[0m 	ssl.protocol = TLSv1.3
[36mkafka1             |[0m 	ssl.provider = null
[36mkafka1             |[0m 	ssl.secure.random.implementation = null
[36mkafka1             |[0m 	ssl.trustmanager.algorithm = PKIX
[36mkafka1             |[0m 	ssl.truststore.location = null
[36mkafka1             |[0m 	ssl.truststore.password = null
[36mkafka1             |[0m 	ssl.truststore.type = JKS
[36mkafka1             |[0m 	transaction.timeout.ms = 60000
[36mkafka1             |[0m 	transactional.id = null
[36mkafka1             |[0m 	value.serializer = class io.confluent.telemetry.serde.OpencensusMetricsProto
[36mkafka1             |[0m  (org.apache.kafka.clients.producer.ProducerConfig)
[36mkafka1             |[0m [2021-01-09 19:50:27,523] INFO Kafka version: 6.0.1-ce (org.apache.kafka.common.utils.AppInfoParser)
[36mkafka1             |[0m [2021-01-09 19:50:27,523] INFO Kafka commitId: f75f566c7a4b38d8 (org.apache.kafka.common.utils.AppInfoParser)
[36mkafka1             |[0m [2021-01-09 19:50:27,524] INFO Kafka startTimeMs: 1610221827523 (org.apache.kafka.common.utils.AppInfoParser)
[36mkafka1             |[0m [2021-01-09 19:50:27,902] INFO Starting Confluent telemetry reporter with an interval of 60000 ms for resource = (type = kafka) (io.confluent.telemetry.MetricsCollectorTask)
[36mkafka1             |[0m [2021-01-09 19:50:27,999] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[36mkafka1             |[0m [2021-01-09 19:50:28,099] INFO [SocketServer brokerId=0] Starting socket server acceptors and processors (kafka.network.SocketServer)
[36mkafka1             |[0m [2021-01-09 19:50:28,247] INFO [SocketServer brokerId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[36mkafka1             |[0m [2021-01-09 19:50:28,260] INFO [SocketServer brokerId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[36mkafka1             |[0m [2021-01-09 19:50:28,261] INFO [SocketServer brokerId=0] Started socket server acceptors and processors (kafka.network.SocketServer)
[36mkafka1             |[0m [2021-01-09 19:50:28,427] INFO KafkaHttpServerConfig values:
[36mkafka1             |[0m 	access.control.allow.headers =
[36mkafka1             |[0m 	access.control.allow.methods =
[36mkafka1             |[0m 	access.control.allow.origin =
[36mkafka1             |[0m 	access.control.skip.options = true
[36mkafka1             |[0m 	authentication.method = NONE
[36mkafka1             |[0m 	authentication.realm =
[36mkafka1             |[0m 	authentication.roles = [*]
[36mkafka1             |[0m 	authentication.skip.paths = []
[36mkafka1             |[0m 	compression.enable = true
[36mkafka1             |[0m 	debug = false
[36mkafka1             |[0m 	idle.timeout.ms = 30000
[36mkafka1             |[0m 	listeners = [http://0.0.0.0:8090]
[36mkafka1             |[0m 	metric.reporters = []
[36mkafka1             |[0m 	metrics.jmx.prefix = rest-utils
[36mkafka1             |[0m 	metrics.num.samples = 2
[36mkafka1             |[0m 	metrics.sample.window.ms = 30000
[36mkafka1             |[0m 	metrics.tag.map = []
[36mkafka1             |[0m 	port = 8090
[36mkafka1             |[0m 	request.logger.name = io.confluent.rest-utils.requests
[36mkafka1             |[0m 	request.queue.capacity = 2147483647
[36mkafka1             |[0m 	request.queue.capacity.growby = 64
[36mkafka1             |[0m 	request.queue.capacity.init = 128
[36mkafka1             |[0m 	resource.extension.classes = []
[36mkafka1             |[0m 	response.http.headers.config =
[36mkafka1             |[0m 	response.mediatype.default = application/json
[36mkafka1             |[0m 	response.mediatype.preferred = [application/json]
[36mkafka1             |[0m 	rest.servlet.initializor.classes = []
[36mkafka1             |[0m 	shutdown.graceful.ms = 1000
[36mkafka1             |[0m 	ssl.cipher.suites = []
[36mkafka1             |[0m 	ssl.client.auth = false
[36mkafka1             |[0m 	ssl.client.authentication = NONE
[36mkafka1             |[0m 	ssl.enabled.protocols = []
[36mkafka1             |[0m 	ssl.endpoint.identification.algorithm = null
[36mkafka1             |[0m 	ssl.key.password = [hidden]
[36mkafka1             |[0m 	ssl.keymanager.algorithm =
[36mkafka1             |[0m 	ssl.keystore.location =
[36mkafka1             |[0m 	ssl.keystore.password = [hidden]
[36mkafka1             |[0m 	ssl.keystore.reload = false
[36mkafka1             |[0m 	ssl.keystore.type = JKS
[36mkafka1             |[0m 	ssl.keystore.watch.location =
[36mkafka1             |[0m 	ssl.protocol = TLS
[36mkafka1             |[0m 	ssl.provider =
[36mkafka1             |[0m 	ssl.trustmanager.algorithm =
[36mkafka1             |[0m 	ssl.truststore.location =
[36mkafka1             |[0m 	ssl.truststore.password = [hidden]
[36mkafka1             |[0m 	ssl.truststore.type = JKS
[36mkafka1             |[0m 	thread.pool.max = 200
[36mkafka1             |[0m 	thread.pool.min = 8
[36mkafka1             |[0m 	websocket.path.prefix = /ws
[36mkafka1             |[0m 	websocket.servlet.initializor.classes = []
[36mkafka1             |[0m  (io.confluent.http.server.KafkaHttpServerConfig)
[36mkafka1             |[0m [2021-01-09 19:50:28,646] INFO [Producer clientId=confluent-metrics-reporter] Cluster ID: VVoXKnO1RKKKeAWg2zUUqg (org.apache.kafka.clients.Metadata)
[36mkafka1             |[0m [2021-01-09 19:50:28,647] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Cluster ID: VVoXKnO1RKKKeAWg2zUUqg (org.apache.kafka.clients.Metadata)
[36mkafka1             |[0m [2021-01-09 19:50:28,673] INFO RestConfig values:
[36mkafka1             |[0m 	access.control.allow.headers =
[36mkafka1             |[0m 	access.control.allow.methods =
[36mkafka1             |[0m 	access.control.allow.origin =
[36mkafka1             |[0m 	access.control.skip.options = true
[36mkafka1             |[0m 	authentication.method = NONE
[36mkafka1             |[0m 	authentication.realm =
[36mkafka1             |[0m 	authentication.roles = [*]
[36mkafka1             |[0m 	authentication.skip.paths = []
[36mkafka1             |[0m 	compression.enable = true
[36mkafka1             |[0m 	debug = false
[36mkafka1             |[0m 	idle.timeout.ms = 30000
[36mkafka1             |[0m 	listeners = []
[36mkafka1             |[0m 	metric.reporters = []
[36mkafka1             |[0m 	metrics.jmx.prefix = rest-utils
[36mkafka1             |[0m 	metrics.num.samples = 2
[36mkafka1             |[0m 	metrics.sample.window.ms = 30000
[36mkafka1             |[0m 	metrics.tag.map = []
[36mkafka1             |[0m 	port = 8080
[36mkafka1             |[0m 	request.logger.name = io.confluent.rest-utils.requests
[36mkafka1             |[0m 	request.queue.capacity = 2147483647
[36mkafka1             |[0m 	request.queue.capacity.growby = 64
[36mkafka1             |[0m 	request.queue.capacity.init = 128
[36mkafka1             |[0m 	resource.extension.classes = []
[36mkafka1             |[0m 	response.http.headers.config =
[36mkafka1             |[0m 	response.mediatype.default = application/json
[36mkafka1             |[0m 	response.mediatype.preferred = [application/json]
[36mkafka1             |[0m 	rest.servlet.initializor.classes = []
[36mkafka1             |[0m 	shutdown.graceful.ms = 1000
[36mkafka1             |[0m 	ssl.cipher.suites = []
[36mkafka1             |[0m 	ssl.client.auth = false
[36mkafka1             |[0m 	ssl.client.authentication = NONE
[36mkafka1             |[0m 	ssl.enabled.protocols = []
[36mkafka1             |[0m 	ssl.endpoint.identification.algorithm = null
[36mkafka1             |[0m 	ssl.key.password = [hidden]
[36mkafka1             |[0m 	ssl.keymanager.algorithm =
[36mkafka1             |[0m 	ssl.keystore.location =
[36mkafka1             |[0m 	ssl.keystore.password = [hidden]
[36mkafka1             |[0m 	ssl.keystore.reload = false
[36mkafka1             |[0m 	ssl.keystore.type = JKS
[36mkafka1             |[0m 	ssl.keystore.watch.location =
[36mkafka1             |[0m 	ssl.protocol = TLS
[36mkafka1             |[0m 	ssl.provider =
[36mkafka1             |[0m 	ssl.trustmanager.algorithm =
[36mkafka1             |[0m 	ssl.truststore.location =
[36mkafka1             |[0m 	ssl.truststore.password = [hidden]
[36mkafka1             |[0m 	ssl.truststore.type = JKS
[36mkafka1             |[0m 	thread.pool.max = 200
[36mkafka1             |[0m 	thread.pool.min = 8
[36mkafka1             |[0m 	websocket.path.prefix = /ws
[36mkafka1             |[0m 	websocket.servlet.initializor.classes = []
[36mkafka1             |[0m  (io.confluent.rest.RestConfig)
[36mkafka1             |[0m [2021-01-09 19:50:28,733] INFO Logging initialized @24879ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log)
[36mkafka1             |[0m [2021-01-09 19:50:28,761] INFO Loaded Application class io.confluent.metadataapi.app.MetadataApiApplication. (io.confluent.http.server.KafkaHttpApplicationLoader)
[36mkafka1             |[0m [2021-01-09 19:50:28,765] INFO Application from provider RbacApplicationProvider is disabled. Skipping. (io.confluent.http.server.KafkaHttpApplicationLoader)
[36mkafka1             |[0m [2021-01-09 19:50:28,803] INFO KafkaConfig values:
[36mkafka1             |[0m 	advertised.host.name = null
[36mkafka1             |[0m 	advertised.listeners = PLAINTEXT://kafka1:19092,PLAINTEXT_HOST://kafka1:9092
[36mkafka1             |[0m 	advertised.port = null
[36mkafka1             |[0m 	alter.config.policy.class.name = null
[36mkafka1             |[0m 	alter.log.dirs.replication.quota.window.num = 11
[36mkafka1             |[0m 	alter.log.dirs.replication.quota.window.size.seconds = 1
[36mkafka1             |[0m 	authorizer.class.name =
[36mkafka1             |[0m 	auto.create.topics.enable = true
[36mkafka1             |[0m 	auto.leader.rebalance.enable = true
[36mkafka1             |[0m 	background.threads = 10
[36mkafka1             |[0m 	broker.id = 0
[36mkafka1             |[0m 	broker.id.generation.enable = true
[36mkafka1             |[0m 	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
[36mkafka1             |[0m 	broker.rack = null
[36mkafka1             |[0m 	broker.session.uuid = 6GhCwTukSJe6HKL1CSnVWA
[36mkafka1             |[0m 	client.quota.callback.class = null
[36mkafka1             |[0m 	compression.type = producer
[36mkafka1             |[0m 	confluent.append.record.interceptor.classes = []
[36mkafka1             |[0m 	confluent.apply.create.topic.policy.to.create.partitions = false
[36mkafka1             |[0m 	confluent.authorizer.authority.name =
[36mkafka1             |[0m 	confluent.backpressure.disk.enable = false
[36mkafka1             |[0m 	confluent.backpressure.disk.free.threshold.bytes = 21474836480
[36mkafka1             |[0m 	confluent.backpressure.disk.produce.bytes.per.second = 131072
[36mkafka1             |[0m 	confluent.backpressure.disk.threshold.recovery.factor = 1.5
[36mkafka1             |[0m 	confluent.backpressure.request.min.broker.limit = 200
[36mkafka1             |[0m 	confluent.backpressure.request.queue.size.percentile = p95
[36mkafka1             |[0m 	confluent.backpressure.types = null
[36mkafka1             |[0m 	confluent.balancer.api.state.topic = _confluent_balancer_api_state
[36mkafka1             |[0m 	confluent.balancer.class = io.confluent.databalancer.KafkaDataBalanceManager
[36mkafka1             |[0m 	confluent.balancer.disk.max.load = 0.85
[36mkafka1             |[0m 	confluent.balancer.enable = true
[36mkafka1             |[0m 	confluent.balancer.exclude.topic.names = []
[36mkafka1             |[0m 	confluent.balancer.exclude.topic.prefixes = []
[36mkafka1             |[0m 	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
[36mkafka1             |[0m 	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
[36mkafka1             |[0m 	confluent.balancer.max.replicas = 2147483647
[36mkafka1             |[0m 	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
[36mkafka1             |[0m 	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
[36mkafka1             |[0m 	confluent.balancer.task.history.retention.days = 30
[36mkafka1             |[0m 	confluent.balancer.throttle.bytes.per.second = 10485760
[36mkafka1             |[0m 	confluent.balancer.topic.replication.factor = 2
[36mkafka1             |[0m 	confluent.basic.auth.credentials.source = null
[36mkafka1             |[0m 	confluent.basic.auth.user.info = null
[36mkafka1             |[0m 	confluent.bearer.auth.credentials.source = null
[36mkafka1             |[0m 	confluent.bearer.auth.token = null
[36mkafka1             |[0m 	confluent.broker.registration.delay.ms = 0
[36mkafka1             |[0m 	confluent.cluster.link.enable = false
[36mkafka1             |[0m 	confluent.cluster.link.replication.quota.window.num = 11
[36mkafka1             |[0m 	confluent.cluster.link.replication.quota.window.size.seconds = 1
[36mkafka1             |[0m 	confluent.enable.stray.partition.deletion = false
[36mkafka1             |[0m 	confluent.http.server.start.timeout.ms = 30000
[36mkafka1             |[0m 	confluent.http.server.stop.timeout.ms = 30000
[36mkafka1             |[0m 	confluent.internal.rest.server.bind.port = null
[36mkafka1             |[0m 	confluent.log.placement.constraints =
[36mkafka1             |[0m 	confluent.metadata.server.cluster.registry.clusters = []
[36mkafka1             |[0m 	confluent.missing.id.cache.ttl.sec = 60
[36mkafka1             |[0m 	confluent.missing.id.query.range = 200
[36mkafka1             |[0m 	confluent.multitenant.listener.names = null
[36mkafka1             |[0m 	confluent.offsets.topic.placement.constraints =
[36mkafka1             |[0m 	confluent.operator.managed = false
[36mkafka1             |[0m 	confluent.prefer.tier.fetch.ms = -1
[36mkafka1             |[0m 	confluent.quota.tenant.broker.max.consumer.rate = 13107200
[36mkafka1             |[0m 	confluent.quota.tenant.broker.max.producer.rate = 13107200
[36mkafka1             |[0m 	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
[36mkafka1             |[0m 	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
[36mkafka1             |[0m 	confluent.reporters.telemetry.auto.enable = true
[36mkafka1             |[0m 	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
[36mkafka1             |[0m 	confluent.schema.registry.max.cache.size = 10000
[36mkafka1             |[0m 	confluent.schema.registry.max.retries = 1
[36mkafka1             |[0m 	confluent.schema.registry.retries.wait.ms = 0
[36mkafka1             |[0m 	confluent.schema.registry.url = http://schema-registry:8081
[36mkafka1             |[0m 	confluent.security.event.logger.authentication.enable = false
[36mkafka1             |[0m 	confluent.security.event.logger.enable = true
[36mkafka1             |[0m 	confluent.security.event.router.config =
[36mkafka1             |[0m 	confluent.segment.speculative.prefetch.enable = false
[36mkafka1             |[0m 	confluent.ssl.key.password = null
[36mkafka1             |[0m 	confluent.ssl.keystore.location = null
[36mkafka1             |[0m 	confluent.ssl.keystore.password = null
[36mkafka1             |[0m 	confluent.ssl.keystore.type = null
[36mkafka1             |[0m 	confluent.ssl.protocol = null
[36mkafka1             |[0m 	confluent.ssl.truststore.location = null
[36mkafka1             |[0m 	confluent.ssl.truststore.password = null
[36mkafka1             |[0m 	confluent.ssl.truststore.type = null
[36mkafka1             |[0m 	confluent.tier.archiver.num.threads = 2
[36mkafka1             |[0m 	confluent.tier.backend =
[36mkafka1             |[0m 	confluent.tier.enable = false
[36mkafka1             |[0m 	confluent.tier.feature = false
[36mkafka1             |[0m 	confluent.tier.fenced.segment.delete.delay.ms = 600000
[36mkafka1             |[0m 	confluent.tier.fetcher.memorypool.bytes = 0
[36mkafka1             |[0m 	confluent.tier.fetcher.num.threads = 4
[36mkafka1             |[0m 	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
[36mkafka1             |[0m 	confluent.tier.fetcher.offset.cache.period.ms = 60000
[36mkafka1             |[0m 	confluent.tier.fetcher.offset.cache.size = 200000
[36mkafka1             |[0m 	confluent.tier.gcs.bucket = null
[36mkafka1             |[0m 	confluent.tier.gcs.cred.file.path = null
[36mkafka1             |[0m 	confluent.tier.gcs.prefix =
[36mkafka1             |[0m 	confluent.tier.gcs.region = null
[36mkafka1             |[0m 	confluent.tier.gcs.write.chunk.size = 0
[36mkafka1             |[0m 	confluent.tier.local.hotset.bytes = -1
[36mkafka1             |[0m 	confluent.tier.local.hotset.ms = 86400000
[36mkafka1             |[0m 	confluent.tier.max.partition.fetch.bytes.override = 0
[36mkafka1             |[0m 	confluent.tier.metadata.bootstrap.servers = null
[36mkafka1             |[0m 	confluent.tier.metadata.max.poll.ms = 100
[36mkafka1             |[0m 	confluent.tier.metadata.namespace = null
[36mkafka1             |[0m 	confluent.tier.metadata.num.partitions = 50
[36mkafka1             |[0m 	confluent.tier.metadata.replication.factor = 3
[36mkafka1             |[0m 	confluent.tier.metadata.request.timeout.ms = 30000
[36mkafka1             |[0m 	confluent.tier.object.fetcher.num.threads = 1
[36mkafka1             |[0m 	confluent.tier.partition.state.commit.interval.ms = 15000
[36mkafka1             |[0m 	confluent.tier.s3.assumerole.arn = null
[36mkafka1             |[0m 	confluent.tier.s3.auto.abort.threshold.bytes = 500000
[36mkafka1             |[0m 	confluent.tier.s3.aws.endpoint.override = null
[36mkafka1             |[0m 	confluent.tier.s3.aws.signer.override = null
[36mkafka1             |[0m 	confluent.tier.s3.bucket = null
[36mkafka1             |[0m 	confluent.tier.s3.cred.file.path = null
[36mkafka1             |[0m 	confluent.tier.s3.prefix =
[36mkafka1             |[0m 	confluent.tier.s3.region = null
[36mkafka1             |[0m 	confluent.tier.s3.sse.algorithm = AES256
[36mkafka1             |[0m 	confluent.tier.s3.sse.customer.encryption.key = null
[36mkafka1             |[0m 	confluent.tier.segment.hotset.roll.min.bytes = 104857600
[36mkafka1             |[0m 	confluent.tier.topic.delete.check.interval.ms = 10800000
[36mkafka1             |[0m 	confluent.transaction.state.log.placement.constraints =
[36mkafka1             |[0m 	confluent.verify.group.subscription.prefix = false
[36mkafka1             |[0m 	connection.failed.authentication.delay.ms = 100
[36mkafka1             |[0m 	connections.max.idle.ms = 600000
[36mkafka1             |[0m 	connections.max.reauth.ms = 0
[36mkafka1             |[0m 	control.plane.listener.name = null
[36mkafka1             |[0m 	controlled.shutdown.enable = true
[36mkafka1             |[0m 	controlled.shutdown.max.retries = 3
[36mkafka1             |[0m 	controlled.shutdown.retry.backoff.ms = 5000
[36mkafka1             |[0m 	controller.socket.timeout.ms = 30000
[36mkafka1             |[0m 	create.topic.policy.class.name = null
[36mkafka1             |[0m 	default.replication.factor = 1
[36mkafka1             |[0m 	delegation.token.expiry.check.interval.ms = 3600000
[36mkafka1             |[0m 	delegation.token.expiry.time.ms = 86400000
[36mkafka1             |[0m 	delegation.token.master.key = null
[36mkafka1             |[0m 	delegation.token.max.lifetime.ms = 604800000
[36mkafka1             |[0m 	delete.records.purgatory.purge.interval.requests = 1
[36mkafka1             |[0m 	delete.topic.enable = true
[36mkafka1             |[0m 	enable.fips = false
[36mkafka1             |[0m 	fetch.max.bytes = 57671680
[36mkafka1             |[0m 	fetch.purgatory.purge.interval.requests = 1000
[36mkafka1             |[0m 	follower.replication.throttled.rate = 9223372036854775807
[36mkafka1             |[0m 	follower.replication.throttled.replicas = none
[36mkafka1             |[0m 	group.initial.rebalance.delay.ms = 0
[36mkafka1             |[0m 	group.max.session.timeout.ms = 1800000
[36mkafka1             |[0m 	group.max.size = 2147483647
[36mkafka1             |[0m 	group.min.session.timeout.ms = 6000
[36mkafka1             |[0m 	host.name =
[36mkafka1             |[0m 	inter.broker.listener.name = null
[36mkafka1             |[0m 	inter.broker.protocol.version = 2.6-IV0
[36mkafka1             |[0m 	kafka.metrics.polling.interval.secs = 10
[36mkafka1             |[0m 	kafka.metrics.reporters = []
[36mkafka1             |[0m 	leader.imbalance.check.interval.seconds = 300
[36mkafka1             |[0m 	leader.imbalance.per.broker.percentage = 10
[36mkafka1             |[0m 	leader.replication.throttled.rate = 9223372036854775807
[36mkafka1             |[0m 	leader.replication.throttled.replicas = none
[36mkafka1             |[0m 	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
[36mkafka1             |[0m 	listeners = PLAINTEXT://0.0.0.0:19092,PLAINTEXT_HOST://0.0.0.0:9092
[36mkafka1             |[0m 	log.cleaner.backoff.ms = 15000
[36mkafka1             |[0m 	log.cleaner.dedupe.buffer.size = 134217728
[36mkafka1             |[0m 	log.cleaner.delete.retention.ms = 86400000
[36mkafka1             |[0m 	log.cleaner.enable = true
[36mkafka1             |[0m 	log.cleaner.io.buffer.load.factor = 0.9
[36mkafka1             |[0m 	log.cleaner.io.buffer.size = 524288
[36mkafka1             |[0m 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
[36mkafka1             |[0m 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
[36mkafka1             |[0m 	log.cleaner.min.cleanable.ratio = 0.5
[36mkafka1             |[0m 	log.cleaner.min.compaction.lag.ms = 0
[36mkafka1             |[0m 	log.cleaner.threads = 1
[36mkafka1             |[0m 	log.cleanup.policy = [delete]
[36mkafka1             |[0m 	log.deletion.max.segments.per.run = 2147483647
[36mkafka1             |[0m 	log.dir = /tmp/kafka-logs
[36mkafka1             |[0m 	log.dirs = /var/lib/kafka/data
[36mkafka1             |[0m 	log.flush.interval.messages = 9223372036854775807
[36mkafka1             |[0m 	log.flush.interval.ms = null
[36mkafka1             |[0m 	log.flush.offset.checkpoint.interval.ms = 60000
[36mkafka1             |[0m 	log.flush.scheduler.interval.ms = 9223372036854775807
[36mkafka1             |[0m 	log.flush.start.offset.checkpoint.interval.ms = 60000
[36mkafka1             |[0m 	log.index.interval.bytes = 4096
[36mkafka1             |[0m 	log.index.size.max.bytes = 10485760
[36mkafka1             |[0m 	log.message.downconversion.enable = true
[36mkafka1             |[0m 	log.message.format.version = 2.6-IV0
[36mkafka1             |[0m 	log.message.timestamp.difference.max.ms = 9223372036854775807
[36mkafka1             |[0m 	log.message.timestamp.type = CreateTime
[36mkafka1             |[0m 	log.preallocate = false
[36mkafka1             |[0m 	log.retention.bytes = -1
[36mkafka1             |[0m 	log.retention.check.interval.ms = 300000
[36mkafka1             |[0m 	log.retention.hours = 168
[36mkafka1             |[0m 	log.retention.minutes = null
[36mkafka1             |[0m 	log.retention.ms = null
[36mkafka1             |[0m 	log.roll.hours = 168
[36mkafka1             |[0m 	log.roll.jitter.hours = 0
[36mkafka1             |[0m 	log.roll.jitter.ms = null
[36mkafka1             |[0m 	log.roll.ms = null
[36mkafka1             |[0m 	log.segment.bytes = 1073741824
[36mkafka1             |[0m 	log.segment.delete.delay.ms = 60000
[36mkafka1             |[0m 	max.connections = 2147483647
[36mkafka1             |[0m 	max.connections.per.ip = 2147483647
[36mkafka1             |[0m 	max.connections.per.ip.overrides =
[36mkafka1             |[0m 	max.incremental.fetch.session.cache.slots = 1000
[36mkafka1             |[0m 	message.max.bytes = 1048588
[36mkafka1             |[0m 	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
[36mkafka1             |[0m 	metrics.num.samples = 2
[36mkafka1             |[0m 	metrics.recording.level = INFO
[36mkafka1             |[0m 	metrics.sample.window.ms = 30000
[36mkafka1             |[0m 	min.insync.replicas = 1
[36mkafka1             |[0m 	multitenant.metadata.class = null
[36mkafka1             |[0m 	multitenant.metadata.dir = null
[36mkafka1             |[0m 	multitenant.metadata.reload.delay.ms = 120000
[36mkafka1             |[0m 	multitenant.metadata.ssl.certs.path = null
[36mkafka1             |[0m 	multitenant.tenant.delete.batch.size = 10
[36mkafka1             |[0m 	multitenant.tenant.delete.delay = 604800000
[36mkafka1             |[0m 	num.io.threads = 8
[36mkafka1             |[0m 	num.network.threads = 3
[36mkafka1             |[0m 	num.partitions = 1
[36mkafka1             |[0m 	num.recovery.threads.per.data.dir = 1
[36mkafka1             |[0m 	num.replica.alter.log.dirs.threads = null
[36mkafka1             |[0m 	num.replica.fetchers = 1
[36mkafka1             |[0m 	offset.metadata.max.bytes = 4096
[36mkafka1             |[0m 	offsets.commit.required.acks = -1
[36mkafka1             |[0m 	offsets.commit.timeout.ms = 5000
[36mkafka1             |[0m 	offsets.load.buffer.size = 5242880
[36mkafka1             |[0m 	offsets.retention.check.interval.ms = 600000
[36mkafka1             |[0m 	offsets.retention.minutes = 10080
[36mkafka1             |[0m 	offsets.topic.compression.codec = 0
[36mkafka1             |[0m 	offsets.topic.num.partitions = 50
[36mkafka1             |[0m 	offsets.topic.replication.factor = 2
[36mkafka1             |[0m 	offsets.topic.segment.bytes = 104857600
[36mkafka1             |[0m 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
[36mkafka1             |[0m 	password.encoder.iterations = 4096
[36mkafka1             |[0m 	password.encoder.key.length = 128
[36mkafka1             |[0m 	password.encoder.keyfactory.algorithm = null
[36mkafka1             |[0m 	password.encoder.old.secret = null
[36mkafka1             |[0m 	password.encoder.secret = null
[36mkafka1             |[0m 	port = 9092
[36mkafka1             |[0m 	principal.builder.class = null
[36mkafka1             |[0m 	producer.purgatory.purge.interval.requests = 1000
[36mkafka1             |[0m 	queued.max.request.bytes = -1
[36mkafka1             |[0m 	queued.max.requests = 500
[36mkafka1             |[0m 	quota.consumer.default = 9223372036854775807
[36mkafka1             |[0m 	quota.producer.default = 9223372036854775807
[36mkafka1             |[0m 	quota.window.num = 11
[36mkafka1             |[0m 	quota.window.size.seconds = 1
[36mkafka1             |[0m 	replica.fetch.backoff.ms = 1000
[36mkafka1             |[0m 	replica.fetch.max.bytes = 1048576
[36mkafka1             |[0m 	replica.fetch.min.bytes = 1
[36mkafka1             |[0m 	replica.fetch.response.max.bytes = 10485760
[36mkafka1             |[0m 	replica.fetch.wait.max.ms = 500
[36mkafka1             |[0m 	replica.high.watermark.checkpoint.interval.ms = 5000
[36mkafka1             |[0m 	replica.lag.time.max.ms = 30000
[36mkafka1             |[0m 	replica.selector.class = null
[36mkafka1             |[0m 	replica.socket.receive.buffer.bytes = 65536
[36mkafka1             |[0m 	replica.socket.timeout.ms = 30000
[36mkafka1             |[0m 	replication.quota.window.num = 11
[36mkafka1             |[0m 	replication.quota.window.size.seconds = 1
[36mkafka1             |[0m 	request.timeout.ms = 30000
[36mkafka1             |[0m 	reserved.broker.max.id = 1000
[36mkafka1             |[0m 	sasl.client.callback.handler.class = null
[36mkafka1             |[0m 	sasl.enabled.mechanisms = [GSSAPI]
[36mkafka1             |[0m 	sasl.jaas.config = null
[36mkafka1             |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mkafka1             |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mkafka1             |[0m 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
[36mkafka1             |[0m 	sasl.kerberos.service.name = null
[36mkafka1             |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mkafka1             |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mkafka1             |[0m 	sasl.login.callback.handler.class = null
[36mkafka1             |[0m 	sasl.login.class = null
[36mkafka1             |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mkafka1             |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mkafka1             |[0m 	sasl.login.refresh.window.factor = 0.8
[36mkafka1             |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mkafka1             |[0m 	sasl.mechanism.inter.broker.protocol = GSSAPI
[36mkafka1             |[0m 	sasl.server.callback.handler.class = null
[36mkafka1             |[0m 	security.inter.broker.protocol = PLAINTEXT
[36mkafka1             |[0m 	security.providers = null
[36mkafka1             |[0m 	socket.receive.buffer.bytes = 102400
[36mkafka1             |[0m 	socket.request.max.bytes = 104857600
[36mkafka1             |[0m 	socket.send.buffer.bytes = 102400
[36mkafka1             |[0m 	ssl.cipher.suites = []
[36mkafka1             |[0m 	ssl.client.auth = none
[36mkafka1             |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mkafka1             |[0m 	ssl.endpoint.identification.algorithm = https
[36mkafka1             |[0m 	ssl.engine.factory.class = null
[36mkafka1             |[0m 	ssl.key.password = null
[36mkafka1             |[0m 	ssl.keymanager.algorithm = SunX509
[36mkafka1             |[0m 	ssl.keystore.location = null
[36mkafka1             |[0m 	ssl.keystore.password = null
[36mkafka1             |[0m 	ssl.keystore.type = JKS
[36mkafka1             |[0m 	ssl.principal.mapping.rules = DEFAULT
[36mkafka1             |[0m 	ssl.protocol = TLSv1.3
[36mkafka1             |[0m 	ssl.provider = null
[36mkafka1             |[0m 	ssl.secure.random.implementation = null
[36mkafka1             |[0m 	ssl.trustmanager.algorithm = PKIX
[36mkafka1             |[0m 	ssl.truststore.location = null
[36mkafka1             |[0m 	ssl.truststore.password = null
[36mkafka1             |[0m 	ssl.truststore.type = JKS
[36mkafka1             |[0m 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
[36mkafka1             |[0m 	transaction.max.timeout.ms = 900000
[36mkafka1             |[0m 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
[36mkafka1             |[0m 	transaction.state.log.load.buffer.size = 5242880
[36mkafka1             |[0m 	transaction.state.log.min.isr = 1
[36mkafka1             |[0m 	transaction.state.log.num.partitions = 50
[36mkafka1             |[0m 	transaction.state.log.replication.factor = 2
[36mkafka1             |[0m 	transaction.state.log.segment.bytes = 104857600
[36mkafka1             |[0m 	transactional.id.expiration.ms = 604800000
[36mkafka1             |[0m 	unclean.leader.election.enable = false
[36mkafka1             |[0m 	zookeeper.clientCnxnSocket = null
[36mkafka1             |[0m 	zookeeper.connect = zookeeper:2181
[36mkafka1             |[0m 	zookeeper.connection.timeout.ms = null
[36mkafka1             |[0m 	zookeeper.max.in.flight.requests = 10
[36mkafka1             |[0m 	zookeeper.session.timeout.ms = 18000
[36mkafka1             |[0m 	zookeeper.set.acl = false
[36mkafka1             |[0m 	zookeeper.ssl.cipher.suites = null
[36mkafka1             |[0m 	zookeeper.ssl.client.enable = false
[36mkafka1             |[0m 	zookeeper.ssl.crl.enable = false
[36mkafka1             |[0m 	zookeeper.ssl.enabled.protocols = null
[36mkafka1             |[0m 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
[36mkafka1             |[0m 	zookeeper.ssl.keystore.location = null
[36mkafka1             |[0m 	zookeeper.ssl.keystore.password = null
[36mkafka1             |[0m 	zookeeper.ssl.keystore.type = null
[36mkafka1             |[0m 	zookeeper.ssl.ocsp.enable = false
[36mkafka1             |[0m 	zookeeper.ssl.protocol = TLSv1.2
[36mkafka1             |[0m 	zookeeper.ssl.truststore.location = null
[36mkafka1             |[0m 	zookeeper.ssl.truststore.password = null
[36mkafka1             |[0m 	zookeeper.ssl.truststore.type = null
[36mkafka1             |[0m 	zookeeper.sync.time.ms = 2000
[36mkafka1             |[0m  (kafka.server.KafkaConfig)
[36mkafka1             |[0m [2021-01-09 19:50:29,038] INFO KafkaRestConfig values:
[36mkafka1             |[0m 	access.control.allow.headers =
[36mkafka1             |[0m 	access.control.allow.methods =
[36mkafka1             |[0m 	access.control.allow.origin =
[36mkafka1             |[0m 	access.control.skip.options = true
[36mkafka1             |[0m 	advertised.listeners = []
[36mkafka1             |[0m 	api.v2.enable = false
[36mkafka1             |[0m 	api.v3.enable = true
[36mkafka1             |[0m 	authentication.method = NONE
[36mkafka1             |[0m 	authentication.realm =
[36mkafka1             |[0m 	authentication.roles = [*]
[36mkafka1             |[0m 	authentication.skip.paths = []
[36mkafka1             |[0m 	bootstrap.servers = kafka1:19092
[36mkafka1             |[0m 	client.init.timeout.ms = 60000
[36mkafka1             |[0m 	client.sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mkafka1             |[0m 	client.sasl.kerberos.min.time.before.relogin = 60000
[36mkafka1             |[0m 	client.sasl.kerberos.service.name =
[36mkafka1             |[0m 	client.sasl.kerberos.ticket.renew.jitter = 0.05
[36mkafka1             |[0m 	client.sasl.kerberos.ticket.renew.window.factor = 0.8
[36mkafka1             |[0m 	client.sasl.mechanism = GSSAPI
[36mkafka1             |[0m 	client.security.protocol = PLAINTEXT
[36mkafka1             |[0m 	client.ssl.cipher.suites =
[36mkafka1             |[0m 	client.ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1
[36mkafka1             |[0m 	client.ssl.endpoint.identification.algorithm =
[36mkafka1             |[0m 	client.ssl.key.password = [hidden]
[36mkafka1             |[0m 	client.ssl.keymanager.algorithm = SunX509
[36mkafka1             |[0m 	client.ssl.keystore.location =
[36mkafka1             |[0m 	client.ssl.keystore.password = [hidden]
[36mkafka1             |[0m 	client.ssl.keystore.type = JKS
[36mkafka1             |[0m 	client.ssl.protocol = TLS
[36mkafka1             |[0m 	client.ssl.provider =
[36mkafka1             |[0m 	client.ssl.trustmanager.algorithm = PKIX
[36mkafka1             |[0m 	client.ssl.truststore.location =
[36mkafka1             |[0m 	client.ssl.truststore.password = [hidden]
[36mkafka1             |[0m 	client.ssl.truststore.type = JKS
[36mkafka1             |[0m 	client.timeout.ms = 500
[36mkafka1             |[0m 	compression.enable = true
[36mkafka1             |[0m 	confluent.resource.name.authority =
[36mkafka1             |[0m 	consumer.instance.timeout.ms = 300000
[36mkafka1             |[0m 	consumer.iterator.backoff.ms = 50
[36mkafka1             |[0m 	consumer.iterator.timeout.ms = 1
[36mkafka1             |[0m 	consumer.request.max.bytes = 67108864
[36mkafka1             |[0m 	consumer.request.timeout.ms = 1000
[36mkafka1             |[0m 	consumer.threads = 50
[36mkafka1             |[0m 	debug = false
[36mkafka1             |[0m 	fetch.min.bytes = -1
[36mkafka1             |[0m 	host.name =
[36mkafka1             |[0m 	id =
[36mkafka1             |[0m 	idle.timeout.ms = 30000
[36mkafka1             |[0m 	kafka.rest.resource.extension.class = [io.confluent.kafkarest.KafkaRestResourceExtension]
[36mkafka1             |[0m 	listeners = []
[36mkafka1             |[0m 	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
[36mkafka1             |[0m 	metrics.jmx.prefix = kafka.rest
[36mkafka1             |[0m 	metrics.num.samples = 2
[36mkafka1             |[0m 	metrics.sample.window.ms = 30000
[36mkafka1             |[0m 	metrics.tag.map = []
[36mkafka1             |[0m 	port = 8082
[36mkafka1             |[0m 	producer.threads = 5
[36mkafka1             |[0m 	request.logger.name = io.confluent.rest-utils.requests
[36mkafka1             |[0m 	request.queue.capacity = 2147483647
[36mkafka1             |[0m 	request.queue.capacity.growby = 64
[36mkafka1             |[0m 	request.queue.capacity.init = 128
[36mkafka1             |[0m 	resource.extension.classes = []
[36mkafka1             |[0m 	response.http.headers.config =
[36mkafka1             |[0m 	response.mediatype.default = application/json
[36mkafka1             |[0m 	response.mediatype.preferred = [application/json, application/vnd.kafka.v2+json]
[36mkafka1             |[0m 	rest.servlet.initializor.classes = []
[36mkafka1             |[0m 	schema.registry.url = http://localhost:8081
[36mkafka1             |[0m 	shutdown.graceful.ms = 1000
[36mkafka1             |[0m 	simpleconsumer.pool.size.max = 25
[36mkafka1             |[0m 	simpleconsumer.pool.timeout.ms = 1000
[36mkafka1             |[0m 	ssl.cipher.suites = []
[36mkafka1             |[0m 	ssl.client.auth = false
[36mkafka1             |[0m 	ssl.client.authentication = NONE
[36mkafka1             |[0m 	ssl.enabled.protocols = []
[36mkafka1             |[0m 	ssl.endpoint.identification.algorithm = null
[36mkafka1             |[0m 	ssl.key.password = [hidden]
[36mkafka1             |[0m 	ssl.keymanager.algorithm =
[36mkafka1             |[0m 	ssl.keystore.location =
[36mkafka1             |[0m 	ssl.keystore.password = [hidden]
[36mkafka1             |[0m 	ssl.keystore.reload = false
[36mkafka1             |[0m 	ssl.keystore.type = JKS
[36mkafka1             |[0m 	ssl.keystore.watch.location =
[36mkafka1             |[0m 	ssl.protocol = TLS
[36mkafka1             |[0m 	ssl.provider =
[36mkafka1             |[0m 	ssl.trustmanager.algorithm =
[36mkafka1             |[0m 	ssl.truststore.location =
[36mkafka1             |[0m 	ssl.truststore.password = [hidden]
[36mkafka1             |[0m 	ssl.truststore.type = JKS
[36mkafka1             |[0m 	thread.pool.max = 200
[36mkafka1             |[0m 	thread.pool.min = 8
[36mkafka1             |[0m 	websocket.path.prefix = /ws
[36mkafka1             |[0m 	websocket.servlet.initializor.classes = []
[36mkafka1             |[0m  (io.confluent.kafkarest.KafkaRestConfig)
[36mkafka1             |[0m [2021-01-09 19:50:29,048] INFO ConfluentTelemetryConfig values:
[36mkafka1             |[0m 	confluent.telemetry.api.key = null
[36mkafka1             |[0m 	confluent.telemetry.api.secret = null
[36mkafka1             |[0m 	confluent.telemetry.debug.enabled = false
[36mkafka1             |[0m 	confluent.telemetry.enabled = false
[36mkafka1             |[0m 	confluent.telemetry.events.collector.include = .*auto.create.topics.enable.*|.*broker.id.*|.*compression.type.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.tier.s3.region.*|.*delete.topic.enable.*|.*leader.imbalance.check.interval.seconds.*|.*leader.imbalance.per.broker.percentage.*|.*log.dir.*|.*log.dirs.*|.*log.flush.interval.messages.*|.*log.flush.interval.ms.*|.*log.flush.offset.checkpoint.interval.ms.*|.*log.flush.scheduler.interval.ms.*|.*log.flush.start.offset.checkpoint.interval.ms.*|.*log.retention.bytes.*|.*log.retention.hours.*|.*log.retention.minutes.*|.*log.retention.ms.*|.*log.roll.hours.*|.*log.roll.ms.*|.*log.segment.bytes.*|.*log.segment.delete.delay.ms.*|.*message.max.bytes.*|.*min.insync.replicas.*|.*num.io.threads.*|.*num.network.threads.*|.*num.recovery.threads.per.data.dir.*|.*num.replica.alter.log.dirs.threads.*|.*num.replica.fetchers.*|.*offset.metadata.max.bytes.*|.*offsets.commit.required.acks.*|.*offsets.commit.timeout.ms.*|.*offsets.load.buffer.size.*|.*offsets.retention.check.interval.ms.*|.*offsets.retention.minutes.*|.*offsets.topic.compression.codec.*|.*offsets.topic.num.partitions.*|.*offsets.topic.replication.factor.*|.*queued.max.requests.*|.*replica.fetch.min.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*request.timeout.ms.*|.*socket.receive.buffer.bytes.*|.*socket.request.max.bytes.*|.*socket.send.buffer.bytes.*|.*transacation.*.*|.*unclean.leader.election.enable.*|.*zookeeper.connection.timeout.ms.*|.*zookeeper.max.in.flight.requests.*|.*zookeeper.session.timeout.ms.*|.*zookeeper.set.acl.*|.*broker.id.generation.enable.*|.*broker.rack.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*connections.max.idle.ms.*|.*connections.max.reauth.ms.*|.*controlled.*.*|.*controller.socket.timeout.ms.*|.*default.replication.factor.*|.*delegation.token.expiry.time.ms.*|.*delegation.token.max.lifetime.ms.*|.*delete.records.purgatory.purge.interval.requests.*|.*fetch.*.*|.*group.*.*|.*log.cleaner.*.*|.*log.index.*.*|.*log.message.*.*|.*log.preallocate.*|.*log.retention.check.interval.ms.*|.*max.*.*|.*num.partitions.*|.*principal.builder.class.*|.*producer.purgatory.purge.interval.requests.*|.*queued.max.request.bytes.*|.*replica.*.*|.*reserved.broker.max.id.*|.*sasl.client.callback.handler.class.*|.*sasl.enabled.mechanisms.*|.*sasl.login.class.*|.*sasl.mechanism.inter.broker.protocol.*|.*sasl.server.callback.handler.class.*|.*security.inter.broker.protocol.*|.*ssl.client.auth.*|.*ssl.enabled.protocols.*|.*ssl.engine.builder.class.*|.*ssl.protocol.*|.*ssl.provider.*|.*zookeeper.clientCnxnSocket.*|.*zookeeper.ssl.client.enable.*|.*alter.*.*|.*authorizer.class.name.*|.*client.quota.callback.class.*|.*confluent.log.placement.constraints.*|.*confluent.tier.topic.delete.check.interval.ms.*|.*connection.failed.authentication.delay.ms.*|.*create.topic.policy.class.name.*|.*kafka.metrics.polling.interval.secs.*|.*kafka.metrics.reporters.*|.*listener.security.protocol.map.*|.*log.message.downconversion.enable.*|.*metric.reporters.*|.*metrics.*.*|.*password.encoder.cipher.algorithm.*|.*password.encoder.iterations.*|.*password.encoder.key.length.*|.*password.encoder.keyfactory.algorithm.*|.*quota.*.*|.*replication.*.*|.*security.providers.*|.*ssl.endpoint.identification.algorithm.*|.*enable.fips.*
[36mkafka1             |[0m 	confluent.telemetry.events.enable = false
[36mkafka1             |[0m 	confluent.telemetry.metrics.collector.include = .*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*
[36mkafka1             |[0m 	confluent.telemetry.metrics.collector.interval.ms = 60000
[36mkafka1             |[0m 	confluent.telemetry.proxy.password = null
[36mkafka1             |[0m 	confluent.telemetry.proxy.url = null
[36mkafka1             |[0m 	confluent.telemetry.proxy.username = null
[36mkafka1             |[0m  (io.confluent.telemetry.ConfluentTelemetryConfig)
[36mkafka1             |[0m [2021-01-09 19:50:29,050] INFO VolumeMetricsCollectorConfig values:
[36mkafka1             |[0m 	confluent.telemetry.metrics.collector.volume.update.ms = 15000
[36mkafka1             |[0m  (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
[36mkafka1             |[0m [2021-01-09 19:50:29,055] INFO HttpExporterConfig values:
[36mkafka1             |[0m 	api.key = null
[36mkafka1             |[0m 	api.secret = null
[36mkafka1             |[0m 	buffer.batch.duration.max.ms = null
[36mkafka1             |[0m 	buffer.batch.items.max = null
[36mkafka1             |[0m 	buffer.inflight.submissions.max = null
[36mkafka1             |[0m 	buffer.pending.batches.max = null
[36mkafka1             |[0m 	client.attempts.max = null
[36mkafka1             |[0m 	client.base.url = https://collector.telemetry.confluent.cloud
[36mkafka1             |[0m 	client.compression = null
[36mkafka1             |[0m 	client.connect.timeout.ms = null
[36mkafka1             |[0m 	client.request.timeout.ms = null
[36mkafka1             |[0m 	client.retry.delay.seconds = null
[36mkafka1             |[0m 	enabled = false
[36mkafka1             |[0m 	metrics.include = .*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*
[36mkafka1             |[0m 	proxy.password = null
[36mkafka1             |[0m 	proxy.url = null
[36mkafka1             |[0m 	proxy.username = null
[36mkafka1             |[0m 	type = http
[36mkafka1             |[0m  (io.confluent.telemetry.exporter.http.HttpExporterConfig)
[36mkafka1             |[0m [2021-01-09 19:50:29,055] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
[36mkafka1             |[0m [2021-01-09 19:50:29,057] WARN Ignoring redefinition of existing telemetry label kafka_rest.version (io.confluent.shaded.io.confluent.telemetry.ResourceBuilderFacade)
[36mkafka1             |[0m [2021-01-09 19:50:29,059] INFO ConfluentTelemetryConfig values:
[36mkafka1             |[0m 	confluent.telemetry.api.key = null
[36mkafka1             |[0m 	confluent.telemetry.api.secret = null
[36mkafka1             |[0m 	confluent.telemetry.debug.enabled = false
[36mkafka1             |[0m 	confluent.telemetry.enabled = false
[36mkafka1             |[0m 	confluent.telemetry.events.collector.include = .*auto.create.topics.enable.*|.*broker.id.*|.*compression.type.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.tier.s3.region.*|.*delete.topic.enable.*|.*leader.imbalance.check.interval.seconds.*|.*leader.imbalance.per.broker.percentage.*|.*log.dir.*|.*log.dirs.*|.*log.flush.interval.messages.*|.*log.flush.interval.ms.*|.*log.flush.offset.checkpoint.interval.ms.*|.*log.flush.scheduler.interval.ms.*|.*log.flush.start.offset.checkpoint.interval.ms.*|.*log.retention.bytes.*|.*log.retention.hours.*|.*log.retention.minutes.*|.*log.retention.ms.*|.*log.roll.hours.*|.*log.roll.ms.*|.*log.segment.bytes.*|.*log.segment.delete.delay.ms.*|.*message.max.bytes.*|.*min.insync.replicas.*|.*num.io.threads.*|.*num.network.threads.*|.*num.recovery.threads.per.data.dir.*|.*num.replica.alter.log.dirs.threads.*|.*num.replica.fetchers.*|.*offset.metadata.max.bytes.*|.*offsets.commit.required.acks.*|.*offsets.commit.timeout.ms.*|.*offsets.load.buffer.size.*|.*offsets.retention.check.interval.ms.*|.*offsets.retention.minutes.*|.*offsets.topic.compression.codec.*|.*offsets.topic.num.partitions.*|.*offsets.topic.replication.factor.*|.*queued.max.requests.*|.*replica.fetch.min.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*request.timeout.ms.*|.*socket.receive.buffer.bytes.*|.*socket.request.max.bytes.*|.*socket.send.buffer.bytes.*|.*transacation.*.*|.*unclean.leader.election.enable.*|.*zookeeper.connection.timeout.ms.*|.*zookeeper.max.in.flight.requests.*|.*zookeeper.session.timeout.ms.*|.*zookeeper.set.acl.*|.*broker.id.generation.enable.*|.*broker.rack.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*connections.max.idle.ms.*|.*connections.max.reauth.ms.*|.*controlled.*.*|.*controller.socket.timeout.ms.*|.*default.replication.factor.*|.*delegation.token.expiry.time.ms.*|.*delegation.token.max.lifetime.ms.*|.*delete.records.purgatory.purge.interval.requests.*|.*fetch.*.*|.*group.*.*|.*log.cleaner.*.*|.*log.index.*.*|.*log.message.*.*|.*log.preallocate.*|.*log.retention.check.interval.ms.*|.*max.*.*|.*num.partitions.*|.*principal.builder.class.*|.*producer.purgatory.purge.interval.requests.*|.*queued.max.request.bytes.*|.*replica.*.*|.*reserved.broker.max.id.*|.*sasl.client.callback.handler.class.*|.*sasl.enabled.mechanisms.*|.*sasl.login.class.*|.*sasl.mechanism.inter.broker.protocol.*|.*sasl.server.callback.handler.class.*|.*security.inter.broker.protocol.*|.*ssl.client.auth.*|.*ssl.enabled.protocols.*|.*ssl.engine.builder.class.*|.*ssl.protocol.*|.*ssl.provider.*|.*zookeeper.clientCnxnSocket.*|.*zookeeper.ssl.client.enable.*|.*alter.*.*|.*authorizer.class.name.*|.*client.quota.callback.class.*|.*confluent.log.placement.constraints.*|.*confluent.tier.topic.delete.check.interval.ms.*|.*connection.failed.authentication.delay.ms.*|.*create.topic.policy.class.name.*|.*kafka.metrics.polling.interval.secs.*|.*kafka.metrics.reporters.*|.*listener.security.protocol.map.*|.*log.message.downconversion.enable.*|.*metric.reporters.*|.*metrics.*.*|.*password.encoder.cipher.algorithm.*|.*password.encoder.iterations.*|.*password.encoder.key.length.*|.*password.encoder.keyfactory.algorithm.*|.*quota.*.*|.*replication.*.*|.*security.providers.*|.*ssl.endpoint.identification.algorithm.*|.*enable.fips.*
[36mkafka1             |[0m 	confluent.telemetry.events.enable = false
[36mkafka1             |[0m 	confluent.telemetry.metrics.collector.include = .*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*|.*io.confluent.kafka.rest/.*(connections_active|connections_closed_rate|request_error_rate|request_latency_avg|request_latency_max|request_rate|response_size_avg|response_size_max).*
[36mkafka1             |[0m 	confluent.telemetry.metrics.collector.interval.ms = 60000
[36mkafka1             |[0m 	confluent.telemetry.proxy.password = null
[36mkafka1             |[0m 	confluent.telemetry.proxy.url = null
[36mkafka1             |[0m 	confluent.telemetry.proxy.username = null
[36mkafka1             |[0m  (io.confluent.telemetry.ConfluentTelemetryConfig)
[36mkafka1             |[0m [2021-01-09 19:50:29,060] INFO VolumeMetricsCollectorConfig values:
[36mkafka1             |[0m 	confluent.telemetry.metrics.collector.volume.update.ms = 15000
[36mkafka1             |[0m  (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
[36mkafka1             |[0m [2021-01-09 19:50:29,064] INFO HttpExporterConfig values:
[36mkafka1             |[0m 	api.key = null
[36mkafka1             |[0m 	api.secret = null
[36mkafka1             |[0m 	buffer.batch.duration.max.ms = null
[36mkafka1             |[0m 	buffer.batch.items.max = null
[36mkafka1             |[0m 	buffer.inflight.submissions.max = null
[36mkafka1             |[0m 	buffer.pending.batches.max = null
[36mkafka1             |[0m 	client.attempts.max = null
[36mkafka1             |[0m 	client.base.url = https://collector.telemetry.confluent.cloud
[36mkafka1             |[0m 	client.compression = null
[36mkafka1             |[0m 	client.connect.timeout.ms = null
[36mkafka1             |[0m 	client.request.timeout.ms = null
[36mkafka1             |[0m 	client.retry.delay.seconds = null
[36mkafka1             |[0m 	enabled = false
[36mkafka1             |[0m 	metrics.include = .*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*|.*io.confluent.kafka.rest/.*(connections_active|connections_closed_rate|request_error_rate|request_latency_avg|request_latency_max|request_rate|response_size_avg|response_size_max).*
[36mkafka1             |[0m 	proxy.password = null
[36mkafka1             |[0m 	proxy.url = null
[36mkafka1             |[0m 	proxy.username = null
[36mkafka1             |[0m 	type = http
[36mkafka1             |[0m  (io.confluent.telemetry.exporter.http.HttpExporterConfig)
[36mkafka1             |[0m [2021-01-09 19:50:29,064] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
[36mkafka1             |[0m [2021-01-09 19:50:29,071] INFO Starting Confluent telemetry reporter with an interval of 60000 ms for resource = (type = kafka_rest) (io.confluent.telemetry.MetricsCollectorTask)
[36mkafka1             |[0m [2021-01-09 19:50:29,081] INFO Loaded Application class io.confluent.kafkarest.servlet.EmbeddedKafkaRestApplication. (io.confluent.http.server.KafkaHttpApplicationLoader)
[36mkafka1             |[0m [2021-01-09 19:50:29,158] INFO Initial capacity 128, increased by 64, maximum capacity 2147483647. (io.confluent.rest.ApplicationServer)
[36mkafka1             |[0m [2021-01-09 19:50:30,837] INFO Adding listener: http://0.0.0.0:8090 (io.confluent.rest.ApplicationServer)
[36mkafka1             |[0m [2021-01-09 19:50:31,139] INFO Loaded KafkaHttpServer implementation class io.confluent.http.server.KafkaHttpServerImpl (io.confluent.kafka.http.server.KafkaHttpServerLoader)
[36mkafka1             |[0m [2021-01-09 19:50:31,142] INFO KafkaHttpServer transitioned from NEW to STARTING.. (io.confluent.http.server.KafkaHttpServerImpl)
[36mkafka1             |[0m [2021-01-09 19:50:33,155] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 2 for 21 partitions (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:33,209] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_partition_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=8, controllerEpoch=1, leader=2, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:33,209] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_partition_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=7, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:33,209] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_partition_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=10, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:33,210] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_partition_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=9, controllerEpoch=1, leader=1, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:33,210] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_partition_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=31, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:33,210] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_partition_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=2, controllerEpoch=1, leader=2, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:33,210] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_partition_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=1, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:33,210] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_partition_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=4, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:33,210] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_partition_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=3, controllerEpoch=1, leader=1, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:33,210] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_partition_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=22, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:33,210] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_partition_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=21, controllerEpoch=1, leader=1, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:33,211] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_partition_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=26, controllerEpoch=1, leader=2, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:33,211] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_partition_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=25, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:33,211] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_partition_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=28, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:33,211] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_partition_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=27, controllerEpoch=1, leader=1, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:33,211] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_partition_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=14, controllerEpoch=1, leader=2, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:33,211] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_partition_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=13, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:33,211] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_partition_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=16, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:33,211] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_partition_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=15, controllerEpoch=1, leader=1, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:33,211] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_partition_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=20, controllerEpoch=1, leader=2, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:33,212] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_partition_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=19, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:34,612] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 2 epoch 1 starting the become-leader transition for partition _confluent_balancer_partition_samples-28 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:34,612] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 2 epoch 1 starting the become-leader transition for partition _confluent_balancer_partition_samples-13 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:34,634] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 2 epoch 1 starting the become-leader transition for partition _confluent_balancer_partition_samples-10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:34,637] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 2 epoch 1 starting the become-leader transition for partition _confluent_balancer_partition_samples-25 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:34,637] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 2 epoch 1 starting the become-leader transition for partition _confluent_balancer_partition_samples-22 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:34,642] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 2 epoch 1 starting the become-leader transition for partition _confluent_balancer_partition_samples-7 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:34,645] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 2 epoch 1 starting the become-leader transition for partition _confluent_balancer_partition_samples-4 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:34,645] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 2 epoch 1 starting the become-leader transition for partition _confluent_balancer_partition_samples-19 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:34,645] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 2 epoch 1 starting the become-leader transition for partition _confluent_balancer_partition_samples-16 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:34,645] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 2 epoch 1 starting the become-leader transition for partition _confluent_balancer_partition_samples-1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:34,645] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 2 epoch 1 starting the become-leader transition for partition _confluent_balancer_partition_samples-31 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:34,776] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent_balancer_partition_samples-28, _confluent_balancer_partition_samples-13, _confluent_balancer_partition_samples-7, _confluent_balancer_partition_samples-31, _confluent_balancer_partition_samples-19, _confluent_balancer_partition_samples-10, _confluent_balancer_partition_samples-22, _confluent_balancer_partition_samples-1, _confluent_balancer_partition_samples-25, _confluent_balancer_partition_samples-16, _confluent_balancer_partition_samples-4) (kafka.server.ReplicaFetcherManager)
[36mkafka1             |[0m [2021-01-09 19:50:34,805] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 2 epoch 1 as part of the become-leader transition for 11 partitions (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:35,806] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 11.0.9.1+1-LTS (org.eclipse.jetty.server.Server)
[36mkafka1             |[0m [2021-01-09 19:50:36,586] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session)
[36mkafka1             |[0m [2021-01-09 19:50:36,592] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session)
[36mkafka1             |[0m [2021-01-09 19:50:36,636] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session)
[36mkafka1             |[0m [2021-01-09 19:50:36,921] INFO [Log partition=_confluent_balancer_partition_samples-28, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 603 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:37,087] INFO [Log partition=_confluent_balancer_partition_samples-28, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:37,304] INFO [MergedLog partition=_confluent_balancer_partition_samples-28, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:37,478] INFO Created log for partition _confluent_balancer_partition_samples-28 in /var/lib/kafka/data/_confluent_balancer_partition_samples-28 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 3600000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:37,540] INFO [Partition _confluent_balancer_partition_samples-28 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-28 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:37,559] INFO [Partition _confluent_balancer_partition_samples-28 broker=0] Log loaded for partition _confluent_balancer_partition_samples-28 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:37,580] INFO [Broker id=0] Leader _confluent_balancer_partition_samples-28 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:37,873] INFO [Log partition=_confluent_balancer_partition_samples-13, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 22 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:37,889] INFO [Log partition=_confluent_balancer_partition_samples-13, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:37,895] INFO [MergedLog partition=_confluent_balancer_partition_samples-13, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:37,922] INFO Created log for partition _confluent_balancer_partition_samples-13 in /var/lib/kafka/data/_confluent_balancer_partition_samples-13 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 3600000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:37,923] INFO [Partition _confluent_balancer_partition_samples-13 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-13 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:37,927] INFO [Partition _confluent_balancer_partition_samples-13 broker=0] Log loaded for partition _confluent_balancer_partition_samples-13 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:37,929] INFO [Broker id=0] Leader _confluent_balancer_partition_samples-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:37,959] INFO [Log partition=_confluent_balancer_partition_samples-10, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 12 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:37,963] INFO [Log partition=_confluent_balancer_partition_samples-10, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:37,972] INFO [MergedLog partition=_confluent_balancer_partition_samples-10, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:37,983] INFO Created log for partition _confluent_balancer_partition_samples-10 in /var/lib/kafka/data/_confluent_balancer_partition_samples-10 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 3600000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:37,985] INFO [Partition _confluent_balancer_partition_samples-10 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-10 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:37,989] INFO [Partition _confluent_balancer_partition_samples-10 broker=0] Log loaded for partition _confluent_balancer_partition_samples-10 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:37,991] INFO [Broker id=0] Leader _confluent_balancer_partition_samples-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:38,025] INFO [Log partition=_confluent_balancer_partition_samples-25, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 17 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:38,053] INFO [Log partition=_confluent_balancer_partition_samples-25, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:38,060] INFO [MergedLog partition=_confluent_balancer_partition_samples-25, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:38,082] INFO Created log for partition _confluent_balancer_partition_samples-25 in /var/lib/kafka/data/_confluent_balancer_partition_samples-25 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 3600000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:38,092] INFO [Partition _confluent_balancer_partition_samples-25 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-25 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:38,093] INFO [Partition _confluent_balancer_partition_samples-25 broker=0] Log loaded for partition _confluent_balancer_partition_samples-25 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:38,093] INFO [Broker id=0] Leader _confluent_balancer_partition_samples-25 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:38,148] INFO [Log partition=_confluent_balancer_partition_samples-22, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 21 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:38,241] INFO [Log partition=_confluent_balancer_partition_samples-22, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:38,324] INFO [MergedLog partition=_confluent_balancer_partition_samples-22, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:38,386] INFO Created log for partition _confluent_balancer_partition_samples-22 in /var/lib/kafka/data/_confluent_balancer_partition_samples-22 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 3600000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:38,395] INFO [Partition _confluent_balancer_partition_samples-22 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-22 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:38,401] INFO [Partition _confluent_balancer_partition_samples-22 broker=0] Log loaded for partition _confluent_balancer_partition_samples-22 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:38,401] INFO [Broker id=0] Leader _confluent_balancer_partition_samples-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:38,537] INFO [Log partition=_confluent_balancer_partition_samples-7, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 92 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:38,541] INFO [Log partition=_confluent_balancer_partition_samples-7, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:38,574] INFO [MergedLog partition=_confluent_balancer_partition_samples-7, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:38,646] INFO Created log for partition _confluent_balancer_partition_samples-7 in /var/lib/kafka/data/_confluent_balancer_partition_samples-7 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 3600000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:38,646] INFO [Partition _confluent_balancer_partition_samples-7 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-7 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:38,651] INFO [Partition _confluent_balancer_partition_samples-7 broker=0] Log loaded for partition _confluent_balancer_partition_samples-7 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:38,657] INFO [Broker id=0] Leader _confluent_balancer_partition_samples-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:38,675] INFO [Log partition=_confluent_balancer_partition_samples-4, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:38,716] INFO [Log partition=_confluent_balancer_partition_samples-4, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:38,739] INFO [MergedLog partition=_confluent_balancer_partition_samples-4, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:38,746] INFO Created log for partition _confluent_balancer_partition_samples-4 in /var/lib/kafka/data/_confluent_balancer_partition_samples-4 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 3600000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:38,756] INFO [Partition _confluent_balancer_partition_samples-4 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-4 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:38,781] INFO [Partition _confluent_balancer_partition_samples-4 broker=0] Log loaded for partition _confluent_balancer_partition_samples-4 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:38,782] INFO [Broker id=0] Leader _confluent_balancer_partition_samples-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:38,842] INFO [Log partition=_confluent_balancer_partition_samples-19, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 31 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:38,850] INFO [Log partition=_confluent_balancer_partition_samples-19, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:38,873] INFO [MergedLog partition=_confluent_balancer_partition_samples-19, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:38,883] INFO Created log for partition _confluent_balancer_partition_samples-19 in /var/lib/kafka/data/_confluent_balancer_partition_samples-19 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 3600000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:38,883] INFO [Partition _confluent_balancer_partition_samples-19 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-19 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:38,897] INFO [Partition _confluent_balancer_partition_samples-19 broker=0] Log loaded for partition _confluent_balancer_partition_samples-19 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:38,902] INFO [Broker id=0] Leader _confluent_balancer_partition_samples-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:38,975] INFO [Log partition=_confluent_balancer_partition_samples-16, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 53 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:38,983] INFO [Log partition=_confluent_balancer_partition_samples-16, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:38,994] INFO [MergedLog partition=_confluent_balancer_partition_samples-16, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:39,004] INFO Created log for partition _confluent_balancer_partition_samples-16 in /var/lib/kafka/data/_confluent_balancer_partition_samples-16 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 3600000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:39,004] INFO [Partition _confluent_balancer_partition_samples-16 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-16 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:39,023] INFO [Partition _confluent_balancer_partition_samples-16 broker=0] Log loaded for partition _confluent_balancer_partition_samples-16 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:39,024] INFO [Broker id=0] Leader _confluent_balancer_partition_samples-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:39,101] INFO [Log partition=_confluent_balancer_partition_samples-1, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 32 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:39,157] INFO [Log partition=_confluent_balancer_partition_samples-1, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:39,163] INFO [MergedLog partition=_confluent_balancer_partition_samples-1, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:39,182] INFO Created log for partition _confluent_balancer_partition_samples-1 in /var/lib/kafka/data/_confluent_balancer_partition_samples-1 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 3600000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:39,188] INFO [Partition _confluent_balancer_partition_samples-1 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-1 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:39,189] INFO [Partition _confluent_balancer_partition_samples-1 broker=0] Log loaded for partition _confluent_balancer_partition_samples-1 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:39,191] INFO [Broker id=0] Leader _confluent_balancer_partition_samples-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:39,273] INFO [Log partition=_confluent_balancer_partition_samples-31, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 14 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:39,282] INFO [Log partition=_confluent_balancer_partition_samples-31, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:39,291] INFO [MergedLog partition=_confluent_balancer_partition_samples-31, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:39,294] INFO Created log for partition _confluent_balancer_partition_samples-31 in /var/lib/kafka/data/_confluent_balancer_partition_samples-31 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 3600000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:39,302] INFO [Partition _confluent_balancer_partition_samples-31 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-31 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:39,303] INFO [Partition _confluent_balancer_partition_samples-31 broker=0] Log loaded for partition _confluent_balancer_partition_samples-31 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:39,303] INFO [Broker id=0] Leader _confluent_balancer_partition_samples-31 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:39,487] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 2 epoch 1 for the become-leader transition for partition _confluent_balancer_partition_samples-28 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:39,487] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 2 epoch 1 for the become-leader transition for partition _confluent_balancer_partition_samples-13 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:39,495] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 2 epoch 1 for the become-leader transition for partition _confluent_balancer_partition_samples-10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:39,495] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 2 epoch 1 for the become-leader transition for partition _confluent_balancer_partition_samples-25 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:39,502] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 2 epoch 1 for the become-leader transition for partition _confluent_balancer_partition_samples-22 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:39,502] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 2 epoch 1 for the become-leader transition for partition _confluent_balancer_partition_samples-7 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:39,503] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 2 epoch 1 for the become-leader transition for partition _confluent_balancer_partition_samples-4 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:39,503] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 2 epoch 1 for the become-leader transition for partition _confluent_balancer_partition_samples-19 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:39,504] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 2 epoch 1 for the become-leader transition for partition _confluent_balancer_partition_samples-16 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:39,504] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 2 epoch 1 for the become-leader transition for partition _confluent_balancer_partition_samples-1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:39,504] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 2 epoch 1 for the become-leader transition for partition _confluent_balancer_partition_samples-31 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:39,529] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 2 epoch 1 starting the become-follower transition for partition _confluent_balancer_partition_samples-26 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:39,538] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 2 epoch 1 starting the become-follower transition for partition _confluent_balancer_partition_samples-27 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:39,538] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 2 epoch 1 starting the become-follower transition for partition _confluent_balancer_partition_samples-8 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:39,538] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 2 epoch 1 starting the become-follower transition for partition _confluent_balancer_partition_samples-9 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:39,539] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 2 epoch 1 starting the become-follower transition for partition _confluent_balancer_partition_samples-20 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:39,541] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 2 epoch 1 starting the become-follower transition for partition _confluent_balancer_partition_samples-21 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:39,541] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 2 epoch 1 starting the become-follower transition for partition _confluent_balancer_partition_samples-2 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:39,542] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 2 epoch 1 starting the become-follower transition for partition _confluent_balancer_partition_samples-3 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:39,542] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 2 epoch 1 starting the become-follower transition for partition _confluent_balancer_partition_samples-14 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:39,542] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 2 epoch 1 starting the become-follower transition for partition _confluent_balancer_partition_samples-15 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:39,688] INFO [Log partition=_confluent_balancer_partition_samples-26, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 28 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:39,785] INFO [Log partition=_confluent_balancer_partition_samples-26, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:39,835] INFO [MergedLog partition=_confluent_balancer_partition_samples-26, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:39,877] INFO Created log for partition _confluent_balancer_partition_samples-26 in /var/lib/kafka/data/_confluent_balancer_partition_samples-26 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 3600000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:39,892] INFO [Partition _confluent_balancer_partition_samples-26 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-26 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:39,892] INFO [Partition _confluent_balancer_partition_samples-26 broker=0] Log loaded for partition _confluent_balancer_partition_samples-26 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:39,911] INFO [Broker id=0] Follower _confluent_balancer_partition_samples-26 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:40,085] INFO [Log partition=_confluent_balancer_partition_samples-27, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 77 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:40,089] INFO [Log partition=_confluent_balancer_partition_samples-27, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:40,114] INFO [MergedLog partition=_confluent_balancer_partition_samples-27, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:40,168] INFO Created log for partition _confluent_balancer_partition_samples-27 in /var/lib/kafka/data/_confluent_balancer_partition_samples-27 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 3600000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:40,169] INFO [Partition _confluent_balancer_partition_samples-27 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-27 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:40,172] INFO [Partition _confluent_balancer_partition_samples-27 broker=0] Log loaded for partition _confluent_balancer_partition_samples-27 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:40,172] INFO [Broker id=0] Follower _confluent_balancer_partition_samples-27 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:40,361] INFO [Log partition=_confluent_balancer_partition_samples-8, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 26 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:40,390] INFO [Log partition=_confluent_balancer_partition_samples-8, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:40,400] INFO [MergedLog partition=_confluent_balancer_partition_samples-8, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:40,415] INFO Created log for partition _confluent_balancer_partition_samples-8 in /var/lib/kafka/data/_confluent_balancer_partition_samples-8 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 3600000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:40,415] INFO [Partition _confluent_balancer_partition_samples-8 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-8 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:40,436] INFO [Partition _confluent_balancer_partition_samples-8 broker=0] Log loaded for partition _confluent_balancer_partition_samples-8 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:40,445] INFO [Broker id=0] Follower _confluent_balancer_partition_samples-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:40,511] INFO [Log partition=_confluent_balancer_partition_samples-9, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 32 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:40,545] INFO [Log partition=_confluent_balancer_partition_samples-9, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:40,566] INFO [MergedLog partition=_confluent_balancer_partition_samples-9, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:40,590] INFO Created log for partition _confluent_balancer_partition_samples-9 in /var/lib/kafka/data/_confluent_balancer_partition_samples-9 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 3600000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:40,599] INFO [Partition _confluent_balancer_partition_samples-9 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-9 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:40,608] INFO [Partition _confluent_balancer_partition_samples-9 broker=0] Log loaded for partition _confluent_balancer_partition_samples-9 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:40,613] INFO [Broker id=0] Follower _confluent_balancer_partition_samples-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:40,795] INFO AdminClientConfig values:
[36mkafka1             |[0m 	bootstrap.servers = [kafka1:9092]
[36mkafka1             |[0m 	client.dns.lookup = use_all_dns_ips
[36mkafka1             |[0m 	client.id =
[36mkafka1             |[0m 	connections.max.idle.ms = 300000
[36mkafka1             |[0m 	default.api.timeout.ms = 60000
[36mkafka1             |[0m 	metadata.max.age.ms = 300000
[36mkafka1             |[0m 	metric.reporters = []
[36mkafka1             |[0m 	metrics.num.samples = 2
[36mkafka1             |[0m 	metrics.recording.level = INFO
[36mkafka1             |[0m 	metrics.sample.window.ms = 30000
[36mkafka1             |[0m 	receive.buffer.bytes = 65536
[36mkafka1             |[0m 	reconnect.backoff.max.ms = 1000
[36mkafka1             |[0m 	reconnect.backoff.ms = 50
[36mkafka1             |[0m 	request.timeout.ms = 30000
[36mkafka1             |[0m 	retries = 2147483647
[36mkafka1             |[0m 	retry.backoff.ms = 100
[36mkafka1             |[0m 	sasl.client.callback.handler.class = null
[36mkafka1             |[0m 	sasl.jaas.config = null
[36mkafka1             |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mkafka1             |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mkafka1             |[0m 	sasl.kerberos.service.name = null
[36mkafka1             |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mkafka1             |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mkafka1             |[0m 	sasl.login.callback.handler.class = null
[36mkafka1             |[0m 	sasl.login.class = null
[36mkafka1             |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mkafka1             |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mkafka1             |[0m 	sasl.login.refresh.window.factor = 0.8
[36mkafka1             |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mkafka1             |[0m 	sasl.mechanism = GSSAPI
[36mkafka1             |[0m 	security.protocol = PLAINTEXT
[36mkafka1             |[0m 	security.providers = null
[36mkafka1             |[0m 	send.buffer.bytes = 131072
[36mkafka1             |[0m 	ssl.cipher.suites = null
[36mkafka1             |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mkafka1             |[0m 	ssl.endpoint.identification.algorithm = https
[36mkafka1             |[0m 	ssl.engine.factory.class = null
[36mkafka1             |[0m 	ssl.key.password = null
[36mkafka1             |[0m 	ssl.keymanager.algorithm = SunX509
[36mkafka1             |[0m 	ssl.keystore.location = null
[36mkafka1             |[0m 	ssl.keystore.password = null
[36mkafka1             |[0m 	ssl.keystore.type = JKS
[36mkafka1             |[0m 	ssl.protocol = TLSv1.3
[36mkafka1             |[0m 	ssl.provider = null
[36mkafka1             |[0m 	ssl.secure.random.implementation = null
[36mkafka1             |[0m 	ssl.trustmanager.algorithm = PKIX
[36mkafka1             |[0m 	ssl.truststore.location = null
[36mkafka1             |[0m 	ssl.truststore.password = null
[36mkafka1             |[0m 	ssl.truststore.type = JKS
[36mkafka1             |[0m  (org.apache.kafka.clients.admin.AdminClientConfig)
[36mkafka1             |[0m [2021-01-09 19:50:40,945] INFO [Log partition=_confluent_balancer_partition_samples-20, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 224 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:40,958] INFO [Log partition=_confluent_balancer_partition_samples-20, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:40,983] INFO [MergedLog partition=_confluent_balancer_partition_samples-20, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:40,986] INFO Created log for partition _confluent_balancer_partition_samples-20 in /var/lib/kafka/data/_confluent_balancer_partition_samples-20 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 3600000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:41,006] INFO [Partition _confluent_balancer_partition_samples-20 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-20 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:41,007] INFO [Partition _confluent_balancer_partition_samples-20 broker=0] Log loaded for partition _confluent_balancer_partition_samples-20 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:41,008] INFO [Broker id=0] Follower _confluent_balancer_partition_samples-20 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:41,088] INFO [Log partition=_confluent_balancer_partition_samples-21, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 13 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:41,093] INFO [Log partition=_confluent_balancer_partition_samples-21, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:41,103] INFO [MergedLog partition=_confluent_balancer_partition_samples-21, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:41,106] INFO Created log for partition _confluent_balancer_partition_samples-21 in /var/lib/kafka/data/_confluent_balancer_partition_samples-21 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 3600000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:41,113] INFO [Partition _confluent_balancer_partition_samples-21 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-21 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:41,120] INFO [Partition _confluent_balancer_partition_samples-21 broker=0] Log loaded for partition _confluent_balancer_partition_samples-21 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:41,120] INFO [Broker id=0] Follower _confluent_balancer_partition_samples-21 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:41,246] INFO [Log partition=_confluent_balancer_partition_samples-2, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 61 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:41,251] INFO [Log partition=_confluent_balancer_partition_samples-2, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:41,263] INFO [MergedLog partition=_confluent_balancer_partition_samples-2, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:41,280] INFO Created log for partition _confluent_balancer_partition_samples-2 in /var/lib/kafka/data/_confluent_balancer_partition_samples-2 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 3600000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:41,280] INFO [Partition _confluent_balancer_partition_samples-2 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-2 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:41,284] INFO [Partition _confluent_balancer_partition_samples-2 broker=0] Log loaded for partition _confluent_balancer_partition_samples-2 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:41,285] INFO [Broker id=0] Follower _confluent_balancer_partition_samples-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:41,339] INFO [Log partition=_confluent_balancer_partition_samples-3, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 22 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:41,356] INFO [Log partition=_confluent_balancer_partition_samples-3, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:41,369] INFO [MergedLog partition=_confluent_balancer_partition_samples-3, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:41,380] INFO Created log for partition _confluent_balancer_partition_samples-3 in /var/lib/kafka/data/_confluent_balancer_partition_samples-3 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 3600000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:41,383] INFO [Partition _confluent_balancer_partition_samples-3 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-3 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:41,383] INFO [Partition _confluent_balancer_partition_samples-3 broker=0] Log loaded for partition _confluent_balancer_partition_samples-3 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:41,385] INFO [Broker id=0] Follower _confluent_balancer_partition_samples-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:41,453] INFO [Log partition=_confluent_balancer_partition_samples-14, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 16 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:41,501] INFO [Log partition=_confluent_balancer_partition_samples-14, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:41,507] INFO [MergedLog partition=_confluent_balancer_partition_samples-14, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:41,528] INFO Created log for partition _confluent_balancer_partition_samples-14 in /var/lib/kafka/data/_confluent_balancer_partition_samples-14 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 3600000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:41,538] INFO [Partition _confluent_balancer_partition_samples-14 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-14 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:41,552] INFO [Partition _confluent_balancer_partition_samples-14 broker=0] Log loaded for partition _confluent_balancer_partition_samples-14 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:41,553] INFO [Broker id=0] Follower _confluent_balancer_partition_samples-14 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:41,563] WARN The configuration 'topic.replicas' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mkafka1             |[0m [2021-01-09 19:50:41,589] INFO [Log partition=_confluent_balancer_partition_samples-15, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 26 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:41,585] INFO Kafka version: 6.0.1-ce (org.apache.kafka.common.utils.AppInfoParser)
[36mkafka1             |[0m [2021-01-09 19:50:41,606] INFO Kafka commitId: f75f566c7a4b38d8 (org.apache.kafka.common.utils.AppInfoParser)
[36mkafka1             |[0m [2021-01-09 19:50:41,606] INFO Kafka startTimeMs: 1610221841585 (org.apache.kafka.common.utils.AppInfoParser)
[36mkafka1             |[0m [2021-01-09 19:50:41,611] INFO [Log partition=_confluent_balancer_partition_samples-15, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:41,611] INFO [MergedLog partition=_confluent_balancer_partition_samples-15, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:41,643] INFO Created log for partition _confluent_balancer_partition_samples-15 in /var/lib/kafka/data/_confluent_balancer_partition_samples-15 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 3600000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:41,643] INFO [Partition _confluent_balancer_partition_samples-15 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-15 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:41,643] INFO [Partition _confluent_balancer_partition_samples-15 broker=0] Log loaded for partition _confluent_balancer_partition_samples-15 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:41,643] INFO [Broker id=0] Follower _confluent_balancer_partition_samples-15 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:41,653] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent_balancer_partition_samples-21, _confluent_balancer_partition_samples-8, _confluent_balancer_partition_samples-26, _confluent_balancer_partition_samples-9, _confluent_balancer_partition_samples-27, _confluent_balancer_partition_samples-14, _confluent_balancer_partition_samples-15, _confluent_balancer_partition_samples-2, _confluent_balancer_partition_samples-20, _confluent_balancer_partition_samples-3) (kafka.server.ReplicaFetcherManager)
[36mkafka1             |[0m [2021-01-09 19:50:41,697] INFO [Broker id=0] Stopped fetchers as part of become-follower request from controller 2 epoch 1 with correlation id 1 for 10 partitions (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:42,686] INFO [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Starting (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:43,332] INFO [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-8 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:43,495] INFO [ReplicaFetcherManager on broker 0] Added fetcher to broker 2 for partitions HashMap(_confluent_balancer_partition_samples-2 -> (offset=0, leaderEpoch=0), _confluent_balancer_partition_samples-8 -> (offset=0, leaderEpoch=0), _confluent_balancer_partition_samples-26 -> (offset=0, leaderEpoch=0), _confluent_balancer_partition_samples-14 -> (offset=0, leaderEpoch=0), _confluent_balancer_partition_samples-20 -> (offset=0, leaderEpoch=0)) (kafka.server.ReplicaFetcherManager)
[36mkafka1             |[0m [2021-01-09 19:50:43,526] INFO [Log partition=_confluent_balancer_partition_samples-8, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:43,565] INFO [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-26 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:43,566] INFO [Log partition=_confluent_balancer_partition_samples-26, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:43,566] INFO [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-14 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:43,576] INFO [Log partition=_confluent_balancer_partition_samples-14, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:43,577] INFO [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-2 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:43,577] INFO [Log partition=_confluent_balancer_partition_samples-2, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:43,577] INFO [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-20 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:43,577] INFO [Log partition=_confluent_balancer_partition_samples-20, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:43,641] INFO [ReplicaFetcherManager on broker 0] Added fetcher to broker 1 for partitions HashMap(_confluent_balancer_partition_samples-27 -> (offset=0, leaderEpoch=0), _confluent_balancer_partition_samples-21 -> (offset=0, leaderEpoch=0), _confluent_balancer_partition_samples-9 -> (offset=0, leaderEpoch=0), _confluent_balancer_partition_samples-3 -> (offset=0, leaderEpoch=0), _confluent_balancer_partition_samples-15 -> (offset=0, leaderEpoch=0)) (kafka.server.ReplicaFetcherManager)
[36mkafka1             |[0m [2021-01-09 19:50:43,651] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 2 epoch 1 for the become-follower transition for partition _confluent_balancer_partition_samples-26 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:43,651] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 2 epoch 1 for the become-follower transition for partition _confluent_balancer_partition_samples-27 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:43,651] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 2 epoch 1 for the become-follower transition for partition _confluent_balancer_partition_samples-8 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:43,685] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 2 epoch 1 for the become-follower transition for partition _confluent_balancer_partition_samples-9 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:43,685] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 2 epoch 1 for the become-follower transition for partition _confluent_balancer_partition_samples-20 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:43,685] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 2 epoch 1 for the become-follower transition for partition _confluent_balancer_partition_samples-21 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:43,685] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 2 epoch 1 for the become-follower transition for partition _confluent_balancer_partition_samples-2 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:43,685] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 2 epoch 1 for the become-follower transition for partition _confluent_balancer_partition_samples-3 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:43,685] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 2 epoch 1 for the become-follower transition for partition _confluent_balancer_partition_samples-14 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:43,685] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 1 from controller 2 epoch 1 for the become-follower transition for partition _confluent_balancer_partition_samples-15 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:43,631] ERROR Error checking or creating metrics topic (io.confluent.metrics.reporter.ConfluentMetricsReporter)
[36mkafka1             |[0m org.apache.kafka.common.errors.TopicExistsException: Topic '_confluent-metrics' already exists.
[36mkafka1             |[0m [2021-01-09 19:50:43,677] INFO [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Starting (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:43,826] INFO [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-21 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:43,828] INFO [Log partition=_confluent_balancer_partition_samples-21, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:43,829] INFO [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-9 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:43,830] INFO [Log partition=_confluent_balancer_partition_samples-9, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:43,831] INFO [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-27 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:43,832] INFO [Log partition=_confluent_balancer_partition_samples-27, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:43,832] INFO [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-15 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:43,832] INFO [Log partition=_confluent_balancer_partition_samples-15, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:43,832] INFO [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-3 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:43,835] INFO [Log partition=_confluent_balancer_partition_samples-3, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:44,121] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition _confluent_balancer_partition_samples-21 at offset 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[36mkafka1             |[0m [2021-01-09 19:50:44,142] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition _confluent_balancer_partition_samples-9 at offset 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[36mkafka1             |[0m [2021-01-09 19:50:44,142] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition _confluent_balancer_partition_samples-27 at offset 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[36mkafka1             |[0m [2021-01-09 19:50:44,142] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition _confluent_balancer_partition_samples-15 at offset 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[36mkafka1             |[0m [2021-01-09 19:50:44,143] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition _confluent_balancer_partition_samples-3 at offset 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[36mkafka1             |[0m [2021-01-09 19:50:44,161] INFO [Broker id=0] Finished LeaderAndIsr request in 11147ms correlationId 1 from controller 2 for 21 partitions (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:44,957] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=5, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1], zkVersion=0, replicas=[2, 1], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-5 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:44,959] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=7, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-7 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:44,959] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=9, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-9 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:44,984] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=11, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1], zkVersion=0, replicas=[2, 1], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-11 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:44,984] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=30, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2], zkVersion=0, replicas=[1, 2], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-30 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:44,997] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=1, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-1 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,001] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-3 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,009] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=22, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-22 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,009] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=24, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2], zkVersion=0, replicas=[1, 2], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-24 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,034] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=26, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-26 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,035] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=28, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-28 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,035] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=14, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-14 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,038] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=16, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-16 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,040] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=18, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2], zkVersion=0, replicas=[1, 2], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-18 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,062] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=20, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-20 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,062] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=6, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2], zkVersion=0, replicas=[1, 2], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-6 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,062] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=8, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-8 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,063] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=10, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-10 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,063] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=12, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2], zkVersion=0, replicas=[1, 2], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-12 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,067] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=29, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1], zkVersion=0, replicas=[2, 1], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-29 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,067] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2], zkVersion=0, replicas=[1, 2], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-0 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,067] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=31, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-31 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,067] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=2, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-2 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,067] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=4, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-4 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,068] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=21, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-21 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,070] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=23, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1], zkVersion=0, replicas=[2, 1], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-23 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,082] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=25, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-25 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,082] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=27, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-27 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,083] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=13, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-13 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,088] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=15, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-15 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,110] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=17, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1], zkVersion=0, replicas=[2, 1], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-17 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,111] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_partition_samples', partitionIndex=19, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], offlineReplicas=[]) for partition _confluent_balancer_partition_samples-19 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,453] INFO [Broker id=0] Add 32 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,609] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 2 for 8 partitions (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,609] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent-metrics', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=11, controllerEpoch=1, leader=2, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,610] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent-metrics', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=10, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,611] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent-metrics', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=7, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,616] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent-metrics', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=5, controllerEpoch=1, leader=2, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,624] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent-metrics', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=6, controllerEpoch=1, leader=1, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,624] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent-metrics', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=4, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,624] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent-metrics', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=1, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:45,624] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent-metrics', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=0, controllerEpoch=1, leader=1, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:46,214] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 2 epoch 1 starting the become-leader transition for partition _confluent-metrics-4 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:46,214] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 2 epoch 1 starting the become-leader transition for partition _confluent-metrics-7 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:46,219] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 2 epoch 1 starting the become-leader transition for partition _confluent-metrics-10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:46,221] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 2 epoch 1 starting the become-leader transition for partition _confluent-metrics-1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:46,555] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-metrics-4, _confluent-metrics-7, _confluent-metrics-10, _confluent-metrics-1) (kafka.server.ReplicaFetcherManager)
[36mkafka1             |[0m [2021-01-09 19:50:46,559] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 2 epoch 1 as part of the become-leader transition for 4 partitions (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:46,646] INFO [Log partition=_confluent-metrics-4, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 25 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:46,651] INFO [Log partition=_confluent-metrics-4, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:46,653] INFO [MergedLog partition=_confluent-metrics-4, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:46,681] INFO Created log for partition _confluent-metrics-4 in /var/lib/kafka/data/_confluent-metrics-4 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 259200000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 14400000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:46,683] INFO [Partition _confluent-metrics-4 broker=0] No checkpointed highwatermark is found for partition _confluent-metrics-4 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:46,689] INFO [Partition _confluent-metrics-4 broker=0] Log loaded for partition _confluent-metrics-4 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:46,690] INFO [Broker id=0] Leader _confluent-metrics-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:46,810] INFO [Log partition=_confluent-metrics-7, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 54 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:46,936] INFO [Log partition=_confluent-metrics-7, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:46,938] INFO [MergedLog partition=_confluent-metrics-7, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:46,948] INFO Created log for partition _confluent-metrics-7 in /var/lib/kafka/data/_confluent-metrics-7 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 259200000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 14400000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:46,948] INFO [Partition _confluent-metrics-7 broker=0] No checkpointed highwatermark is found for partition _confluent-metrics-7 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:46,948] INFO [Partition _confluent-metrics-7 broker=0] Log loaded for partition _confluent-metrics-7 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:46,962] INFO [Broker id=0] Leader _confluent-metrics-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:46,986] INFO [Log partition=_confluent-metrics-10, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:47,001] INFO [Log partition=_confluent-metrics-10, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:47,047] INFO [MergedLog partition=_confluent-metrics-10, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:47,057] INFO Created log for partition _confluent-metrics-10 in /var/lib/kafka/data/_confluent-metrics-10 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 259200000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 14400000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:47,057] INFO [Partition _confluent-metrics-10 broker=0] No checkpointed highwatermark is found for partition _confluent-metrics-10 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:47,057] INFO [Partition _confluent-metrics-10 broker=0] Log loaded for partition _confluent-metrics-10 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:47,057] INFO [Broker id=0] Leader _confluent-metrics-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:47,168] INFO [Log partition=_confluent-metrics-1, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 72 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:47,205] INFO [Log partition=_confluent-metrics-1, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:47,210] INFO [MergedLog partition=_confluent-metrics-1, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:47,227] INFO Created log for partition _confluent-metrics-1 in /var/lib/kafka/data/_confluent-metrics-1 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 259200000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 14400000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:47,227] INFO [Partition _confluent-metrics-1 broker=0] No checkpointed highwatermark is found for partition _confluent-metrics-1 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:47,227] INFO [Partition _confluent-metrics-1 broker=0] Log loaded for partition _confluent-metrics-1 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:47,227] INFO [Broker id=0] Leader _confluent-metrics-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:47,228] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 3 from controller 2 epoch 1 for the become-leader transition for partition _confluent-metrics-4 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:47,228] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 3 from controller 2 epoch 1 for the become-leader transition for partition _confluent-metrics-7 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:47,228] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 3 from controller 2 epoch 1 for the become-leader transition for partition _confluent-metrics-10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:47,228] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 3 from controller 2 epoch 1 for the become-leader transition for partition _confluent-metrics-1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:47,228] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 2 epoch 1 starting the become-follower transition for partition _confluent-metrics-5 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:47,228] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 2 epoch 1 starting the become-follower transition for partition _confluent-metrics-6 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:47,229] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 2 epoch 1 starting the become-follower transition for partition _confluent-metrics-11 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:47,248] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 2 epoch 1 starting the become-follower transition for partition _confluent-metrics-0 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:47,324] INFO [Log partition=_confluent-metrics-5, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 40 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:47,355] INFO [Log partition=_confluent-metrics-5, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:47,383] INFO [MergedLog partition=_confluent-metrics-5, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:47,391] INFO Created log for partition _confluent-metrics-5 in /var/lib/kafka/data/_confluent-metrics-5 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 259200000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 14400000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:47,391] INFO [Partition _confluent-metrics-5 broker=0] No checkpointed highwatermark is found for partition _confluent-metrics-5 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:47,391] INFO [Partition _confluent-metrics-5 broker=0] Log loaded for partition _confluent-metrics-5 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:47,441] INFO [Broker id=0] Follower _confluent-metrics-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:47,636] INFO [Log partition=_confluent-metrics-6, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 20 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:47,655] INFO [Log partition=_confluent-metrics-6, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:47,814] INFO [MergedLog partition=_confluent-metrics-6, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:48,037] INFO Created log for partition _confluent-metrics-6 in /var/lib/kafka/data/_confluent-metrics-6 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 259200000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 14400000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:48,037] INFO [Partition _confluent-metrics-6 broker=0] No checkpointed highwatermark is found for partition _confluent-metrics-6 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:48,037] INFO [Partition _confluent-metrics-6 broker=0] Log loaded for partition _confluent-metrics-6 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:48,038] INFO [Broker id=0] Follower _confluent-metrics-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:48,134] INFO [Log partition=_confluent-metrics-11, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 20 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:48,180] INFO [Log partition=_confluent-metrics-11, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:48,241] INFO [MergedLog partition=_confluent-metrics-11, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:48,368] INFO Created log for partition _confluent-metrics-11 in /var/lib/kafka/data/_confluent-metrics-11 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 259200000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 14400000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:48,369] INFO [Partition _confluent-metrics-11 broker=0] No checkpointed highwatermark is found for partition _confluent-metrics-11 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:48,369] INFO [Partition _confluent-metrics-11 broker=0] Log loaded for partition _confluent-metrics-11 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:48,370] INFO [Broker id=0] Follower _confluent-metrics-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:48,415] INFO [Log partition=_confluent-metrics-0, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 17 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:48,444] INFO [Log partition=_confluent-metrics-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:48,475] INFO [MergedLog partition=_confluent-metrics-0, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:48,614] INFO Created log for partition _confluent-metrics-0 in /var/lib/kafka/data/_confluent-metrics-0 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 259200000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 10485760, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 14400000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:48,614] INFO [Partition _confluent-metrics-0 broker=0] No checkpointed highwatermark is found for partition _confluent-metrics-0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:48,614] INFO [Partition _confluent-metrics-0 broker=0] Log loaded for partition _confluent-metrics-0 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:48,614] INFO [Broker id=0] Follower _confluent-metrics-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:48,615] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-metrics-11, _confluent-metrics-5, _confluent-metrics-6, _confluent-metrics-0) (kafka.server.ReplicaFetcherManager)
[36mkafka1             |[0m [2021-01-09 19:50:48,615] INFO [Broker id=0] Stopped fetchers as part of become-follower request from controller 2 epoch 1 with correlation id 3 for 4 partitions (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:48,616] INFO [ReplicaFetcherManager on broker 0] Added fetcher to broker 2 for partitions Map(_confluent-metrics-11 -> (offset=0, leaderEpoch=0), _confluent-metrics-5 -> (offset=0, leaderEpoch=0)) (kafka.server.ReplicaFetcherManager)
[36mkafka1             |[0m [2021-01-09 19:50:48,616] INFO [ReplicaFetcherManager on broker 0] Added fetcher to broker 1 for partitions Map(_confluent-metrics-0 -> (offset=0, leaderEpoch=0), _confluent-metrics-6 -> (offset=0, leaderEpoch=0)) (kafka.server.ReplicaFetcherManager)
[36mkafka1             |[0m [2021-01-09 19:50:48,616] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 3 from controller 2 epoch 1 for the become-follower transition for partition _confluent-metrics-5 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:48,616] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 3 from controller 2 epoch 1 for the become-follower transition for partition _confluent-metrics-6 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:48,616] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 3 from controller 2 epoch 1 for the become-follower transition for partition _confluent-metrics-11 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:48,616] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 3 from controller 2 epoch 1 for the become-follower transition for partition _confluent-metrics-0 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:48,757] INFO [Broker id=0] Finished LeaderAndIsr request in 3148ms correlationId 3 from controller 2 for 8 partitions (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:48,850] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent-metrics', partitionIndex=11, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], offlineReplicas=[]) for partition _confluent-metrics-11 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 4 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:48,911] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent-metrics', partitionIndex=9, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2], zkVersion=0, replicas=[1, 2], observers=[], offlineReplicas=[]) for partition _confluent-metrics-9 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 4 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:48,915] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent-metrics', partitionIndex=10, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], offlineReplicas=[]) for partition _confluent-metrics-10 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 4 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:48,926] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent-metrics', partitionIndex=7, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], offlineReplicas=[]) for partition _confluent-metrics-7 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 4 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:48,932] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent-metrics', partitionIndex=8, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1], zkVersion=0, replicas=[2, 1], observers=[], offlineReplicas=[]) for partition _confluent-metrics-8 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 4 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:48,946] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent-metrics', partitionIndex=5, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], offlineReplicas=[]) for partition _confluent-metrics-5 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 4 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:48,948] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent-metrics', partitionIndex=6, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], offlineReplicas=[]) for partition _confluent-metrics-6 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 4 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:48,950] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent-metrics', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2], zkVersion=0, replicas=[1, 2], observers=[], offlineReplicas=[]) for partition _confluent-metrics-3 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 4 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:48,954] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent-metrics', partitionIndex=4, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], offlineReplicas=[]) for partition _confluent-metrics-4 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 4 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:48,957] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent-metrics', partitionIndex=1, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], offlineReplicas=[]) for partition _confluent-metrics-1 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 4 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:48,959] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent-metrics', partitionIndex=2, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1], zkVersion=0, replicas=[2, 1], observers=[], offlineReplicas=[]) for partition _confluent-metrics-2 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 4 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:48,962] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent-metrics', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], offlineReplicas=[]) for partition _confluent-metrics-0 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 4 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:48,983] INFO [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 4 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:49,090] INFO [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-metrics-6 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:49,090] INFO [Log partition=_confluent-metrics-6, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:49,090] INFO [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-metrics-0 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:49,091] INFO [Log partition=_confluent-metrics-0, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:49,156] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 2 for 22 partitions (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:49,157] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_broker_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=23, controllerEpoch=1, leader=1, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 5 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:49,157] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_broker_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=25, controllerEpoch=1, leader=2, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 5 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:49,158] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_broker_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=24, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 5 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:49,160] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_broker_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=27, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 5 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:49,161] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_broker_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=29, controllerEpoch=1, leader=1, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 5 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:49,194] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_broker_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=15, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 5 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:49,195] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_broker_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=17, controllerEpoch=1, leader=1, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 5 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:49,196] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_broker_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=19, controllerEpoch=1, leader=2, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 5 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:49,201] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_broker_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=18, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 5 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:49,202] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_broker_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=21, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 5 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:49,203] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_broker_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=7, controllerEpoch=1, leader=2, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 5 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:49,203] INFO [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-metrics-11 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:49,205] INFO [Log partition=_confluent-metrics-11, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:49,206] INFO [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-metrics-5 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:49,206] INFO [Log partition=_confluent-metrics-5, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:49,205] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_broker_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=6, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 5 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:49,210] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_broker_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=9, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 5 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:49,220] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_broker_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=11, controllerEpoch=1, leader=1, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 5 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:49,227] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_broker_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=13, controllerEpoch=1, leader=2, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 5 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:49,231] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_broker_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=12, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 5 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:49,232] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_broker_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=31, controllerEpoch=1, leader=2, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 5 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:49,238] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_broker_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=30, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 5 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:49,241] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_broker_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=1, controllerEpoch=1, leader=2, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 5 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:49,243] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_broker_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=0, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 5 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:49,244] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_broker_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=3, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 5 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:49,244] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent_balancer_broker_samples', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=5, controllerEpoch=1, leader=1, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 5 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:49,702] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition _confluent-metrics-6 at offset 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[36mkafka1             |[0m [2021-01-09 19:50:49,702] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition _confluent-metrics-0 at offset 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[36mkafka1             |[0m [2021-01-09 19:50:50,159] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 2 epoch 1 starting the become-leader transition for partition _confluent_balancer_broker_samples-27 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:50,174] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 2 epoch 1 starting the become-leader transition for partition _confluent_balancer_broker_samples-12 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:50,184] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 2 epoch 1 starting the become-leader transition for partition _confluent_balancer_broker_samples-9 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:50,185] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 2 epoch 1 starting the become-leader transition for partition _confluent_balancer_broker_samples-24 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:50,186] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 2 epoch 1 starting the become-leader transition for partition _confluent_balancer_broker_samples-21 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:50,186] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 2 epoch 1 starting the become-leader transition for partition _confluent_balancer_broker_samples-6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:50,188] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 2 epoch 1 starting the become-leader transition for partition _confluent_balancer_broker_samples-3 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:50,193] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 2 epoch 1 starting the become-leader transition for partition _confluent_balancer_broker_samples-18 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:50,193] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 2 epoch 1 starting the become-leader transition for partition _confluent_balancer_broker_samples-15 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:50,194] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 2 epoch 1 starting the become-leader transition for partition _confluent_balancer_broker_samples-0 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:50,195] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 2 epoch 1 starting the become-leader transition for partition _confluent_balancer_broker_samples-30 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:50,196] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent_balancer_broker_samples-24, _confluent_balancer_broker_samples-21, _confluent_balancer_broker_samples-3, _confluent_balancer_broker_samples-15, _confluent_balancer_broker_samples-30, _confluent_balancer_broker_samples-0, _confluent_balancer_broker_samples-9, _confluent_balancer_broker_samples-18, _confluent_balancer_broker_samples-12, _confluent_balancer_broker_samples-27, _confluent_balancer_broker_samples-6) (kafka.server.ReplicaFetcherManager)
[36mkafka1             |[0m [2021-01-09 19:50:50,197] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 2 epoch 1 as part of the become-leader transition for 11 partitions (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:50,230] INFO [Log partition=_confluent_balancer_broker_samples-27, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 7 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:50,282] INFO [Log partition=_confluent_balancer_broker_samples-27, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:50,309] INFO [MergedLog partition=_confluent_balancer_broker_samples-27, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:50,509] INFO Created log for partition _confluent_balancer_broker_samples-27 in /var/lib/kafka/data/_confluent_balancer_broker_samples-27 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 12000000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:50,544] INFO [Partition _confluent_balancer_broker_samples-27 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-27 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:50,554] INFO [Partition _confluent_balancer_broker_samples-27 broker=0] Log loaded for partition _confluent_balancer_broker_samples-27 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:50,555] INFO [Broker id=0] Leader _confluent_balancer_broker_samples-27 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:50,590] INFO [Log partition=_confluent_balancer_broker_samples-12, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:50,592] INFO [Log partition=_confluent_balancer_broker_samples-12, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:50,594] INFO [MergedLog partition=_confluent_balancer_broker_samples-12, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:50,596] INFO Created log for partition _confluent_balancer_broker_samples-12 in /var/lib/kafka/data/_confluent_balancer_broker_samples-12 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 12000000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:50,596] INFO [Partition _confluent_balancer_broker_samples-12 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-12 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:50,597] INFO [Partition _confluent_balancer_broker_samples-12 broker=0] Log loaded for partition _confluent_balancer_broker_samples-12 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:50,597] INFO [Broker id=0] Leader _confluent_balancer_broker_samples-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:50,965] INFO [Log partition=_confluent_balancer_broker_samples-9, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 45 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:50,969] INFO [Log partition=_confluent_balancer_broker_samples-9, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:50,970] INFO [MergedLog partition=_confluent_balancer_broker_samples-9, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:50,974] INFO Created log for partition _confluent_balancer_broker_samples-9 in /var/lib/kafka/data/_confluent_balancer_broker_samples-9 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 12000000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:50,994] INFO [Partition _confluent_balancer_broker_samples-9 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-9 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:50,995] INFO [Partition _confluent_balancer_broker_samples-9 broker=0] Log loaded for partition _confluent_balancer_broker_samples-9 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:50,995] INFO [Broker id=0] Leader _confluent_balancer_broker_samples-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:51,046] INFO [Log partition=_confluent_balancer_broker_samples-24, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 8 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:51,060] INFO [Log partition=_confluent_balancer_broker_samples-24, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:51,076] INFO [MergedLog partition=_confluent_balancer_broker_samples-24, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:51,078] INFO Created log for partition _confluent_balancer_broker_samples-24 in /var/lib/kafka/data/_confluent_balancer_broker_samples-24 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 12000000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:51,079] INFO [Partition _confluent_balancer_broker_samples-24 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-24 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:51,079] INFO [Partition _confluent_balancer_broker_samples-24 broker=0] Log loaded for partition _confluent_balancer_broker_samples-24 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:51,093] INFO [Broker id=0] Leader _confluent_balancer_broker_samples-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:51,177] INFO [Log partition=_confluent_balancer_broker_samples-21, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 3 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:51,185] INFO [Log partition=_confluent_balancer_broker_samples-21, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:51,192] INFO [MergedLog partition=_confluent_balancer_broker_samples-21, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:51,200] INFO Created log for partition _confluent_balancer_broker_samples-21 in /var/lib/kafka/data/_confluent_balancer_broker_samples-21 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 12000000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:51,201] INFO [Partition _confluent_balancer_broker_samples-21 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-21 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:51,202] INFO [Partition _confluent_balancer_broker_samples-21 broker=0] Log loaded for partition _confluent_balancer_broker_samples-21 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:51,220] INFO [Broker id=0] Leader _confluent_balancer_broker_samples-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:51,250] INFO [Log partition=_confluent_balancer_broker_samples-6, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 9 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:51,311] INFO [Log partition=_confluent_balancer_broker_samples-6, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:51,328] INFO [MergedLog partition=_confluent_balancer_broker_samples-6, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:51,352] INFO Created log for partition _confluent_balancer_broker_samples-6 in /var/lib/kafka/data/_confluent_balancer_broker_samples-6 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 12000000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:51,361] INFO [Partition _confluent_balancer_broker_samples-6 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-6 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:51,362] INFO [Partition _confluent_balancer_broker_samples-6 broker=0] Log loaded for partition _confluent_balancer_broker_samples-6 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:51,367] INFO [Broker id=0] Leader _confluent_balancer_broker_samples-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:51,409] INFO [Log partition=_confluent_balancer_broker_samples-3, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 15 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:51,474] INFO [Log partition=_confluent_balancer_broker_samples-3, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:51,476] INFO [MergedLog partition=_confluent_balancer_broker_samples-3, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:51,478] INFO Created log for partition _confluent_balancer_broker_samples-3 in /var/lib/kafka/data/_confluent_balancer_broker_samples-3 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 12000000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:51,483] INFO [Partition _confluent_balancer_broker_samples-3 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-3 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:51,488] INFO [Partition _confluent_balancer_broker_samples-3 broker=0] Log loaded for partition _confluent_balancer_broker_samples-3 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:51,492] INFO [Broker id=0] Leader _confluent_balancer_broker_samples-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:51,536] INFO [Log partition=_confluent_balancer_broker_samples-18, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:51,538] INFO [Log partition=_confluent_balancer_broker_samples-18, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:51,542] INFO [MergedLog partition=_confluent_balancer_broker_samples-18, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:51,545] INFO Created log for partition _confluent_balancer_broker_samples-18 in /var/lib/kafka/data/_confluent_balancer_broker_samples-18 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 12000000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:51,546] INFO [Partition _confluent_balancer_broker_samples-18 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-18 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:51,547] INFO [Partition _confluent_balancer_broker_samples-18 broker=0] Log loaded for partition _confluent_balancer_broker_samples-18 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:51,548] INFO [Broker id=0] Leader _confluent_balancer_broker_samples-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:51,579] INFO [Log partition=_confluent_balancer_broker_samples-15, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 9 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:51,590] INFO [Log partition=_confluent_balancer_broker_samples-15, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:51,592] INFO [MergedLog partition=_confluent_balancer_broker_samples-15, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:51,602] INFO Created log for partition _confluent_balancer_broker_samples-15 in /var/lib/kafka/data/_confluent_balancer_broker_samples-15 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 12000000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:51,637] INFO [Partition _confluent_balancer_broker_samples-15 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-15 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:51,638] INFO [Partition _confluent_balancer_broker_samples-15 broker=0] Log loaded for partition _confluent_balancer_broker_samples-15 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:51,640] INFO [Broker id=0] Leader _confluent_balancer_broker_samples-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:51,718] INFO [Log partition=_confluent_balancer_broker_samples-0, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 10 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:51,740] INFO [Log partition=_confluent_balancer_broker_samples-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:51,750] INFO [MergedLog partition=_confluent_balancer_broker_samples-0, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m Jan 09, 2021 7:50:51 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
[36mkafka1             |[0m WARNING: A provider io.confluent.metadataapi.resources.MetadataResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.metadataapi.resources.MetadataResource will be ignored.
[36mkafka1             |[0m [2021-01-09 19:50:51,760] INFO Created log for partition _confluent_balancer_broker_samples-0 in /var/lib/kafka/data/_confluent_balancer_broker_samples-0 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 12000000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:51,770] INFO [Partition _confluent_balancer_broker_samples-0 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:51,770] INFO [Partition _confluent_balancer_broker_samples-0 broker=0] Log loaded for partition _confluent_balancer_broker_samples-0 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:51,772] INFO [Broker id=0] Leader _confluent_balancer_broker_samples-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:51,843] INFO [Log partition=_confluent_balancer_broker_samples-30, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 59 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:51,863] INFO [Log partition=_confluent_balancer_broker_samples-30, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:52,002] INFO [MergedLog partition=_confluent_balancer_broker_samples-30, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:52,012] INFO Created log for partition _confluent_balancer_broker_samples-30 in /var/lib/kafka/data/_confluent_balancer_broker_samples-30 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 12000000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:52,092] INFO [Partition _confluent_balancer_broker_samples-30 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-30 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:52,093] INFO [Partition _confluent_balancer_broker_samples-30 broker=0] Log loaded for partition _confluent_balancer_broker_samples-30 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:52,093] INFO [Broker id=0] Leader _confluent_balancer_broker_samples-30 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,095] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 5 from controller 2 epoch 1 for the become-leader transition for partition _confluent_balancer_broker_samples-27 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,096] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 5 from controller 2 epoch 1 for the become-leader transition for partition _confluent_balancer_broker_samples-12 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,096] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 5 from controller 2 epoch 1 for the become-leader transition for partition _confluent_balancer_broker_samples-9 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,098] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 5 from controller 2 epoch 1 for the become-leader transition for partition _confluent_balancer_broker_samples-24 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,101] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 5 from controller 2 epoch 1 for the become-leader transition for partition _confluent_balancer_broker_samples-21 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,184] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 5 from controller 2 epoch 1 for the become-leader transition for partition _confluent_balancer_broker_samples-6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,195] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 5 from controller 2 epoch 1 for the become-leader transition for partition _confluent_balancer_broker_samples-3 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,197] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 5 from controller 2 epoch 1 for the become-leader transition for partition _confluent_balancer_broker_samples-18 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,204] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 5 from controller 2 epoch 1 for the become-leader transition for partition _confluent_balancer_broker_samples-15 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,224] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 5 from controller 2 epoch 1 for the become-leader transition for partition _confluent_balancer_broker_samples-0 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,232] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 5 from controller 2 epoch 1 for the become-leader transition for partition _confluent_balancer_broker_samples-30 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,233] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 2 epoch 1 starting the become-follower transition for partition _confluent_balancer_broker_samples-11 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,237] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 2 epoch 1 starting the become-follower transition for partition _confluent_balancer_broker_samples-25 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,245] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 2 epoch 1 starting the become-follower transition for partition _confluent_balancer_broker_samples-23 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,251] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 2 epoch 1 starting the become-follower transition for partition _confluent_balancer_broker_samples-7 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,256] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 2 epoch 1 starting the become-follower transition for partition _confluent_balancer_broker_samples-5 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,258] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 2 epoch 1 starting the become-follower transition for partition _confluent_balancer_broker_samples-19 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,259] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 2 epoch 1 starting the become-follower transition for partition _confluent_balancer_broker_samples-17 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,263] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 2 epoch 1 starting the become-follower transition for partition _confluent_balancer_broker_samples-1 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,264] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 2 epoch 1 starting the become-follower transition for partition _confluent_balancer_broker_samples-31 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,293] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 2 epoch 1 starting the become-follower transition for partition _confluent_balancer_broker_samples-29 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,313] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 2 epoch 1 starting the become-follower transition for partition _confluent_balancer_broker_samples-13 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,331] INFO [Log partition=_confluent_balancer_broker_samples-11, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 4 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:52,346] INFO [Log partition=_confluent_balancer_broker_samples-11, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:52,352] INFO [MergedLog partition=_confluent_balancer_broker_samples-11, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:52,355] INFO Created log for partition _confluent_balancer_broker_samples-11 in /var/lib/kafka/data/_confluent_balancer_broker_samples-11 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 12000000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:52,375] INFO [Partition _confluent_balancer_broker_samples-11 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-11 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:52,376] INFO [Partition _confluent_balancer_broker_samples-11 broker=0] Log loaded for partition _confluent_balancer_broker_samples-11 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:52,380] INFO [Broker id=0] Follower _confluent_balancer_broker_samples-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,428] INFO [Log partition=_confluent_balancer_broker_samples-25, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 9 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:52,433] INFO [Log partition=_confluent_balancer_broker_samples-25, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:52,437] INFO [MergedLog partition=_confluent_balancer_broker_samples-25, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:52,455] INFO Created log for partition _confluent_balancer_broker_samples-25 in /var/lib/kafka/data/_confluent_balancer_broker_samples-25 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 12000000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:52,456] INFO [Partition _confluent_balancer_broker_samples-25 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-25 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:52,461] INFO [Partition _confluent_balancer_broker_samples-25 broker=0] Log loaded for partition _confluent_balancer_broker_samples-25 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:52,464] INFO [Broker id=0] Follower _confluent_balancer_broker_samples-25 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,501] INFO [Log partition=_confluent_balancer_broker_samples-23, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 7 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:52,521] INFO [Log partition=_confluent_balancer_broker_samples-23, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:52,521] INFO [MergedLog partition=_confluent_balancer_broker_samples-23, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:52,522] INFO Created log for partition _confluent_balancer_broker_samples-23 in /var/lib/kafka/data/_confluent_balancer_broker_samples-23 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 12000000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:52,523] INFO [Partition _confluent_balancer_broker_samples-23 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-23 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:52,547] INFO [Partition _confluent_balancer_broker_samples-23 broker=0] Log loaded for partition _confluent_balancer_broker_samples-23 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:52,556] INFO [Broker id=0] Follower _confluent_balancer_broker_samples-23 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,598] INFO [Log partition=_confluent_balancer_broker_samples-7, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 21 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:52,618] INFO [Log partition=_confluent_balancer_broker_samples-7, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:52,624] INFO [MergedLog partition=_confluent_balancer_broker_samples-7, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:52,625] INFO Created log for partition _confluent_balancer_broker_samples-7 in /var/lib/kafka/data/_confluent_balancer_broker_samples-7 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 12000000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:52,625] INFO [Partition _confluent_balancer_broker_samples-7 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-7 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:52,625] INFO [Partition _confluent_balancer_broker_samples-7 broker=0] Log loaded for partition _confluent_balancer_broker_samples-7 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:52,626] INFO [Broker id=0] Follower _confluent_balancer_broker_samples-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:52,705] INFO [Log partition=_confluent_balancer_broker_samples-5, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 59 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:52,726] INFO [Log partition=_confluent_balancer_broker_samples-5, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:52,727] INFO [MergedLog partition=_confluent_balancer_broker_samples-5, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:52,804] INFO Created log for partition _confluent_balancer_broker_samples-5 in /var/lib/kafka/data/_confluent_balancer_broker_samples-5 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 12000000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:52,804] INFO [Partition _confluent_balancer_broker_samples-5 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-5 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:52,804] INFO [Partition _confluent_balancer_broker_samples-5 broker=0] Log loaded for partition _confluent_balancer_broker_samples-5 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:52,805] INFO [Broker id=0] Follower _confluent_balancer_broker_samples-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:53,076] INFO [Log partition=_confluent_balancer_broker_samples-19, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 94 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:53,185] INFO [Log partition=_confluent_balancer_broker_samples-19, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:53,195] INFO [MergedLog partition=_confluent_balancer_broker_samples-19, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:53,228] INFO Created log for partition _confluent_balancer_broker_samples-19 in /var/lib/kafka/data/_confluent_balancer_broker_samples-19 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 12000000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:53,228] INFO [Partition _confluent_balancer_broker_samples-19 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-19 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:53,228] INFO [Partition _confluent_balancer_broker_samples-19 broker=0] Log loaded for partition _confluent_balancer_broker_samples-19 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:53,229] INFO [Broker id=0] Follower _confluent_balancer_broker_samples-19 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:53,294] INFO [Log partition=_confluent_balancer_broker_samples-17, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 22 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:53,296] INFO [Log partition=_confluent_balancer_broker_samples-17, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:53,319] INFO [MergedLog partition=_confluent_balancer_broker_samples-17, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:53,349] INFO Created log for partition _confluent_balancer_broker_samples-17 in /var/lib/kafka/data/_confluent_balancer_broker_samples-17 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 12000000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:53,349] INFO [Partition _confluent_balancer_broker_samples-17 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-17 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:53,349] INFO [Partition _confluent_balancer_broker_samples-17 broker=0] Log loaded for partition _confluent_balancer_broker_samples-17 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:53,352] INFO [Broker id=0] Follower _confluent_balancer_broker_samples-17 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:53,434] INFO [Log partition=_confluent_balancer_broker_samples-1, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 63 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:53,450] INFO [Log partition=_confluent_balancer_broker_samples-1, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:53,491] INFO [MergedLog partition=_confluent_balancer_broker_samples-1, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:53,492] INFO Created log for partition _confluent_balancer_broker_samples-1 in /var/lib/kafka/data/_confluent_balancer_broker_samples-1 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 12000000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:53,492] INFO [Partition _confluent_balancer_broker_samples-1 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-1 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:53,492] INFO [Partition _confluent_balancer_broker_samples-1 broker=0] Log loaded for partition _confluent_balancer_broker_samples-1 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:53,492] INFO [Broker id=0] Follower _confluent_balancer_broker_samples-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:53,531] INFO [Log partition=_confluent_balancer_broker_samples-31, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:53,545] INFO [Log partition=_confluent_balancer_broker_samples-31, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:53,569] INFO [MergedLog partition=_confluent_balancer_broker_samples-31, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:53,592] INFO Created log for partition _confluent_balancer_broker_samples-31 in /var/lib/kafka/data/_confluent_balancer_broker_samples-31 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 12000000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:53,593] INFO [Partition _confluent_balancer_broker_samples-31 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-31 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:53,593] INFO [Partition _confluent_balancer_broker_samples-31 broker=0] Log loaded for partition _confluent_balancer_broker_samples-31 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:53,628] INFO [Broker id=0] Follower _confluent_balancer_broker_samples-31 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:53,664] INFO [Log partition=_confluent_balancer_broker_samples-29, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 32 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:53,686] INFO [Log partition=_confluent_balancer_broker_samples-29, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:53,697] INFO [MergedLog partition=_confluent_balancer_broker_samples-29, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:53,740] INFO Created log for partition _confluent_balancer_broker_samples-29 in /var/lib/kafka/data/_confluent_balancer_broker_samples-29 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 12000000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:53,740] INFO [Partition _confluent_balancer_broker_samples-29 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-29 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:53,754] INFO [Partition _confluent_balancer_broker_samples-29 broker=0] Log loaded for partition _confluent_balancer_broker_samples-29 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:53,754] INFO [Broker id=0] Follower _confluent_balancer_broker_samples-29 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:53,982] INFO [Log partition=_confluent_balancer_broker_samples-13, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 24 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:53,989] INFO [Log partition=_confluent_balancer_broker_samples-13, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:54,032] INFO [MergedLog partition=_confluent_balancer_broker_samples-13, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:54,035] INFO Created log for partition _confluent_balancer_broker_samples-13 in /var/lib/kafka/data/_confluent_balancer_broker_samples-13 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> delete, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 12000000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:54,035] INFO [Partition _confluent_balancer_broker_samples-13 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-13 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:54,058] INFO [Partition _confluent_balancer_broker_samples-13 broker=0] Log loaded for partition _confluent_balancer_broker_samples-13 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:54,059] INFO [Broker id=0] Follower _confluent_balancer_broker_samples-13 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,059] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent_balancer_broker_samples-23, _confluent_balancer_broker_samples-7, _confluent_balancer_broker_samples-25, _confluent_balancer_broker_samples-11, _confluent_balancer_broker_samples-29, _confluent_balancer_broker_samples-13, _confluent_balancer_broker_samples-31, _confluent_balancer_broker_samples-17, _confluent_balancer_broker_samples-1, _confluent_balancer_broker_samples-19, _confluent_balancer_broker_samples-5) (kafka.server.ReplicaFetcherManager)
[36mkafka1             |[0m [2021-01-09 19:50:54,059] INFO [Broker id=0] Stopped fetchers as part of become-follower request from controller 2 epoch 1 with correlation id 5 for 11 partitions (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,062] INFO [ReplicaFetcherManager on broker 0] Added fetcher to broker 2 for partitions HashMap(_confluent_balancer_broker_samples-31 -> (offset=0, leaderEpoch=0), _confluent_balancer_broker_samples-25 -> (offset=0, leaderEpoch=0), _confluent_balancer_broker_samples-13 -> (offset=0, leaderEpoch=0), _confluent_balancer_broker_samples-7 -> (offset=0, leaderEpoch=0), _confluent_balancer_broker_samples-1 -> (offset=0, leaderEpoch=0), _confluent_balancer_broker_samples-19 -> (offset=0, leaderEpoch=0)) (kafka.server.ReplicaFetcherManager)
[36mkafka1             |[0m [2021-01-09 19:50:54,086] INFO [ReplicaFetcherManager on broker 0] Added fetcher to broker 1 for partitions HashMap(_confluent_balancer_broker_samples-17 -> (offset=0, leaderEpoch=0), _confluent_balancer_broker_samples-29 -> (offset=0, leaderEpoch=0), _confluent_balancer_broker_samples-5 -> (offset=0, leaderEpoch=0), _confluent_balancer_broker_samples-23 -> (offset=0, leaderEpoch=0), _confluent_balancer_broker_samples-11 -> (offset=0, leaderEpoch=0)) (kafka.server.ReplicaFetcherManager)
[36mkafka1             |[0m [2021-01-09 19:50:54,086] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 5 from controller 2 epoch 1 for the become-follower transition for partition _confluent_balancer_broker_samples-11 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,086] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 5 from controller 2 epoch 1 for the become-follower transition for partition _confluent_balancer_broker_samples-25 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,087] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 5 from controller 2 epoch 1 for the become-follower transition for partition _confluent_balancer_broker_samples-23 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,087] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 5 from controller 2 epoch 1 for the become-follower transition for partition _confluent_balancer_broker_samples-7 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,087] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 5 from controller 2 epoch 1 for the become-follower transition for partition _confluent_balancer_broker_samples-5 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,109] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 5 from controller 2 epoch 1 for the become-follower transition for partition _confluent_balancer_broker_samples-19 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,109] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 5 from controller 2 epoch 1 for the become-follower transition for partition _confluent_balancer_broker_samples-17 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,109] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 5 from controller 2 epoch 1 for the become-follower transition for partition _confluent_balancer_broker_samples-1 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,110] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 5 from controller 2 epoch 1 for the become-follower transition for partition _confluent_balancer_broker_samples-31 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,110] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 5 from controller 2 epoch 1 for the become-follower transition for partition _confluent_balancer_broker_samples-29 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,110] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 5 from controller 2 epoch 1 for the become-follower transition for partition _confluent_balancer_broker_samples-13 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,122] INFO [Broker id=0] Finished LeaderAndIsr request in 4966ms correlationId 5 from controller 2 for 22 partitions (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,184] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=23, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-23 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,208] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=25, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-25 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,208] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=27, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-27 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,208] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=29, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-29 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,208] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=15, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-15 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,209] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=17, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-17 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,209] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=19, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-19 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,209] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=21, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-21 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,209] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=6, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-6 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,209] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=8, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2], zkVersion=0, replicas=[1, 2], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-8 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,209] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=10, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1], zkVersion=0, replicas=[2, 1], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-10 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,209] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=12, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-12 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,209] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=31, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-31 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,210] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=0, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-0 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,216] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2], zkVersion=0, replicas=[1, 2], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-2 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,232] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=4, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1], zkVersion=0, replicas=[2, 1], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-4 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,232] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=22, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1], zkVersion=0, replicas=[2, 1], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-22 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,232] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=24, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-24 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,232] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=26, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2], zkVersion=0, replicas=[1, 2], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-26 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,233] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=28, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1], zkVersion=0, replicas=[2, 1], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-28 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,233] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=14, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2], zkVersion=0, replicas=[1, 2], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-14 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,233] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=16, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1], zkVersion=0, replicas=[2, 1], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-16 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,233] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=18, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-18 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,233] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=20, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2], zkVersion=0, replicas=[1, 2], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-20 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,233] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=7, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-7 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,233] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=9, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-9 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,233] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=11, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-11 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,234] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=13, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-13 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,234] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=30, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-30 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,234] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=1, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-1 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,234] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=3, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-3 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,234] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent_balancer_broker_samples', partitionIndex=5, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], offlineReplicas=[]) for partition _confluent_balancer_broker_samples-5 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,234] INFO [Broker id=0] Add 32 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 6 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,236] INFO [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-23 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:54,236] INFO [Log partition=_confluent_balancer_broker_samples-23, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:54,237] INFO [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-11 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:54,237] INFO [Log partition=_confluent_balancer_broker_samples-11, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:54,238] INFO [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-29 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:54,238] INFO [Log partition=_confluent_balancer_broker_samples-29, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:54,238] INFO [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-17 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:54,242] INFO [Log partition=_confluent_balancer_broker_samples-17, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:54,294] INFO [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-5 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:54,401] INFO [Log partition=_confluent_balancer_broker_samples-5, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:54,439] INFO [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-7 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:54,442] INFO [Log partition=_confluent_balancer_broker_samples-7, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:54,442] INFO [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-25 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:54,443] INFO [Log partition=_confluent_balancer_broker_samples-25, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:54,443] INFO [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-13 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:54,443] INFO [Log partition=_confluent_balancer_broker_samples-13, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:54,443] INFO [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-31 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:54,443] INFO [Log partition=_confluent_balancer_broker_samples-31, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:54,447] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 2 for 1 partitions (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,447] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_schemas', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=0, controllerEpoch=1, leader=1, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[1, 2, 0], zkVersion=0, replicas=[1, 2, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,444] INFO [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-1 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:54,459] INFO [Log partition=_confluent_balancer_broker_samples-1, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:54,467] INFO [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-19 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:54,467] INFO [Log partition=_confluent_balancer_broker_samples-19, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:54,501] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 2 epoch 1 starting the become-follower transition for partition _schemas-0 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,601] INFO [Log partition=_schemas-0, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 21 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:54,791] INFO [Log partition=_schemas-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:54,792] INFO [MergedLog partition=_schemas-0, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:50:54,794] INFO Created log for partition _schemas-0 in /var/lib/kafka/data/_schemas-0 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 1073741824, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:50:54,851] INFO [Partition _schemas-0 broker=0] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:54,852] INFO [Partition _schemas-0 broker=0] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:50:54,852] INFO [Broker id=0] Follower _schemas-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,853] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_schemas-0) (kafka.server.ReplicaFetcherManager)
[36mkafka1             |[0m [2021-01-09 19:50:54,853] INFO [Broker id=0] Stopped fetchers as part of become-follower request from controller 2 epoch 1 with correlation id 7 for 1 partitions (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:54,896] INFO [ReplicaFetcherManager on broker 0] Added fetcher to broker 1 for partitions Map(_schemas-0 -> (offset=0, leaderEpoch=0)) (kafka.server.ReplicaFetcherManager)
[36mkafka1             |[0m [2021-01-09 19:50:54,896] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 7 from controller 2 epoch 1 for the become-follower transition for partition _schemas-0 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:55,037] INFO [Broker id=0] Finished LeaderAndIsr request in 591ms correlationId 7 from controller 2 for 1 partitions (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:55,194] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition _confluent_balancer_broker_samples-23 at offset 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[36mkafka1             |[0m [2021-01-09 19:50:55,486] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition _confluent_balancer_broker_samples-29 at offset 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[36mkafka1             |[0m [2021-01-09 19:50:55,486] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition _confluent_balancer_broker_samples-17 at offset 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[36mkafka1             |[0m [2021-01-09 19:50:55,497] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition _confluent_balancer_broker_samples-11 at offset 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[36mkafka1             |[0m [2021-01-09 19:50:55,498] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition _confluent_balancer_broker_samples-5 at offset 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[36mkafka1             |[0m [2021-01-09 19:50:55,502] INFO [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _schemas-0 to local high watermark 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:50:55,502] INFO [Log partition=_schemas-0, dir=/var/lib/kafka/data] Truncating to 0 has no effect as the largest offset in the log is -1 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:50:55,616] INFO AdminClientConfig values:
[36mkafka1             |[0m 	bootstrap.servers = [kafka1:9092]
[36mkafka1             |[0m 	client.dns.lookup = use_all_dns_ips
[36mkafka1             |[0m 	client.id =
[36mkafka1             |[0m 	connections.max.idle.ms = 300000
[36mkafka1             |[0m 	default.api.timeout.ms = 60000
[36mkafka1             |[0m 	metadata.max.age.ms = 300000
[36mkafka1             |[0m 	metric.reporters = []
[36mkafka1             |[0m 	metrics.num.samples = 2
[36mkafka1             |[0m 	metrics.recording.level = INFO
[36mkafka1             |[0m 	metrics.sample.window.ms = 30000
[36mkafka1             |[0m 	receive.buffer.bytes = 65536
[36mkafka1             |[0m 	reconnect.backoff.max.ms = 1000
[36mkafka1             |[0m 	reconnect.backoff.ms = 50
[36mkafka1             |[0m 	request.timeout.ms = 30000
[36mkafka1             |[0m 	retries = 2147483647
[36mkafka1             |[0m 	retry.backoff.ms = 100
[36mkafka1             |[0m 	sasl.client.callback.handler.class = null
[36mkafka1             |[0m 	sasl.jaas.config = null
[36mkafka1             |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mkafka1             |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mkafka1             |[0m 	sasl.kerberos.service.name = null
[36mkafka1             |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mkafka1             |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mkafka1             |[0m 	sasl.login.callback.handler.class = null
[36mkafka1             |[0m 	sasl.login.class = null
[36mkafka1             |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mkafka1             |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mkafka1             |[0m 	sasl.login.refresh.window.factor = 0.8
[36mkafka1             |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mkafka1             |[0m 	sasl.mechanism = GSSAPI
[36mkafka1             |[0m 	security.protocol = PLAINTEXT
[36mkafka1             |[0m 	security.providers = null
[36mkafka1             |[0m 	send.buffer.bytes = 131072
[36mkafka1             |[0m 	ssl.cipher.suites = null
[36mkafka1             |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mkafka1             |[0m 	ssl.endpoint.identification.algorithm = https
[36mkafka1             |[0m 	ssl.engine.factory.class = null
[36mkafka1             |[0m 	ssl.key.password = null
[36mkafka1             |[0m 	ssl.keymanager.algorithm = SunX509
[36mkafka1             |[0m 	ssl.keystore.location = null
[36mkafka1             |[0m 	ssl.keystore.password = null
[36mkafka1             |[0m 	ssl.keystore.type = JKS
[36mkafka1             |[0m 	ssl.protocol = TLSv1.3
[36mkafka1             |[0m 	ssl.provider = null
[36mkafka1             |[0m 	ssl.secure.random.implementation = null
[36mkafka1             |[0m 	ssl.trustmanager.algorithm = PKIX
[36mkafka1             |[0m 	ssl.truststore.location = null
[36mkafka1             |[0m 	ssl.truststore.password = null
[36mkafka1             |[0m 	ssl.truststore.type = JKS
[36mkafka1             |[0m  (org.apache.kafka.clients.admin.AdminClientConfig)
[36mkafka1             |[0m [2021-01-09 19:50:55,718] WARN The configuration 'topic.replicas' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mkafka1             |[0m [2021-01-09 19:50:55,722] INFO Kafka version: 6.0.1-ce (org.apache.kafka.common.utils.AppInfoParser)
[36mkafka1             |[0m [2021-01-09 19:50:55,722] INFO Kafka commitId: f75f566c7a4b38d8 (org.apache.kafka.common.utils.AppInfoParser)
[36mkafka1             |[0m [2021-01-09 19:50:55,723] INFO Kafka startTimeMs: 1610221855722 (org.apache.kafka.common.utils.AppInfoParser)
[36mkafka1             |[0m [2021-01-09 19:50:55,795] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2, 0], zkVersion=0, replicas=[1, 2, 0], observers=[], offlineReplicas=[]) for partition _schemas-0 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 8 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:55,843] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 8 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:50:56,772] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition _schemas-0 at offset 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[36mkafka1             |[0m [2021-01-09 19:50:57,871] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition _confluent_balancer_broker_samples-23 at offset 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[36mkafka1             |[0m [2021-01-09 19:50:57,879] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition _confluent_balancer_broker_samples-29 at offset 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[36mkafka1             |[0m [2021-01-09 19:50:57,885] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition _confluent_balancer_broker_samples-17 at offset 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[36mkafka1             |[0m [2021-01-09 19:50:57,901] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition _confluent_balancer_broker_samples-11 at offset 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[36mkafka1             |[0m [2021-01-09 19:50:57,905] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition _confluent_balancer_broker_samples-5 at offset 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[36mkafka1             |[0m [2021-01-09 19:50:58,574] ERROR [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error for partition _schemas-0 at offset 0 (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
[36mkafka1             |[0m [2021-01-09 19:50:58,741] INFO HV000001: Hibernate Validator 6.0.17.Final (org.hibernate.validator.internal.util.Version)
[36mkafka1             |[0m [2021-01-09 19:51:01,182] ERROR [KafkaServer id=0] Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)
[36mkafka1             |[0m java.lang.IllegalStateException: Kafka HTTP server failed to start up.
[36mkafka1             |[0m 	at kafka.server.KafkaServer.startup(KafkaServer.scala:603)
[36mkafka1             |[0m 	at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:44)
[36mkafka1             |[0m 	at kafka.Kafka$.main(Kafka.scala:82)
[36mkafka1             |[0m 	at kafka.Kafka.main(Kafka.scala)
[36mkafka1             |[0m [2021-01-09 19:51:01,196] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)
[36mkafka1             |[0m [2021-01-09 19:51:01,401] INFO Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600, confluent.placement.constraints=} and initial partition assignment HashMap(0 -> ReplicaAssignment(replicas=2,1, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 1 -> ReplicaAssignment(replicas=0,2, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 2 -> ReplicaAssignment(replicas=1,0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 3 -> ReplicaAssignment(replicas=2,0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 4 -> ReplicaAssignment(replicas=0,1, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 5 -> ReplicaAssignment(replicas=1,2, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 6 -> ReplicaAssignment(replicas=2,1, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 7 -> ReplicaAssignment(replicas=0,2, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 8 -> ReplicaAssignment(replicas=1,0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 9 -> ReplicaAssignment(replicas=2,0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 10 -> ReplicaAssignment(replicas=0,1, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 11 -> ReplicaAssignment(replicas=1,2, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 12 -> ReplicaAssignment(replicas=2,1, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 13 -> ReplicaAssignment(replicas=0,2, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 14 -> ReplicaAssignment(replicas=1,0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 15 -> ReplicaAssignment(replicas=2,0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 16 -> ReplicaAssignment(replicas=0,1, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 17 -> ReplicaAssignment(replicas=1,2, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 18 -> ReplicaAssignment(replicas=2,1, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 19 -> ReplicaAssignment(replicas=0,2, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 20 -> ReplicaAssignment(replicas=1,0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 21 -> ReplicaAssignment(replicas=2,0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 22 -> ReplicaAssignment(replicas=0,1, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 23 -> ReplicaAssignment(replicas=1,2, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 24 -> ReplicaAssignment(replicas=2,1, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 25 -> ReplicaAssignment(replicas=0,2, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 26 -> ReplicaAssignment(replicas=1,0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 27 -> ReplicaAssignment(replicas=2,0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 28 -> ReplicaAssignment(replicas=0,1, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 29 -> ReplicaAssignment(replicas=1,2, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 30 -> ReplicaAssignment(replicas=2,1, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 31 -> ReplicaAssignment(replicas=0,2, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 32 -> ReplicaAssignment(replicas=1,0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 33 -> ReplicaAssignment(replicas=2,0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 34 -> ReplicaAssignment(replicas=0,1, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 35 -> ReplicaAssignment(replicas=1,2, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 36 -> ReplicaAssignment(replicas=2,1, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 37 -> ReplicaAssignment(replicas=0,2, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 38 -> ReplicaAssignment(replicas=1,0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 39 -> ReplicaAssignment(replicas=2,0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 40 -> ReplicaAssignment(replicas=0,1, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 41 -> ReplicaAssignment(replicas=1,2, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 42 -> ReplicaAssignment(replicas=2,1, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 43 -> ReplicaAssignment(replicas=0,2, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 44 -> ReplicaAssignment(replicas=1,0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 45 -> ReplicaAssignment(replicas=2,0, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 46 -> ReplicaAssignment(replicas=0,1, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 47 -> ReplicaAssignment(replicas=1,2, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 48 -> ReplicaAssignment(replicas=2,1, addingReplicas=, removingReplicas=, observers=, targetObservers=None), 49 -> ReplicaAssignment(replicas=0,2, addingReplicas=, removingReplicas=, observers=, targetObservers=None)) (kafka.zk.AdminZkClient)
[36mkafka1             |[0m [2021-01-09 19:51:01,483] INFO [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 2 is successful (kafka.server.KafkaApis)
[36mkafka1             |[0m [2021-01-09 19:51:03,353] INFO [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 for 33 partitions (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,370] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=15, controllerEpoch=1, leader=2, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,370] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=13, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,370] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=46, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,370] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=44, controllerEpoch=1, leader=1, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,370] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=9, controllerEpoch=1, leader=2, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,371] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=21, controllerEpoch=1, leader=2, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,371] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=19, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,371] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=32, controllerEpoch=1, leader=1, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,371] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=28, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,372] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=26, controllerEpoch=1, leader=1, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,372] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=7, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,373] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=40, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,373] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=38, controllerEpoch=1, leader=1, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,374] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=3, controllerEpoch=1, leader=2, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,374] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=1, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,374] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=34, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,375] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=16, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,375] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=45, controllerEpoch=1, leader=2, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,376] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=14, controllerEpoch=1, leader=1, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,376] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=43, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,381] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=10, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,381] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=22, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,385] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=20, controllerEpoch=1, leader=1, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,385] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=49, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,385] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=31, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,386] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=27, controllerEpoch=1, leader=2, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,386] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=25, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,387] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=39, controllerEpoch=1, leader=2, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,389] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=8, controllerEpoch=1, leader=1, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,389] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=37, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,390] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=4, controllerEpoch=1, leader=0, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,392] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=33, controllerEpoch=1, leader=2, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,392] TRACE [Broker id=0] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', clusterLinkId=null, clusterLinkTopicState=null, linkedLeaderEpoch=-1, partitionIndex=2, controllerEpoch=1, leader=1, confluentIsUncleanLeader=false, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 2 epoch 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,847] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-leader transition for partition __consumer_offsets-37 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,847] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-leader transition for partition __consumer_offsets-7 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,847] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-leader transition for partition __consumer_offsets-22 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,847] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-leader transition for partition __consumer_offsets-10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,848] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-leader transition for partition __consumer_offsets-31 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,848] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-leader transition for partition __consumer_offsets-46 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,848] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-leader transition for partition __consumer_offsets-1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,863] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-leader transition for partition __consumer_offsets-16 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,863] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-leader transition for partition __consumer_offsets-19 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,863] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-leader transition for partition __consumer_offsets-34 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,863] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-leader transition for partition __consumer_offsets-4 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,863] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-leader transition for partition __consumer_offsets-25 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,863] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-leader transition for partition __consumer_offsets-40 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,864] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-leader transition for partition __consumer_offsets-43 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,864] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-leader transition for partition __consumer_offsets-13 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,864] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-leader transition for partition __consumer_offsets-28 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,864] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-leader transition for partition __consumer_offsets-49 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,871] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-4, __consumer_offsets-25, __consumer_offsets-49, __consumer_offsets-31, __consumer_offsets-37, __consumer_offsets-19, __consumer_offsets-13, __consumer_offsets-43, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-7, __consumer_offsets-46, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-10, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[36mkafka1             |[0m [2021-01-09 19:51:03,871] INFO [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 9 from controller 2 epoch 1 as part of the become-leader transition for 17 partitions (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:03,946] INFO [Log partition=__consumer_offsets-37, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 43 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:04,059] INFO [Log partition=__consumer_offsets-37, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:04,060] INFO [MergedLog partition=__consumer_offsets-37, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:04,071] INFO Created log for partition __consumer_offsets-37 in /var/lib/kafka/data/__consumer_offsets-37 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:04,082] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:04,082] INFO [Partition __consumer_offsets-37 broker=0] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:04,082] INFO [Broker id=0] Leader __consumer_offsets-37 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:04,135] INFO [Log partition=__consumer_offsets-7, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 14 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:04,140] INFO [Log partition=__consumer_offsets-7, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:04,146] INFO [MergedLog partition=__consumer_offsets-7, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:04,154] INFO Created log for partition __consumer_offsets-7 in /var/lib/kafka/data/__consumer_offsets-7 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:04,154] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:04,154] INFO [Partition __consumer_offsets-7 broker=0] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:04,154] INFO [Broker id=0] Leader __consumer_offsets-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:04,232] INFO [Log partition=__consumer_offsets-22, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 19 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:04,246] INFO [Log partition=__consumer_offsets-22, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:04,247] INFO [MergedLog partition=__consumer_offsets-22, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:04,248] INFO Created log for partition __consumer_offsets-22 in /var/lib/kafka/data/__consumer_offsets-22 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:04,248] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:04,248] INFO [Partition __consumer_offsets-22 broker=0] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:04,248] INFO [Broker id=0] Leader __consumer_offsets-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:04,351] INFO [Log partition=__consumer_offsets-10, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 29 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:04,366] INFO [Log partition=__consumer_offsets-10, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:04,517] INFO [MergedLog partition=__consumer_offsets-10, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:04,520] INFO Created log for partition __consumer_offsets-10 in /var/lib/kafka/data/__consumer_offsets-10 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:04,520] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:04,522] INFO [Partition __consumer_offsets-10 broker=0] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:04,523] INFO [Broker id=0] Leader __consumer_offsets-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:04,632] INFO [Log partition=__consumer_offsets-31, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 13 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:04,672] INFO [Log partition=__consumer_offsets-31, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:04,701] INFO [MergedLog partition=__consumer_offsets-31, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:04,772] INFO Created log for partition __consumer_offsets-31 in /var/lib/kafka/data/__consumer_offsets-31 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:04,773] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:04,773] INFO [Partition __consumer_offsets-31 broker=0] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:04,773] INFO [Broker id=0] Leader __consumer_offsets-31 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:04,792] INFO [Log partition=__consumer_offsets-46, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:04,832] INFO [Log partition=__consumer_offsets-46, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:04,844] INFO [MergedLog partition=__consumer_offsets-46, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:04,894] INFO Created log for partition __consumer_offsets-46 in /var/lib/kafka/data/__consumer_offsets-46 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:04,907] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:04,907] INFO [Partition __consumer_offsets-46 broker=0] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:04,910] INFO [Broker id=0] Leader __consumer_offsets-46 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:04,979] INFO [Log partition=__consumer_offsets-1, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 33 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:05,204] INFO [Log partition=__consumer_offsets-1, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:05,241] INFO [MergedLog partition=__consumer_offsets-1, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:05,270] INFO Created log for partition __consumer_offsets-1 in /var/lib/kafka/data/__consumer_offsets-1 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:05,273] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:05,323] INFO [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:05,324] INFO [Broker id=0] Leader __consumer_offsets-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:05,391] INFO [Log partition=__consumer_offsets-16, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 30 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:05,423] INFO [Log partition=__consumer_offsets-16, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:05,424] INFO [MergedLog partition=__consumer_offsets-16, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:05,445] INFO Created log for partition __consumer_offsets-16 in /var/lib/kafka/data/__consumer_offsets-16 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:05,446] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:05,446] INFO [Partition __consumer_offsets-16 broker=0] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:05,447] INFO [Broker id=0] Leader __consumer_offsets-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:05,481] INFO [Log partition=__consumer_offsets-19, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 13 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:05,510] INFO [Log partition=__consumer_offsets-19, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:05,518] INFO [MergedLog partition=__consumer_offsets-19, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:05,610] INFO Created log for partition __consumer_offsets-19 in /var/lib/kafka/data/__consumer_offsets-19 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:05,627] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:05,628] INFO [Partition __consumer_offsets-19 broker=0] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:05,628] INFO [Broker id=0] Leader __consumer_offsets-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:05,637] INFO [Log partition=__consumer_offsets-34, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:05,639] INFO [Log partition=__consumer_offsets-34, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:05,639] INFO [MergedLog partition=__consumer_offsets-34, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:05,801] INFO Created log for partition __consumer_offsets-34 in /var/lib/kafka/data/__consumer_offsets-34 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:05,801] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:05,802] INFO [Partition __consumer_offsets-34 broker=0] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:05,845] INFO [Broker id=0] Leader __consumer_offsets-34 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:05,947] INFO [Log partition=__consumer_offsets-4, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 26 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:06,042] INFO [Log partition=__consumer_offsets-4, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:06,090] INFO [MergedLog partition=__consumer_offsets-4, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:06,109] INFO Created log for partition __consumer_offsets-4 in /var/lib/kafka/data/__consumer_offsets-4 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:06,109] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:06,117] INFO [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:06,193] INFO [Broker id=0] Leader __consumer_offsets-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:06,331] INFO [Log partition=__consumer_offsets-25, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 39 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:06,364] INFO [Log partition=__consumer_offsets-25, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:06,379] INFO [MergedLog partition=__consumer_offsets-25, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:06,386] INFO Created log for partition __consumer_offsets-25 in /var/lib/kafka/data/__consumer_offsets-25 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:06,394] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:06,394] INFO [Partition __consumer_offsets-25 broker=0] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:06,398] INFO [Broker id=0] Leader __consumer_offsets-25 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:06,440] INFO [Log partition=__consumer_offsets-40, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:06,466] INFO [Log partition=__consumer_offsets-40, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:06,479] INFO [MergedLog partition=__consumer_offsets-40, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:06,505] INFO Created log for partition __consumer_offsets-40 in /var/lib/kafka/data/__consumer_offsets-40 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:06,505] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:06,507] INFO [Partition __consumer_offsets-40 broker=0] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:06,528] INFO [Broker id=0] Leader __consumer_offsets-40 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:06,773] INFO [Log partition=__consumer_offsets-43, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 201 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:06,783] INFO [Log partition=__consumer_offsets-43, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:06,808] INFO [MergedLog partition=__consumer_offsets-43, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:06,838] INFO Created log for partition __consumer_offsets-43 in /var/lib/kafka/data/__consumer_offsets-43 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:06,839] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:06,842] INFO [Partition __consumer_offsets-43 broker=0] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:06,842] INFO [Broker id=0] Leader __consumer_offsets-43 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:06,855] INFO Started o.e.j.s.ServletContextHandler@194346ca{/v1/metadata,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
[36mkafka1             |[0m [2021-01-09 19:51:06,905] INFO [Log partition=__consumer_offsets-13, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 50 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:06,914] INFO [Log partition=__consumer_offsets-13, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:06,933] INFO [MergedLog partition=__consumer_offsets-13, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:06,944] INFO Created log for partition __consumer_offsets-13 in /var/lib/kafka/data/__consumer_offsets-13 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:06,962] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:06,965] INFO [Partition __consumer_offsets-13 broker=0] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:06,973] INFO [Broker id=0] Leader __consumer_offsets-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,078] INFO [Log partition=__consumer_offsets-28, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 57 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:07,112] INFO [Log partition=__consumer_offsets-28, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:07,119] INFO [MergedLog partition=__consumer_offsets-28, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:07,127] INFO Created log for partition __consumer_offsets-28 in /var/lib/kafka/data/__consumer_offsets-28 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:07,131] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:07,134] INFO [Partition __consumer_offsets-28 broker=0] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:07,138] INFO [Broker id=0] Leader __consumer_offsets-28 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,185] INFO [Log partition=__consumer_offsets-49, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 23 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:07,189] INFO [Log partition=__consumer_offsets-49, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:07,204] INFO [MergedLog partition=__consumer_offsets-49, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:07,212] INFO Created log for partition __consumer_offsets-49 in /var/lib/kafka/data/__consumer_offsets-49 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:07,224] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:07,224] INFO [Partition __consumer_offsets-49 broker=0] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:07,228] INFO [Broker id=0] Leader __consumer_offsets-49 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2 observers  addingReplicas  removingReplicas .Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,229] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-leader transition for partition __consumer_offsets-37 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,235] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-leader transition for partition __consumer_offsets-7 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,238] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-leader transition for partition __consumer_offsets-22 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,240] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-leader transition for partition __consumer_offsets-10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,241] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-leader transition for partition __consumer_offsets-31 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,242] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-leader transition for partition __consumer_offsets-46 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,248] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-leader transition for partition __consumer_offsets-1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,248] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-leader transition for partition __consumer_offsets-16 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,250] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-leader transition for partition __consumer_offsets-19 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,254] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-leader transition for partition __consumer_offsets-34 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,257] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-leader transition for partition __consumer_offsets-4 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,258] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-leader transition for partition __consumer_offsets-25 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,259] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-leader transition for partition __consumer_offsets-40 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,261] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-leader transition for partition __consumer_offsets-43 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,271] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-leader transition for partition __consumer_offsets-13 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,277] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-leader transition for partition __consumer_offsets-28 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,278] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-leader transition for partition __consumer_offsets-49 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,281] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-follower transition for partition __consumer_offsets-3 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,281] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-follower transition for partition __consumer_offsets-20 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,282] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-follower transition for partition __consumer_offsets-39 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,286] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-follower transition for partition __consumer_offsets-9 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,291] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-follower transition for partition __consumer_offsets-27 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,292] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-follower transition for partition __consumer_offsets-44 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,297] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-follower transition for partition __consumer_offsets-14 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,307] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-follower transition for partition __consumer_offsets-33 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,308] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-follower transition for partition __consumer_offsets-2 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,308] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-follower transition for partition __consumer_offsets-21 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,309] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-follower transition for partition __consumer_offsets-38 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,309] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-follower transition for partition __consumer_offsets-8 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,314] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-follower transition for partition __consumer_offsets-26 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,314] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-follower transition for partition __consumer_offsets-45 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,315] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-follower transition for partition __consumer_offsets-15 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,316] TRACE [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 2 epoch 1 starting the become-follower transition for partition __consumer_offsets-32 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,343] INFO [Log partition=__consumer_offsets-3, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:07,376] INFO [Log partition=__consumer_offsets-3, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:07,385] INFO [MergedLog partition=__consumer_offsets-3, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:07,401] INFO Created log for partition __consumer_offsets-3 in /var/lib/kafka/data/__consumer_offsets-3 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:07,416] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:07,416] INFO [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:07,416] INFO [Broker id=0] Follower __consumer_offsets-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,544] INFO [Log partition=__consumer_offsets-20, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 16 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:07,562] INFO [Log partition=__consumer_offsets-20, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:07,591] INFO [MergedLog partition=__consumer_offsets-20, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:07,669] INFO Created log for partition __consumer_offsets-20 in /var/lib/kafka/data/__consumer_offsets-20 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:07,669] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:07,670] INFO [Partition __consumer_offsets-20 broker=0] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:07,674] INFO [Broker id=0] Follower __consumer_offsets-20 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:07,840] INFO [Log partition=__consumer_offsets-39, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 1 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:07,874] INFO [Log partition=__consumer_offsets-39, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:07,883] INFO [MergedLog partition=__consumer_offsets-39, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:07,887] INFO Created log for partition __consumer_offsets-39 in /var/lib/kafka/data/__consumer_offsets-39 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:07,887] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:07,901] INFO [Partition __consumer_offsets-39 broker=0] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:07,902] INFO [Broker id=0] Follower __consumer_offsets-39 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:08,211] INFO [Log partition=__consumer_offsets-9, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 227 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:08,268] INFO [Log partition=__consumer_offsets-9, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:08,292] INFO [MergedLog partition=__consumer_offsets-9, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:08,313] INFO Created log for partition __consumer_offsets-9 in /var/lib/kafka/data/__consumer_offsets-9 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:08,313] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:08,331] INFO [Partition __consumer_offsets-9 broker=0] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:08,331] INFO [Broker id=0] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:08,429] INFO [Log partition=__consumer_offsets-27, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 57 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:08,445] INFO [Log partition=__consumer_offsets-27, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:08,469] INFO [MergedLog partition=__consumer_offsets-27, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:08,471] INFO Created log for partition __consumer_offsets-27 in /var/lib/kafka/data/__consumer_offsets-27 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:08,471] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:08,471] INFO [Partition __consumer_offsets-27 broker=0] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:08,471] INFO [Broker id=0] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:08,509] INFO [Log partition=__consumer_offsets-44, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 11 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:08,594] INFO [Log partition=__consumer_offsets-44, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:08,626] INFO [MergedLog partition=__consumer_offsets-44, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:08,634] INFO Created log for partition __consumer_offsets-44 in /var/lib/kafka/data/__consumer_offsets-44 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:08,634] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:08,635] INFO [Partition __consumer_offsets-44 broker=0] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:08,635] INFO [Broker id=0] Follower __consumer_offsets-44 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:08,726] INFO [Log partition=__consumer_offsets-14, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 22 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:08,751] INFO [Log partition=__consumer_offsets-14, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:08,776] INFO [MergedLog partition=__consumer_offsets-14, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:08,786] INFO Created log for partition __consumer_offsets-14 in /var/lib/kafka/data/__consumer_offsets-14 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:08,787] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:08,787] INFO [Partition __consumer_offsets-14 broker=0] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:08,787] INFO [Broker id=0] Follower __consumer_offsets-14 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:09,040] INFO [Log partition=__consumer_offsets-33, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 75 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:09,042] INFO [Log partition=__consumer_offsets-33, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:09,047] INFO [MergedLog partition=__consumer_offsets-33, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:09,062] INFO Created log for partition __consumer_offsets-33 in /var/lib/kafka/data/__consumer_offsets-33 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:09,069] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:09,070] INFO [Partition __consumer_offsets-33 broker=0] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:09,071] INFO [Broker id=0] Follower __consumer_offsets-33 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:09,148] INFO [Log partition=__consumer_offsets-2, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 45 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:09,160] INFO [Log partition=__consumer_offsets-2, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:09,181] INFO [MergedLog partition=__consumer_offsets-2, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:09,201] INFO Created log for partition __consumer_offsets-2 in /var/lib/kafka/data/__consumer_offsets-2 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:09,219] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:09,220] INFO [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:09,224] INFO [Broker id=0] Follower __consumer_offsets-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:09,444] INFO [Log partition=__consumer_offsets-21, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 25 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:09,452] INFO [Log partition=__consumer_offsets-21, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:09,472] INFO [MergedLog partition=__consumer_offsets-21, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:09,474] INFO Created log for partition __consumer_offsets-21 in /var/lib/kafka/data/__consumer_offsets-21 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:09,474] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:09,475] INFO [Partition __consumer_offsets-21 broker=0] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:09,475] INFO [Broker id=0] Follower __consumer_offsets-21 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:09,490] INFO [Log partition=__consumer_offsets-38, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 2 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:09,496] INFO [Log partition=__consumer_offsets-38, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:09,508] INFO [MergedLog partition=__consumer_offsets-38, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:09,510] INFO Created log for partition __consumer_offsets-38 in /var/lib/kafka/data/__consumer_offsets-38 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:09,510] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:09,511] INFO [Partition __consumer_offsets-38 broker=0] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:09,512] INFO [Broker id=0] Follower __consumer_offsets-38 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:09,528] INFO [Log partition=__consumer_offsets-8, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 3 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:09,532] INFO [Log partition=__consumer_offsets-8, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:09,533] INFO [MergedLog partition=__consumer_offsets-8, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:09,537] INFO Created log for partition __consumer_offsets-8 in /var/lib/kafka/data/__consumer_offsets-8 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:09,542] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:09,542] INFO [Partition __consumer_offsets-8 broker=0] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:09,542] INFO [Broker id=0] Follower __consumer_offsets-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:09,555] INFO [Log partition=__consumer_offsets-26, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 6 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:10,029] INFO [Log partition=__consumer_offsets-26, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:10,031] INFO [MergedLog partition=__consumer_offsets-26, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:10,034] INFO Created log for partition __consumer_offsets-26 in /var/lib/kafka/data/__consumer_offsets-26 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:10,036] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:10,039] INFO [Partition __consumer_offsets-26 broker=0] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:10,039] INFO [Broker id=0] Follower __consumer_offsets-26 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,097] INFO [Log partition=__consumer_offsets-45, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 4 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:10,113] INFO [Log partition=__consumer_offsets-45, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:10,133] INFO [MergedLog partition=__consumer_offsets-45, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:10,141] INFO Created log for partition __consumer_offsets-45 in /var/lib/kafka/data/__consumer_offsets-45 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:10,141] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:10,146] INFO [Partition __consumer_offsets-45 broker=0] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:10,147] INFO [Broker id=0] Follower __consumer_offsets-45 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,315] INFO [Log partition=__consumer_offsets-15, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 110 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:10,383] INFO [Log partition=__consumer_offsets-15, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:10,388] INFO [MergedLog partition=__consumer_offsets-15, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:10,407] INFO Created log for partition __consumer_offsets-15 in /var/lib/kafka/data/__consumer_offsets-15 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:10,408] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:10,408] INFO [Partition __consumer_offsets-15 broker=0] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:10,411] INFO [Broker id=0] Follower __consumer_offsets-15 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,497] INFO [Log partition=__consumer_offsets-32, dir=/var/lib/kafka/data] Completed load of log with 1 segments, log start offset (merged: 0, local: 0) and log end offset 0 in 19 ms (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:10,515] INFO [Log partition=__consumer_offsets-32, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mkafka1             |[0m [2021-01-09 19:51:10,525] INFO [MergedLog partition=__consumer_offsets-32, dir=/var/lib/kafka/data] Completed load of log with 1 segments containing 1 local segments and 0 tiered segments, tier start offset 0, first untiered offset 0, local start offset 0, log end offset 0 (kafka.log.MergedLog)
[36mkafka1             |[0m [2021-01-09 19:51:10,543] INFO Created log for partition __consumer_offsets-32 in /var/lib/kafka/data/__consumer_offsets-32 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, confluent.missing.id.cache.ttl.sec -> 60, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, confluent.tier.local.hotset.ms -> 86400000, confluent.tier.local.hotset.bytes -> -1, segment.bytes -> 104857600, retention.ms -> 604800000, flush.messages -> 9223372036854775807, confluent.append.record.interceptor.classes -> [], confluent.tier.enable -> false, message.format.version -> 2.6-IV0, confluent.tier.segment.hotset.roll.min.bytes -> 104857600, confluent.segment.speculative.prefetch.enable -> false, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, confluent.schema.registry.url -> http://schema-registry:8081, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, confluent.placement.constraints -> , unclean.leader.election.enable -> false, confluent.schema.registry.max.cache.size -> 10000, retention.bytes -> -1, delete.retention.ms -> 86400000, confluent.missing.id.query.range -> 200, confluent.prefer.tier.fetch.ms -> -1, confluent.schema.registry.max.retries -> 1, segment.ms -> 604800000, confluent.schema.registry.retries.wait.ms -> 0, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:10,543] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:10,543] INFO [Partition __consumer_offsets-32 broker=0] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[36mkafka1             |[0m [2021-01-09 19:51:10,544] INFO [Broker id=0] Follower __consumer_offsets-32 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1. (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,544] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-15, __consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-44, __consumer_offsets-9, __consumer_offsets-21, __consumer_offsets-20, __consumer_offsets-32, __consumer_offsets-27, __consumer_offsets-26, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-38, __consumer_offsets-3, __consumer_offsets-33, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
[36mkafka1             |[0m [2021-01-09 19:51:10,544] INFO [Broker id=0] Stopped fetchers as part of become-follower request from controller 2 epoch 1 with correlation id 9 for 16 partitions (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,556] TRACE [Broker id=0] Skipped the adding-fetcher step of the become-follower state change with correlation id 9 from controller 2 epoch 1 for partition __consumer_offsets-3 with leader 2 since it is shutting down (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,556] TRACE [Broker id=0] Skipped the adding-fetcher step of the become-follower state change with correlation id 9 from controller 2 epoch 1 for partition __consumer_offsets-20 with leader 1 since it is shutting down (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,556] TRACE [Broker id=0] Skipped the adding-fetcher step of the become-follower state change with correlation id 9 from controller 2 epoch 1 for partition __consumer_offsets-39 with leader 2 since it is shutting down (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,556] TRACE [Broker id=0] Skipped the adding-fetcher step of the become-follower state change with correlation id 9 from controller 2 epoch 1 for partition __consumer_offsets-9 with leader 2 since it is shutting down (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,592] TRACE [Broker id=0] Skipped the adding-fetcher step of the become-follower state change with correlation id 9 from controller 2 epoch 1 for partition __consumer_offsets-27 with leader 2 since it is shutting down (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,592] TRACE [Broker id=0] Skipped the adding-fetcher step of the become-follower state change with correlation id 9 from controller 2 epoch 1 for partition __consumer_offsets-44 with leader 1 since it is shutting down (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,592] TRACE [Broker id=0] Skipped the adding-fetcher step of the become-follower state change with correlation id 9 from controller 2 epoch 1 for partition __consumer_offsets-14 with leader 1 since it is shutting down (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,592] TRACE [Broker id=0] Skipped the adding-fetcher step of the become-follower state change with correlation id 9 from controller 2 epoch 1 for partition __consumer_offsets-33 with leader 2 since it is shutting down (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,593] TRACE [Broker id=0] Skipped the adding-fetcher step of the become-follower state change with correlation id 9 from controller 2 epoch 1 for partition __consumer_offsets-2 with leader 1 since it is shutting down (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,593] TRACE [Broker id=0] Skipped the adding-fetcher step of the become-follower state change with correlation id 9 from controller 2 epoch 1 for partition __consumer_offsets-21 with leader 2 since it is shutting down (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,593] TRACE [Broker id=0] Skipped the adding-fetcher step of the become-follower state change with correlation id 9 from controller 2 epoch 1 for partition __consumer_offsets-38 with leader 1 since it is shutting down (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,593] TRACE [Broker id=0] Skipped the adding-fetcher step of the become-follower state change with correlation id 9 from controller 2 epoch 1 for partition __consumer_offsets-8 with leader 1 since it is shutting down (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,593] TRACE [Broker id=0] Skipped the adding-fetcher step of the become-follower state change with correlation id 9 from controller 2 epoch 1 for partition __consumer_offsets-26 with leader 1 since it is shutting down (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,593] TRACE [Broker id=0] Skipped the adding-fetcher step of the become-follower state change with correlation id 9 from controller 2 epoch 1 for partition __consumer_offsets-45 with leader 2 since it is shutting down (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,593] TRACE [Broker id=0] Skipped the adding-fetcher step of the become-follower state change with correlation id 9 from controller 2 epoch 1 for partition __consumer_offsets-15 with leader 2 since it is shutting down (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,593] TRACE [Broker id=0] Skipped the adding-fetcher step of the become-follower state change with correlation id 9 from controller 2 epoch 1 for partition __consumer_offsets-32 with leader 1 since it is shutting down (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,593] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-follower transition for partition __consumer_offsets-3 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,594] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-follower transition for partition __consumer_offsets-20 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,594] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-follower transition for partition __consumer_offsets-39 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,597] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-follower transition for partition __consumer_offsets-9 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,597] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-follower transition for partition __consumer_offsets-27 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,597] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-follower transition for partition __consumer_offsets-44 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,598] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-follower transition for partition __consumer_offsets-14 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,598] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-follower transition for partition __consumer_offsets-33 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,599] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-follower transition for partition __consumer_offsets-2 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,676] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-follower transition for partition __consumer_offsets-21 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,707] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-follower transition for partition __consumer_offsets-38 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,716] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-follower transition for partition __consumer_offsets-8 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,718] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-follower transition for partition __consumer_offsets-26 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,719] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-follower transition for partition __consumer_offsets-45 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,719] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-follower transition for partition __consumer_offsets-15 with leader 2 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,721] TRACE [Broker id=0] Completed LeaderAndIsr request correlationId 9 from controller 2 epoch 1 for the become-follower transition for partition __consumer_offsets-32 with leader 1 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:10,853] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,080] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,111] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,111] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,111] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,178] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,178] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,178] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,178] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,178] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,179] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,179] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,179] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,179] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,179] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,179] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,179] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,203] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,377] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,386] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 499 milliseconds, of which 179 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,407] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 325 milliseconds, of which 325 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,408] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 297 milliseconds, of which 297 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,408] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,409] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,409] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,409] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,409] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,409] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,410] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,410] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,410] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,412] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,478] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,479] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,479] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,477] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 366 milliseconds, of which 366 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,488] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 377 milliseconds, of which 372 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,492] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 313 milliseconds, of which 313 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,493] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 315 milliseconds, of which 314 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,511] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,512] INFO [Broker id=0] Finished LeaderAndIsr request in 8159ms correlationId 9 from controller 2 for 33 partitions (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,526] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 347 milliseconds, of which 315 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,580] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 402 milliseconds, of which 348 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,587] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 408 milliseconds, of which 407 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,587] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 408 milliseconds, of which 408 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,591] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 412 milliseconds, of which 412 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,596] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 417 milliseconds, of which 416 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,596] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 417 milliseconds, of which 417 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,597] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 418 milliseconds, of which 417 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,600] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 421 milliseconds, of which 420 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,600] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 421 milliseconds, of which 421 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,684] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-3. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,700] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-20. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,701] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], offlineReplicas=[]) for partition __consumer_offsets-13 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,731] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], offlineReplicas=[]) for partition __consumer_offsets-46 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,732] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], offlineReplicas=[]) for partition __consumer_offsets-9 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,732] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1], zkVersion=0, replicas=[2, 1], observers=[], offlineReplicas=[]) for partition __consumer_offsets-42 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,732] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], offlineReplicas=[]) for partition __consumer_offsets-21 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,732] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2], zkVersion=0, replicas=[1, 2], observers=[], offlineReplicas=[]) for partition __consumer_offsets-17 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,733] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1], zkVersion=0, replicas=[2, 1], observers=[], offlineReplicas=[]) for partition __consumer_offsets-30 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,733] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], offlineReplicas=[]) for partition __consumer_offsets-26 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,733] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2], zkVersion=0, replicas=[1, 2], observers=[], offlineReplicas=[]) for partition __consumer_offsets-5 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,733] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], offlineReplicas=[]) for partition __consumer_offsets-38 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,744] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], offlineReplicas=[]) for partition __consumer_offsets-1 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,746] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], offlineReplicas=[]) for partition __consumer_offsets-34 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,746] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], offlineReplicas=[]) for partition __consumer_offsets-16 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,747] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], offlineReplicas=[]) for partition __consumer_offsets-45 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,747] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1], zkVersion=0, replicas=[2, 1], observers=[], offlineReplicas=[]) for partition __consumer_offsets-12 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,747] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2], zkVersion=0, replicas=[1, 2], observers=[], offlineReplicas=[]) for partition __consumer_offsets-41 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,747] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1], zkVersion=0, replicas=[2, 1], observers=[], offlineReplicas=[]) for partition __consumer_offsets-24 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,751] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], offlineReplicas=[]) for partition __consumer_offsets-20 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,754] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], offlineReplicas=[]) for partition __consumer_offsets-49 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,755] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1], zkVersion=0, replicas=[2, 1], observers=[], offlineReplicas=[]) for partition __consumer_offsets-0 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,755] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2], zkVersion=0, replicas=[1, 2], observers=[], offlineReplicas=[]) for partition __consumer_offsets-29 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,755] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], offlineReplicas=[]) for partition __consumer_offsets-25 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,756] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], offlineReplicas=[]) for partition __consumer_offsets-8 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,759] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], offlineReplicas=[]) for partition __consumer_offsets-37 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,759] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], offlineReplicas=[]) for partition __consumer_offsets-4 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,759] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], offlineReplicas=[]) for partition __consumer_offsets-33 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,759] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], offlineReplicas=[]) for partition __consumer_offsets-15 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,759] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1], zkVersion=0, replicas=[2, 1], observers=[], offlineReplicas=[]) for partition __consumer_offsets-48 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,759] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2], zkVersion=0, replicas=[1, 2], observers=[], offlineReplicas=[]) for partition __consumer_offsets-11 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,760] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], offlineReplicas=[]) for partition __consumer_offsets-44 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,760] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2], zkVersion=0, replicas=[1, 2], observers=[], offlineReplicas=[]) for partition __consumer_offsets-23 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,760] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], offlineReplicas=[]) for partition __consumer_offsets-19 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,760] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], offlineReplicas=[]) for partition __consumer_offsets-32 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,784] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], offlineReplicas=[]) for partition __consumer_offsets-28 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,812] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], offlineReplicas=[]) for partition __consumer_offsets-7 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,827] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], offlineReplicas=[]) for partition __consumer_offsets-40 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,831] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], offlineReplicas=[]) for partition __consumer_offsets-3 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,831] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1], zkVersion=0, replicas=[2, 1], observers=[], offlineReplicas=[]) for partition __consumer_offsets-36 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,831] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2], zkVersion=0, replicas=[1, 2], observers=[], offlineReplicas=[]) for partition __consumer_offsets-47 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,831] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], offlineReplicas=[]) for partition __consumer_offsets-14 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,831] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], offlineReplicas=[]) for partition __consumer_offsets-43 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,872] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], offlineReplicas=[]) for partition __consumer_offsets-10 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,872] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 1], zkVersion=0, replicas=[0, 1], observers=[], offlineReplicas=[]) for partition __consumer_offsets-22 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,877] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1], zkVersion=0, replicas=[2, 1], observers=[], offlineReplicas=[]) for partition __consumer_offsets-18 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,877] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=1, leader=0, leaderEpoch=0, isr=[0, 2], zkVersion=0, replicas=[0, 2], observers=[], offlineReplicas=[]) for partition __consumer_offsets-31 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,877] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], offlineReplicas=[]) for partition __consumer_offsets-27 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,877] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 0], zkVersion=0, replicas=[2, 0], observers=[], offlineReplicas=[]) for partition __consumer_offsets-39 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,877] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2, 1], zkVersion=0, replicas=[2, 1], observers=[], offlineReplicas=[]) for partition __consumer_offsets-6 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,877] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 2], zkVersion=0, replicas=[1, 2], observers=[], offlineReplicas=[]) for partition __consumer_offsets-35 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,877] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1, 0], zkVersion=0, replicas=[1, 0], observers=[], offlineReplicas=[]) for partition __consumer_offsets-2 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,878] INFO [Broker id=0] Add 50 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 10 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:11,887] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-39. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,931] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-9. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,931] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-27. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,931] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-44. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,932] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-14. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,932] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-33. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,933] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-2. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,933] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-21. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,933] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-38. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,940] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-8. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,941] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-26. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,941] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-45. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,942] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-15. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:11,942] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-32. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
[36mkafka1             |[0m [2021-01-09 19:51:16,988] INFO Started o.e.j.s.ServletContextHandler@1071e4ac{/kafka,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
[36mkafka1             |[0m [2021-01-09 19:51:18,243] TRACE [Broker id=0] Cached leader info UpdateMetadataPartitionState(topicName='_confluent-ksql-default__command_topic', partitionIndex=0, controllerEpoch=1, leader=2, leaderEpoch=0, isr=[2], zkVersion=0, replicas=[2], observers=[], offlineReplicas=[]) for partition _confluent-ksql-default__command_topic-0 in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 11 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:18,244] INFO [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 2 epoch 1 with correlation id 11 (state.change.logger)
[36mkafka1             |[0m [2021-01-09 19:51:18,492] INFO Started o.e.j.s.ServletContextHandler@1495a49b{/ws,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
[36mkafka1             |[0m [2021-01-09 19:51:18,712] INFO Started o.e.j.s.ServletContextHandler@104e16d6{/ws,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
[36mkafka1             |[0m [2021-01-09 19:51:19,071] INFO Started NetworkTrafficServerConnector@4d7a64ca{HTTP/1.1,[http/1.1]}{0.0.0.0:8090} (org.eclipse.jetty.server.AbstractConnector)
[36mkafka1             |[0m [2021-01-09 19:51:19,077] INFO Started @75258ms (org.eclipse.jetty.server.Server)
[36mkafka1             |[0m [2021-01-09 19:51:19,082] INFO KafkaHttpServer transitioned from STARTING to RUNNING.. (io.confluent.http.server.KafkaHttpServerImpl)
[36mkafka1             |[0m [2021-01-09 19:51:19,083] INFO KafkaHttpServer transitioned from RUNNING to STOPPING.. (io.confluent.http.server.KafkaHttpServerImpl)
[36mkafka1             |[0m [2021-01-09 19:51:19,155] INFO Stopped NetworkTrafficServerConnector@4d7a64ca{HTTP/1.1,[http/1.1]}{0.0.0.0:8090} (org.eclipse.jetty.server.AbstractConnector)
[36mkafka1             |[0m [2021-01-09 19:51:19,156] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session)
[36mkafka1             |[0m [2021-01-09 19:51:19,193] INFO Stopped o.e.j.s.ServletContextHandler@104e16d6{/ws,null,UNAVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
[36mkafka1             |[0m [2021-01-09 19:51:19,194] INFO Stopped o.e.j.s.ServletContextHandler@1495a49b{/ws,null,UNAVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
[36mkafka1             |[0m [2021-01-09 19:51:19,412] INFO Stopped o.e.j.s.ServletContextHandler@1071e4ac{/kafka,null,UNAVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
[36mkafka1             |[0m [2021-01-09 19:51:19,459] INFO Stopped o.e.j.s.ServletContextHandler@194346ca{/v1/metadata,null,UNAVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
[36mkafka1             |[0m [2021-01-09 19:51:19,467] INFO Stopping TelemetryReporter collectorTask (io.confluent.telemetry.reporter.TelemetryReporter)
[36mkafka1             |[0m [2021-01-09 19:51:19,518] INFO KafkaHttpServer transitioned from STOPPING to TERMINATED.. (io.confluent.http.server.KafkaHttpServerImpl)
[36mkafka1             |[0m [2021-01-09 19:51:19,652] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[36mkafka1             |[0m [2021-01-09 19:51:19,677] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[36mkafka1             |[0m [2021-01-09 19:51:19,668] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[36mkafka1             |[0m [2021-01-09 19:51:19,696] INFO [SocketServer brokerId=0] Stopping socket server request processors (kafka.network.SocketServer)
[36mkafka1             |[0m [2021-01-09 19:51:21,252] INFO [SocketServer brokerId=0] Stopped socket server request processors (kafka.network.SocketServer)
[36mkafka1             |[0m [2021-01-09 19:51:21,353] INFO [data-plane Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool)
[36mkafka1             |[0m [2021-01-09 19:51:21,528] INFO [data-plane Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool)
[36mkafka1             |[0m [2021-01-09 19:51:21,624] INFO [ExpirationReaper-0-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:21,816] INFO [ExpirationReaper-0-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:21,819] INFO [ExpirationReaper-0-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:21,836] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis)
[36mkafka1             |[0m [2021-01-09 19:51:21,856] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:22,008] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:22,010] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:22,024] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[36mkafka1             |[0m [2021-01-09 19:51:22,026] INFO [ProducerId Manager 0]: Shutdown complete: last producerId assigned 2000 (kafka.coordinator.transaction.ProducerIdManager)
[36mkafka1             |[0m [2021-01-09 19:51:22,028] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[36mkafka1             |[0m [2021-01-09 19:51:22,096] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[36mkafka1             |[0m [2021-01-09 19:51:22,102] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[36mkafka1             |[0m [2021-01-09 19:51:22,124] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[36mkafka1             |[0m [2021-01-09 19:51:22,159] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[36mkafka1             |[0m [2021-01-09 19:51:22,195] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[36mkafka1             |[0m [2021-01-09 19:51:22,202] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:22,352] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:22,352] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:22,353] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:22,480] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:22,480] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:22,486] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[36mkafka1             |[0m [2021-01-09 19:51:22,497] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager)
[36mkafka1             |[0m [2021-01-09 19:51:22,504] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[36mkafka1             |[0m [2021-01-09 19:51:22,504] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[36mkafka1             |[0m [2021-01-09 19:51:22,504] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[36mkafka1             |[0m [2021-01-09 19:51:22,510] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager)
[36mkafka1             |[0m [2021-01-09 19:51:22,513] INFO [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Shutting down (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:51:22,513] INFO [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Shutting down (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:51:22,514] INFO [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error sending fetch request (sessionId=1757510976, epoch=58) to node 2: (org.apache.kafka.clients.FetchSessionHandler)
[36mkafka1             |[0m java.io.IOException: Client was shutdown before response was read
[36mkafka1             |[0m 	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:109)
[36mkafka1             |[0m 	at kafka.server.ReplicaFetcherBlockingSend.sendRequest(ReplicaFetcherBlockingSend.scala:121)
[36mkafka1             |[0m 	at kafka.server.ReplicaFetcherThread.fetchFromLeader(ReplicaFetcherThread.scala:248)
[36mkafka1             |[0m 	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:469)
[36mkafka1             |[0m 	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:212)
[36mkafka1             |[0m 	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:211)
[36mkafka1             |[0m 	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:193)
[36mkafka1             |[0m 	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:99)
[36mkafka1             |[0m [2021-01-09 19:51:22,516] INFO [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error sending fetch request (sessionId=1867423055, epoch=58) to node 1: (org.apache.kafka.clients.FetchSessionHandler)
[36mkafka1             |[0m java.io.IOException: Client was shutdown before response was read
[36mkafka1             |[0m 	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:109)
[36mkafka1             |[0m 	at kafka.server.ReplicaFetcherBlockingSend.sendRequest(ReplicaFetcherBlockingSend.scala:121)
[36mkafka1             |[0m 	at kafka.server.ReplicaFetcherThread.fetchFromLeader(ReplicaFetcherThread.scala:248)
[36mkafka1             |[0m 	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:469)
[36mkafka1             |[0m 	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:212)
[36mkafka1             |[0m 	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:211)
[36mkafka1             |[0m 	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:193)
[36mkafka1             |[0m 	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:99)
[36mkafka1             |[0m [2021-01-09 19:51:22,526] INFO [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Stopped (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:51:22,527] INFO [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Stopped (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:51:22,529] INFO [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Shutdown completed (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:51:22,567] INFO [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Shutdown completed (kafka.server.ReplicaFetcherThread)
[36mkafka1             |[0m [2021-01-09 19:51:22,627] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager)
[36mkafka1             |[0m [2021-01-09 19:51:22,629] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[36mkafka1             |[0m [2021-01-09 19:51:22,646] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[36mkafka1             |[0m [2021-01-09 19:51:22,647] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:22,733] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:22,733] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:22,734] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:22,777] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:22,806] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:22,863] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:22,933] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:22,940] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:22,980] INFO [ExpirationReaper-0-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:23,185] INFO [ExpirationReaper-0-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:23,185] INFO [ExpirationReaper-0-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:23,192] INFO [ExpirationReaper-0-ListOffsets]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:23,360] INFO [ExpirationReaper-0-ListOffsets]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:23,360] INFO [ExpirationReaper-0-ListOffsets]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mkafka1             |[0m [2021-01-09 19:51:23,368] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,371] INFO Shutting down. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,387] INFO Shutting down the log cleaner. (kafka.log.LogCleaner)
[36mkafka1             |[0m [2021-01-09 19:51:23,393] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner)
[36mkafka1             |[0m [2021-01-09 19:51:23,395] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner)
[36mkafka1             |[0m [2021-01-09 19:51:23,399] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner)
[36mkafka1             |[0m [2021-01-09 19:51:23,506] INFO Tier partition state for _confluent_balancer_partition_samples-25 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,587] INFO Closing log for _confluent_balancer_partition_samples took 94 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,596] INFO Tier partition state for __consumer_offsets-43 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,601] INFO Closing log for __consumer_offsets took 13 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,611] INFO Tier partition state for _confluent_balancer_broker_samples-3 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,617] INFO Closing log for _confluent_balancer_broker_samples took 15 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,618] INFO Tier partition state for __consumer_offsets-32 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,621] INFO Closing log for __consumer_offsets took 4 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,631] INFO Tier partition state for _confluent_balancer_partition_samples-19 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,635] INFO Closing log for _confluent_balancer_partition_samples took 13 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,644] INFO Tier partition state for __consumer_offsets-25 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,646] INFO Closing log for __consumer_offsets took 11 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,647] INFO Tier partition state for _confluent_balancer_broker_samples-7 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,653] INFO Closing log for _confluent_balancer_broker_samples took 6 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,653] INFO Tier partition state for _confluent_balancer_broker_samples-17 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,656] INFO Closing log for _confluent_balancer_broker_samples took 4 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,664] INFO Tier partition state for __consumer_offsets-49 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,671] INFO Closing log for __consumer_offsets took 14 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,688] INFO Tier partition state for _confluent_balancer_partition_samples-10 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,692] INFO Closing log for _confluent_balancer_partition_samples took 21 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,699] INFO Tier partition state for _confluent_balancer_partition_samples-28 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,705] INFO Closing log for _confluent_balancer_partition_samples took 12 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,714] INFO Tier partition state for __consumer_offsets-40 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,724] INFO Closing log for __consumer_offsets took 19 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,728] INFO Tier partition state for __consumer_offsets-27 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,734] INFO Closing log for __consumer_offsets took 10 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,743] INFO Tier partition state for _confluent_balancer_partition_samples-1 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,744] INFO Closing log for _confluent_balancer_partition_samples took 10 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,752] INFO Tier partition state for _confluent_balancer_broker_samples-27 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,754] INFO Closing log for _confluent_balancer_broker_samples took 9 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,773] INFO Tier partition state for _confluent-metrics-6 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,774] INFO [ProducerStateManager partition=_confluent-metrics-6] Writing producer snapshot at offset 2 (kafka.log.ProducerStateManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,784] INFO Closing log for _confluent-metrics took 31 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,790] INFO Tier partition state for _confluent_balancer_broker_samples-12 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,792] INFO Closing log for _confluent_balancer_broker_samples took 7 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,806] INFO Tier partition state for _confluent-metrics-11 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,806] INFO [ProducerStateManager partition=_confluent-metrics-11] Writing producer snapshot at offset 4 (kafka.log.ProducerStateManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,809] INFO Closing log for _confluent-metrics took 16 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,814] INFO Tier partition state for _confluent_balancer_broker_samples-24 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,816] INFO Closing log for _confluent_balancer_broker_samples took 7 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,816] INFO Tier partition state for __consumer_offsets-45 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,822] INFO Closing log for __consumer_offsets took 6 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,822] INFO Tier partition state for _confluent_balancer_broker_samples-25 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,825] INFO Closing log for _confluent_balancer_broker_samples took 4 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,827] INFO Tier partition state for _confluent_balancer_partition_samples-20 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,830] INFO Closing log for _confluent_balancer_partition_samples took 5 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,830] INFO Tier partition state for _confluent_balancer_broker_samples-29 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,834] INFO Closing log for _confluent_balancer_broker_samples took 4 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,835] INFO Tier partition state for _confluent_balancer_broker_samples-31 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,836] INFO Closing log for _confluent_balancer_broker_samples took 1 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,849] INFO Tier partition state for _confluent_balancer_partition_samples-16 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,850] INFO Closing log for _confluent_balancer_partition_samples took 14 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,864] INFO Tier partition state for _confluent-metrics-10 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,865] INFO [ProducerStateManager partition=_confluent-metrics-10] Writing producer snapshot at offset 5 (kafka.log.ProducerStateManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,869] INFO Closing log for _confluent-metrics took 18 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,872] INFO Tier partition state for _confluent_balancer_broker_samples-19 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,878] INFO Closing log for _confluent_balancer_broker_samples took 7 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,879] INFO Tier partition state for __consumer_offsets-20 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,882] INFO Closing log for __consumer_offsets took 4 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,897] INFO Tier partition state for _confluent-metrics-5 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,898] INFO [ProducerStateManager partition=_confluent-metrics-5] Writing producer snapshot at offset 2 (kafka.log.ProducerStateManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,905] INFO Closing log for _confluent-metrics took 22 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,914] INFO Tier partition state for _confluent_balancer_partition_samples-7 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,921] INFO Closing log for _confluent_balancer_partition_samples took 16 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,933] INFO Tier partition state for _confluent_balancer_broker_samples-0 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,934] INFO Closing log for _confluent_balancer_broker_samples took 13 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,948] INFO Tier partition state for _confluent-metrics-1 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,949] INFO [ProducerStateManager partition=_confluent-metrics-1] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,954] INFO Closing log for _confluent-metrics took 20 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,979] INFO Tier partition state for __consumer_offsets-10 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,983] INFO Closing log for __consumer_offsets took 28 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,992] INFO Tier partition state for _confluent_balancer_broker_samples-30 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:23,996] INFO Closing log for _confluent_balancer_broker_samples took 13 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:23,997] INFO Tier partition state for _confluent_balancer_partition_samples-2 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,004] INFO Closing log for _confluent_balancer_partition_samples took 8 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,024] INFO Tier partition state for _confluent-metrics-0 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,025] INFO [ProducerStateManager partition=_confluent-metrics-0] Writing producer snapshot at offset 6 (kafka.log.ProducerStateManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,028] INFO Closing log for _confluent-metrics took 24 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,035] INFO Tier partition state for __consumer_offsets-28 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,038] INFO Closing log for __consumer_offsets took 8 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,043] INFO Tier partition state for __consumer_offsets-4 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,047] INFO Closing log for __consumer_offsets took 10 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,055] INFO Tier partition state for _confluent_balancer_partition_samples-22 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,057] INFO Closing log for _confluent_balancer_partition_samples took 9 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,065] INFO Tier partition state for __consumer_offsets-1 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,066] INFO Closing log for __consumer_offsets took 9 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,067] INFO Tier partition state for _confluent_balancer_partition_samples-14 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,071] INFO Closing log for _confluent_balancer_partition_samples took 4 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,071] INFO Tier partition state for __consumer_offsets-38 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,076] INFO Closing log for __consumer_offsets took 5 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,078] INFO Tier partition state for __consumer_offsets-14 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,130] INFO Closing log for __consumer_offsets took 35 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,138] INFO Tier partition state for _confluent_balancer_broker_samples-6 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,143] INFO Closing log for _confluent_balancer_broker_samples took 12 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,153] INFO Tier partition state for _confluent_balancer_partition_samples-13 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,154] INFO Closing log for _confluent_balancer_partition_samples took 11 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,155] INFO Tier partition state for _confluent_balancer_broker_samples-13 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,161] INFO Closing log for _confluent_balancer_broker_samples took 5 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,161] INFO Tier partition state for _confluent_balancer_partition_samples-26 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,163] INFO Closing log for _confluent_balancer_partition_samples took 1 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,169] INFO Tier partition state for __consumer_offsets-13 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,170] INFO Closing log for __consumer_offsets took 7 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,171] INFO Tier partition state for __consumer_offsets-26 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,175] INFO Closing log for __consumer_offsets took 4 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,175] INFO Tier partition state for __consumer_offsets-21 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,180] INFO Closing log for __consumer_offsets took 5 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,193] INFO Tier partition state for __consumer_offsets-19 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,202] INFO Closing log for __consumer_offsets took 21 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,204] INFO Tier partition state for __consumer_offsets-33 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,221] INFO Closing log for __consumer_offsets took 14 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,254] INFO Tier partition state for __consumer_offsets-37 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,261] INFO Closing log for __consumer_offsets took 39 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,261] INFO Tier partition state for __consumer_offsets-8 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,266] INFO Closing log for __consumer_offsets took 5 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,294] INFO Tier partition state for _confluent_balancer_broker_samples-9 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,309] INFO Closing log for _confluent_balancer_broker_samples took 41 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,359] INFO Tier partition state for __consumer_offsets-3 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,366] INFO Closing log for __consumer_offsets took 56 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,373] INFO Tier partition state for __consumer_offsets-39 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,422] INFO Closing log for __consumer_offsets took 32 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,424] INFO Tier partition state for __consumer_offsets-2 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,446] INFO Closing log for __consumer_offsets took 16 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,468] INFO Tier partition state for _confluent_balancer_broker_samples-15 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,480] INFO Closing log for _confluent_balancer_broker_samples took 30 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,492] INFO Tier partition state for _confluent_balancer_broker_samples-5 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,501] INFO Closing log for _confluent_balancer_broker_samples took 21 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,502] INFO Tier partition state for _confluent_balancer_broker_samples-23 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,505] INFO Closing log for _confluent_balancer_broker_samples took 4 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,505] INFO Tier partition state for _confluent_balancer_broker_samples-1 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,510] INFO Closing log for _confluent_balancer_broker_samples took 4 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,511] INFO Tier partition state for __consumer_offsets-44 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,515] INFO Closing log for __consumer_offsets took 6 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,516] INFO Tier partition state for _confluent_balancer_partition_samples-8 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,532] INFO Closing log for _confluent_balancer_partition_samples took 16 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,579] INFO Tier partition state for _confluent-metrics-4 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,579] INFO [ProducerStateManager partition=_confluent-metrics-4] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,595] INFO Closing log for _confluent-metrics took 62 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,610] INFO Tier partition state for __consumer_offsets-16 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,612] INFO Closing log for __consumer_offsets took 18 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,614] INFO Tier partition state for _confluent_balancer_partition_samples-9 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,617] INFO Closing log for _confluent_balancer_partition_samples took 4 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,638] INFO Tier partition state for _schemas-0 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,638] INFO [ProducerStateManager partition=_schemas-0] Writing producer snapshot at offset 1 (kafka.log.ProducerStateManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,641] INFO Closing log for _schemas took 23 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,658] INFO Tier partition state for _confluent-metrics-7 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,658] INFO [ProducerStateManager partition=_confluent-metrics-7] Writing producer snapshot at offset 2 (kafka.log.ProducerStateManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,665] INFO Closing log for _confluent-metrics took 24 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,666] INFO Tier partition state for _confluent_balancer_partition_samples-27 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,675] INFO Closing log for _confluent_balancer_partition_samples took 9 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,676] INFO Tier partition state for _confluent_balancer_partition_samples-3 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,682] INFO Closing log for _confluent_balancer_partition_samples took 7 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,683] INFO Tier partition state for _confluent_balancer_partition_samples-21 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,687] INFO Closing log for _confluent_balancer_partition_samples took 4 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,688] INFO Tier partition state for _confluent_balancer_partition_samples-15 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,690] INFO Closing log for _confluent_balancer_partition_samples took 2 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,700] INFO Tier partition state for __consumer_offsets-7 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,702] INFO Closing log for __consumer_offsets took 12 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,715] INFO Tier partition state for _confluent_balancer_broker_samples-18 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,723] INFO Closing log for _confluent_balancer_broker_samples took 20 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,733] INFO Tier partition state for _confluent_balancer_partition_samples-4 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,739] INFO Closing log for _confluent_balancer_partition_samples took 16 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,748] INFO Tier partition state for __consumer_offsets-22 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,753] INFO Closing log for __consumer_offsets took 14 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,774] INFO Tier partition state for _confluent_balancer_partition_samples-31 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,775] INFO Closing log for _confluent_balancer_partition_samples took 23 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,800] INFO Tier partition state for _confluent_balancer_broker_samples-21 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,814] INFO Closing log for _confluent_balancer_broker_samples took 38 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,826] INFO Tier partition state for __consumer_offsets-46 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,828] INFO Closing log for __consumer_offsets took 15 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,829] INFO Tier partition state for _confluent_balancer_broker_samples-11 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,832] INFO Closing log for _confluent_balancer_broker_samples took 3 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,844] INFO Tier partition state for __consumer_offsets-31 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,845] INFO Closing log for __consumer_offsets took 13 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,845] INFO Tier partition state for __consumer_offsets-15 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,849] INFO Closing log for __consumer_offsets took 4 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,859] INFO Tier partition state for __consumer_offsets-34 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,863] INFO Closing log for __consumer_offsets took 11 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,864] INFO Tier partition state for __consumer_offsets-9 closed. (kafka.tier.state.FileTierPartitionState)
[36mkafka1             |[0m [2021-01-09 19:51:24,868] INFO Closing log for __consumer_offsets took 2 ms. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,915] INFO Shutdown complete. (kafka.log.LogManager)
[36mkafka1             |[0m [2021-01-09 19:51:24,927] INFO [ControllerEventThread controllerId=0] Shutting down (kafka.controller.ControllerEventManager$ControllerEventThread)
[36mkafka1             |[0m [2021-01-09 19:51:24,929] INFO [ControllerEventThread controllerId=0] Shutdown completed (kafka.controller.ControllerEventManager$ControllerEventThread)
[36mkafka1             |[0m [2021-01-09 19:51:24,929] INFO [ControllerEventThread controllerId=0] Stopped (kafka.controller.ControllerEventManager$ControllerEventThread)
[36mkafka1             |[0m [2021-01-09 19:51:24,947] DEBUG [Controller id=0] Resigning (kafka.controller.KafkaController)
[36mkafka1             |[0m [2021-01-09 19:51:24,950] DEBUG [Controller id=0] Unregister BrokerModifications handler for Set() (kafka.controller.KafkaController)
[36mkafka1             |[0m [2021-01-09 19:51:25,020] INFO [PartitionStateMachine controllerId=0] Stopped partition state machine (kafka.controller.ZkPartitionStateMachine)
[36mkafka1             |[0m [2021-01-09 19:51:25,023] INFO [ReplicaStateMachine controllerId=0] Stopped replica state machine (kafka.controller.ZkReplicaStateMachine)
[36mkafka1             |[0m [2021-01-09 19:51:25,033] INFO [Controller id=0] Resigned (kafka.controller.KafkaController)
[36mkafka1             |[0m [2021-01-09 19:51:25,067] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient)
[36mkafka1             |[0m [2021-01-09 19:51:25,227] INFO EventThread shut down for session: 0x10011273bc80005 (org.apache.zookeeper.ClientCnxn)
[36mkafka1             |[0m [2021-01-09 19:51:25,234] INFO Session: 0x10011273bc80005 closed (org.apache.zookeeper.ZooKeeper)
[36mkafka1             |[0m [2021-01-09 19:51:25,313] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient)
[36mkafka1             |[0m [2021-01-09 19:51:25,314] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[36mkafka1             |[0m [2021-01-09 19:51:25,426] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[36mkafka1             |[0m [2021-01-09 19:51:25,426] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[36mkafka1             |[0m [2021-01-09 19:51:25,426] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[36mkafka1             |[0m [2021-01-09 19:51:26,106] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[36mkafka1             |[0m [2021-01-09 19:51:26,111] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[36mkafka1             |[0m [2021-01-09 19:51:26,111] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[36mkafka1             |[0m [2021-01-09 19:51:26,352] WARN [Producer clientId=confluent-metrics-reporter] Connection to node 1 (kafka2/192.168.176.4:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
[36mkafka1             |[0m [2021-01-09 19:51:26,358] WARN [Producer clientId=confluent-metrics-reporter] Connection to node 0 (kafka1/192.168.176.3:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
[36mkafka1             |[0m [2021-01-09 19:51:26,460] WARN [Producer clientId=confluent-metrics-reporter] Connection to node 1 (kafka2/192.168.176.4:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
[36mkafka1             |[0m [2021-01-09 19:51:26,479] WARN [Producer clientId=confluent-metrics-reporter] Connection to node 0 (kafka1/192.168.176.3:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
[36mkafka1             |[0m [2021-01-09 19:51:26,507] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[36mkafka1             |[0m [2021-01-09 19:51:26,507] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[36mkafka1             |[0m [2021-01-09 19:51:26,509] INFO [SocketServer brokerId=0] Shutting down socket server (kafka.network.SocketServer)
[36mkafka1             |[0m [2021-01-09 19:51:26,643] WARN [Producer clientId=confluent-metrics-reporter] Connection to node 1 (kafka2/192.168.176.4:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
[36mkafka1             |[0m [2021-01-09 19:51:26,689] WARN [Producer clientId=confluent-metrics-reporter] Connection to node 0 (kafka1/192.168.176.3:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
[36mkafka1             |[0m [2021-01-09 19:51:26,930] INFO [SocketServer brokerId=0] Shutdown completed (kafka.network.SocketServer)
[36mkafka1             |[0m [2021-01-09 19:51:26,963] INFO Stopping Confluent metrics reporter (io.confluent.metrics.reporter.ConfluentMetricsReporter)
[36mkafka1             |[0m [2021-01-09 19:51:26,969] INFO [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[36mkafka1             |[0m [2021-01-09 19:51:26,987] INFO [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[36mkafka1             |[0m [2021-01-09 19:51:26,990] WARN [Producer clientId=confluent-metrics-reporter] Connection to node 1 (kafka2/192.168.176.4:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
[36mkafka1             |[0m [2021-01-09 19:51:27,014] WARN Failed to produce metrics message (io.confluent.metrics.reporter.ConfluentMetricsReporter)
[36mkafka1             |[0m org.apache.kafka.common.KafkaException: Producer is closed forcefully.
[36mkafka1             |[0m 	at org.apache.kafka.clients.producer.internals.RecordAccumulator.abortBatches(RecordAccumulator.java:748)
[36mkafka1             |[0m 	at org.apache.kafka.clients.producer.internals.RecordAccumulator.abortIncompleteBatches(RecordAccumulator.java:735)
[36mkafka1             |[0m 	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:280)
[36mkafka1             |[0m 	at java.base/java.lang.Thread.run(Thread.java:834)
[36mkafka1             |[0m [2021-01-09 19:51:27,030] WARN Failed to produce metrics message (io.confluent.metrics.reporter.ConfluentMetricsReporter)
[36mkafka1             |[0m org.apache.kafka.common.KafkaException: Producer is closed forcefully.
[36mkafka1             |[0m 	at org.apache.kafka.clients.producer.internals.RecordAccumulator.abortBatches(RecordAccumulator.java:748)
[36mkafka1             |[0m 	at org.apache.kafka.clients.producer.internals.RecordAccumulator.abortIncompleteBatches(RecordAccumulator.java:735)
[36mkafka1             |[0m 	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:280)
[36mkafka1             |[0m 	at java.base/java.lang.Thread.run(Thread.java:834)
[36mkafka1             |[0m [2021-01-09 19:51:27,034] WARN Failed to produce metrics message (io.confluent.metrics.reporter.ConfluentMetricsReporter)
[36mkafka1             |[0m org.apache.kafka.common.KafkaException: Producer is closed forcefully.
[36mkafka1             |[0m 	at org.apache.kafka.clients.producer.internals.RecordAccumulator.abortBatches(RecordAccumulator.java:748)
[36mkafka1             |[0m 	at org.apache.kafka.clients.producer.internals.RecordAccumulator.abortIncompleteBatches(RecordAccumulator.java:735)
[36mkafka1             |[0m 	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:280)
[36mkafka1             |[0m 	at java.base/java.lang.Thread.run(Thread.java:834)
[36mkafka1             |[0m [2021-01-09 19:51:27,120] WARN Failed to produce metrics message (io.confluent.metrics.reporter.ConfluentMetricsReporter)
[36mkafka1             |[0m org.apache.kafka.common.KafkaException: Producer is closed forcefully.
[36mkafka1             |[0m 	at org.apache.kafka.clients.producer.internals.RecordAccumulator.abortBatches(RecordAccumulator.java:748)
[36mkafka1             |[0m 	at org.apache.kafka.clients.producer.internals.RecordAccumulator.abortIncompleteBatches(RecordAccumulator.java:735)
[36mkafka1             |[0m 	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:280)
[36mkafka1             |[0m 	at java.base/java.lang.Thread.run(Thread.java:834)
[36mkafka1             |[0m [2021-01-09 19:51:27,120] WARN Failed to produce metrics message (io.confluent.metrics.reporter.ConfluentMetricsReporter)
[36mkafka1             |[0m org.apache.kafka.common.KafkaException: Producer is closed forcefully.
[36mkafka1             |[0m 	at org.apache.kafka.clients.producer.internals.RecordAccumulator.abortBatches(RecordAccumulator.java:748)
[36mkafka1             |[0m 	at org.apache.kafka.clients.producer.internals.RecordAccumulator.abortIncompleteBatches(RecordAccumulator.java:735)
[36mkafka1             |[0m 	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:280)
[36mkafka1             |[0m 	at java.base/java.lang.Thread.run(Thread.java:834)
[36mkafka1             |[0m [2021-01-09 19:51:27,128] WARN Failed to produce metrics message (io.confluent.metrics.reporter.ConfluentMetricsReporter)
[36mkafka1             |[0m org.apache.kafka.common.KafkaException: Producer is closed forcefully.
[36mkafka1             |[0m 	at org.apache.kafka.clients.producer.internals.RecordAccumulator.abortBatches(RecordAccumulator.java:748)
[36mkafka1             |[0m 	at org.apache.kafka.clients.producer.internals.RecordAccumulator.abortIncompleteBatches(RecordAccumulator.java:735)
[36mkafka1             |[0m 	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:280)
[36mkafka1             |[0m 	at java.base/java.lang.Thread.run(Thread.java:834)
[36mkafka1             |[0m [2021-01-09 19:51:27,162] INFO Stopping TelemetryReporter collectorTask (io.confluent.telemetry.reporter.TelemetryReporter)
[36mkafka1             |[0m [2021-01-09 19:51:27,162] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Closing the Kafka producer with timeoutMillis = 0 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[36mkafka1             |[0m [2021-01-09 19:51:27,163] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[36mkafka1             |[0m [2021-01-09 19:51:27,199] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer)
[36mkafka1             |[0m [2021-01-09 19:51:27,202] ERROR Exiting Kafka. (kafka.server.KafkaServerStartable)
[36mkafka1             |[0m [2021-01-09 19:51:27,217] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)
